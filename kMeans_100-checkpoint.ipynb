{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Pooja Ajit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Pooja\n",
      "[nltk_data]     Ajit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Pooja\n",
      "[nltk_data]     Ajit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.image import ImageWriter\n",
    "from pdfminer.layout import LAParams\n",
    "\n",
    "import io\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from pathlib import *\n",
    "\n",
    "import time\n",
    "\n",
    "from random import randint\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import operator\n",
    "\n",
    "from nltk.chunk import tree2conlltags\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdfRender():\n",
    "    global documentSet\n",
    "    global mydoc\n",
    "    mydoc ={}\n",
    "    pdf_files =[]\n",
    "    allLines =[]\n",
    "    FILE_PATH = Path(r'E:\\MasterThesis\\FinalPapers\\testData_91')\n",
    "    #FILE_PATH = Path('E:/MasterThesis/FinalPapers')\n",
    "    pdf_files = list(FILE_PATH.glob('*.pdf'))\n",
    "    #An Array which stores the full text of each document\n",
    "    documentSet = pdfparser(pdf_files)\n",
    "    mydoc = dict(zip(pdf_files,documentSet))\n",
    "    #print(len(documentSet))\n",
    "    return documentSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdfparser(pdffileS):\n",
    "    global finalDocumentSet\n",
    "    finalDocumentSet = []\n",
    "    global pdfEx\n",
    "    pdfEx = []\n",
    "    global fullText\n",
    "    for pdffile in pdffileS:\n",
    "        #full= fullText\n",
    "        # Create a example words list(Please add all the related keywords needed)\n",
    "        words_list = [\"Introduction\", \"INTRODUCTION\", \"Background\", \"BACKGROUND\", \"Conclusion\", \"Conclusions\",\n",
    "                      \"CONCLUSION\", \"Acknowledgements\"]\n",
    "        #print(words_list)\n",
    "        with open(pdffile, mode='rb') as f:\n",
    "            fullText = np.array([])\n",
    "            pdfName = os.path.basename(pdffile)\n",
    "            print(pdfName)\n",
    "            #documents = fullText\n",
    "            #words_list = []\n",
    "            #print(words_list)\n",
    "            #fp = open(data, 'rb')\n",
    "            rsrcmgr = PDFResourceManager()\n",
    "            retstr = io.StringIO()\n",
    "            codec = 'utf-8'\n",
    "            laparams = LAParams()\n",
    "            data =[]\n",
    "            details_page = []\n",
    "            abstract = []\n",
    "            device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "            # Create a PDF interpreter object.\n",
    "            interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "            # Process each page contained in the document.\n",
    "            count = 0\n",
    "            for page in PDFPage.get_pages(f):\n",
    "                interpreter.process_page(page)\n",
    "                data = retstr.getvalue()\n",
    "                details_page.append(data)\n",
    "\n",
    "            #print(\"There are\", len(words_list), \"in the words list\")\n",
    "            stri = \" \"\n",
    "            details = stri.join(details_page)\n",
    "            words = details.split()\n",
    "            place = []\n",
    "            dummy_check = []\n",
    "            removed_words = []\n",
    "\n",
    "            print(words_list)\n",
    "            for c, a in enumerate(words):\n",
    "                for b in words_list:\n",
    "                    if b == a and b not in dummy_check:\n",
    "                        print(b, a)\n",
    "                        place.append(details.find(\"{}\".format(b)))\n",
    "                        dummy_check.append(b)\n",
    "                    #  place.append(words.index(a))\n",
    "                    elif b not in words:\n",
    "                        print(b)\n",
    "                        removed_words.append(b)\n",
    "                        words_list.remove(b)\n",
    "                        print(\"The word\", b, \"was not found in the pdf file\")\n",
    "\n",
    "            #print(list(zip(words_list, place)))\n",
    "            final_array = list(zip(words_list, place))\n",
    "            #final_array.sort()\n",
    "            final_array.sort(key=operator.itemgetter(1))\n",
    "            # print(\"Sorting the final array\")\n",
    "            #print(final_array)\n",
    "\n",
    "            # print(\"Extracting the relevant texts from pdf\")\n",
    "            # print(\" \")\n",
    "            print(final_array)\n",
    "            if len(final_array) > 1:\n",
    "                listint = final_array[0]\n",
    "                list2int = final_array[1]\n",
    "                counter = 0\n",
    "\n",
    "                for each in (final_array):\n",
    "                    if counter < len(final_array) - 2:\n",
    "                        new = (details.split(listint[0])[1].split(list2int[0])[0])\n",
    "                        #new = sent_tokenize(new)\n",
    "                        #print(listint[0], \":\", [' '.join(new)])\n",
    "                        #print(\" \")\n",
    "                        #print(new)\n",
    "                        #documents.append(new)\n",
    "                        fullText = np.append(fullText, new)\n",
    "                        counter = counter + 1\n",
    "                        listint = final_array[0 + counter]\n",
    "                        list2int = final_array[1 + counter]\n",
    "\n",
    "                    elif counter < len(final_array) - 1:\n",
    "                        new = (details.split(final_array[counter][0])[1].split(final_array[counter + 1][0])[0])\n",
    "                        #new = sent_tokenize(new)\n",
    "                        #documents.append(new)\n",
    "                        fullText = np.append(fullText, new)\n",
    "                        #print(final_array[counter][0], \":\", [' '.join(new)])\n",
    "                        #print(\" \")\n",
    "                        counter = counter + 1\n",
    "\n",
    "                    else:\n",
    "                        new = (details.split(final_array[counter][0])[1])\n",
    "                        #new = sent_tokenize(new)\n",
    "                        #documents.append(new)\n",
    "                        fullText = np.append(fullText, new)\n",
    "                        #print(final_array[counter][0], \":\", [' '.join(new)])\n",
    "                        #print(\" \")\n",
    "            else:\n",
    "                new = (details.split(final_array[0][0])[1])\n",
    "                # new = sent_tokenize(new)\n",
    "                #documents.append(new)\n",
    "                fullText = np.append(fullText, new)\n",
    "                # print(final_array[counter][0], \":\", [' '.join(new)])\n",
    "                # print(\" \")\n",
    "                \n",
    "        #finalDocumentSet = {pdfName : fullText}\n",
    "        \n",
    "        data=finalDocumentSet.append(fullText)\n",
    "        myName=pdfEx.append(pdfName)\n",
    "        #print(\"Testing==\",finalDocumentSet)\n",
    "        #data = finalDocumentSet.get(pdfName)\n",
    "        #finalDocumentSet = finalDocumentSet\n",
    "        data = str(data)\n",
    "        \n",
    "        data = processData(data)\n",
    "        #data = data.replace(r'\\\\n', \"\")\n",
    "        data = [i.replace('\\\\n', \"\") for i in data]\n",
    "        data = [i.replace('\\\\x0', \"\") for i in data]\n",
    "        words_list = words_list + removed_words\n",
    "        print(\"Updated words list:\")\n",
    "        print(words_list)\n",
    "\n",
    "    #print(len(finalDocumentSet))\n",
    "    \n",
    "    #mydoc = dict(zip(myName,data))\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processData(rawContents):    \n",
    "    cleaned = tokenizeContent(rawContents)    \n",
    "    cleaned1 = removeStopWordsFromTokenized(cleaned)    \n",
    "    cleaned2 = performPorterStemmingOnContents(cleaned1)    \n",
    "    cleaned3 = removePunctuationFromTokenized(cleaned2)    \n",
    "    cleaned4 = convertItemsToLower(cleaned3)    \n",
    "    return cleaned4    \n",
    "        \n",
    "def tokenizeContent(contentsRaw):    \n",
    "    tokenized = nltk.tokenize.sent_tokenize(contentsRaw)    \n",
    "    return tokenized    \n",
    "    \n",
    "def removeStopWordsFromTokenized(contentsTokenized):    \n",
    "    stop_word_set = set(nltk.corpus.stopwords.words(\"english\"))    \n",
    "    filteredContents = [word for word in contentsTokenized if word not in stop_word_set]    \n",
    "    return filteredContents    \n",
    "    \n",
    "def performPorterStemmingOnContents(contentsTokenized):    \n",
    "    porterStemmer = nltk.stem.PorterStemmer()    \n",
    "    filteredContents = [porterStemmer.stem(word) for word in contentsTokenized]    \n",
    "    return filteredContents    \n",
    "    \n",
    "def removePunctuationFromTokenized(contentsTokenized):    \n",
    "    excludePuncuation = set(string.punctuation)    \n",
    "    \n",
    "    # manually add additional punctuation to remove    \n",
    "    doubleSingleQuote = '\\'\\''    \n",
    "    doubleDash = '--'    \n",
    "    doubleTick = '``'    \n",
    "    \n",
    "    excludePuncuation.add(doubleSingleQuote)    \n",
    "    excludePuncuation.add(doubleDash)    \n",
    "    excludePuncuation.add(doubleTick)    \n",
    "    \n",
    "    filteredContents = [word for word in contentsTokenized if word not in excludePuncuation]    \n",
    "    return filteredContents    \n",
    "    \n",
    "def convertItemsToLower(contentsRaw):    \n",
    "    filteredContents = [term.lower() for term in contentsRaw]    \n",
    "    return filteredContents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-s2.0-S0268401218312763-main.pdf\n",
      "['Introduction', 'INTRODUCTION', 'Background', 'BACKGROUND', 'Conclusion', 'Conclusions', 'CONCLUSION', 'Acknowledgements']\n",
      "INTRODUCTION\n",
      "The word INTRODUCTION was not found in the pdf file\n",
      "BACKGROUND\n",
      "The word BACKGROUND was not found in the pdf file\n",
      "CONCLUSION\n",
      "The word CONCLUSION was not found in the pdf file\n",
      "Background\n",
      "The word Background was not found in the pdf file\n",
      "Conclusion\n",
      "The word Conclusion was not found in the pdf file\n",
      "Introduction Introduction\n",
      "Conclusions Conclusions\n",
      "Acknowledgements Acknowledgements\n",
      "[('Introduction', 2482), ('Conclusions', 185619), ('Acknowledgements', 187485)]\n",
      "Updated words list:\n",
      "['Introduction', 'Conclusions', 'Acknowledgements', 'INTRODUCTION', 'BACKGROUND', 'CONCLUSION', 'Background', 'Conclusion']\n",
      "10.1002@asi.23250.pdf\n",
      "['Introduction', 'INTRODUCTION', 'Background', 'BACKGROUND', 'Conclusion', 'Conclusions', 'CONCLUSION', 'Acknowledgements']\n",
      "INTRODUCTION\n",
      "The word INTRODUCTION was not found in the pdf file\n",
      "BACKGROUND\n",
      "The word BACKGROUND was not found in the pdf file\n",
      "Conclusions\n",
      "The word Conclusions was not found in the pdf file\n",
      "Acknowledgements\n",
      "The word Acknowledgements was not found in the pdf file\n",
      "Background\n",
      "The word Background was not found in the pdf file\n",
      "CONCLUSION\n",
      "The word CONCLUSION was not found in the pdf file\n",
      "Introduction Introduction\n",
      "Conclusion Conclusion\n",
      "[('Introduction', 1649), ('Conclusion', 10356)]\n",
      "Updated words list:\n",
      "['Introduction', 'Conclusion', 'INTRODUCTION', 'BACKGROUND', 'Conclusions', 'Acknowledgements', 'Background', 'CONCLUSION']\n",
      "10.1002@jrsm.1094.pdf\n",
      "['Introduction', 'INTRODUCTION', 'Background', 'BACKGROUND', 'Conclusion', 'Conclusions', 'CONCLUSION', 'Acknowledgements']\n",
      "Introduction\n",
      "The word Introduction was not found in the pdf file\n",
      "BACKGROUND\n",
      "The word BACKGROUND was not found in the pdf file\n",
      "Conclusions\n",
      "The word Conclusions was not found in the pdf file\n",
      "INTRODUCTION\n",
      "The word INTRODUCTION was not found in the pdf file\n",
      "CONCLUSION\n",
      "The word CONCLUSION was not found in the pdf file\n",
      "Background Background\n",
      "Conclusion Conclusion\n",
      "Acknowledgements Acknowledgements\n",
      "[('Background', 1849), ('Conclusion', 189547), ('Acknowledgements', 231278)]\n",
      "Updated words list:\n",
      "['Background', 'Conclusion', 'Acknowledgements', 'Introduction', 'BACKGROUND', 'Conclusions', 'INTRODUCTION', 'CONCLUSION']\n",
      "10.1002@jrsm.1311.pdf\n",
      "['Introduction', 'INTRODUCTION', 'Background', 'BACKGROUND', 'Conclusion', 'Conclusions', 'CONCLUSION', 'Acknowledgements']\n",
      "INTRODUCTION\n",
      "The word INTRODUCTION was not found in the pdf file\n",
      "BACKGROUND\n",
      "The word BACKGROUND was not found in the pdf file\n",
      "Conclusions\n",
      "The word Conclusions was not found in the pdf file\n",
      "Background\n",
      "The word Background was not found in the pdf file\n",
      "CONCLUSION\n",
      "The word CONCLUSION was not found in the pdf file\n",
      "Introduction Introduction\n",
      "Conclusion Conclusion\n",
      "Acknowledgements Acknowledgements\n",
      "[('Introduction', 5937), ('Conclusion', 427263), ('Acknowledgements', 428625)]\n",
      "Updated words list:\n",
      "['Introduction', 'Conclusion', 'Acknowledgements', 'INTRODUCTION', 'BACKGROUND', 'Conclusions', 'Background', 'CONCLUSION']\n",
      "A-survey-of-prevalence-of-narrative-and-systematic-reviews-in-five-major-medical-journals2017BMC-Medical-Research-MethodologyOpen-Access.pdf\n",
      "['Introduction', 'INTRODUCTION', 'Background', 'BACKGROUND', 'Conclusion', 'Conclusions', 'CONCLUSION', 'Acknowledgements']\n",
      "Introduction\n",
      "The word Introduction was not found in the pdf file\n",
      "BACKGROUND\n",
      "The word BACKGROUND was not found in the pdf file\n",
      "CONCLUSION\n",
      "The word CONCLUSION was not found in the pdf file\n",
      "INTRODUCTION\n",
      "The word INTRODUCTION was not found in the pdf file\n",
      "Conclusion\n",
      "The word Conclusion was not found in the pdf file\n",
      "Background Background\n",
      "Conclusions Conclusions\n",
      "Acknowledgements Acknowledgements\n",
      "[('Background', 300), ('Conclusions', 2258), ('Acknowledgements', 123239)]\n",
      "Updated words list:\n",
      "['Background', 'Conclusions', 'Acknowledgements', 'Introduction', 'BACKGROUND', 'CONCLUSION', 'INTRODUCTION', 'Conclusion']\n",
      "A-systematic-literature-review-of-actionable-alert-identification-techniques-for-automated-static-code-analysis2011Information-and-Software-Technology.pdf\n",
      "['Introduction', 'INTRODUCTION', 'Background', 'BACKGROUND', 'Conclusion', 'Conclusions', 'CONCLUSION', 'Acknowledgements']\n",
      "INTRODUCTION\n",
      "The word INTRODUCTION was not found in the pdf file\n",
      "BACKGROUND\n",
      "The word BACKGROUND was not found in the pdf file\n",
      "CONCLUSION\n",
      "The word CONCLUSION was not found in the pdf file\n",
      "Background\n",
      "The word Background was not found in the pdf file\n",
      "Conclusion\n",
      "The word Conclusion was not found in the pdf file\n",
      "Introduction Introduction\n",
      "Acknowledgements Acknowledgements\n",
      "Conclusions Conclusions\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e237d84c583b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpdfRender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-14-3db1ab7f97ac>\u001b[0m in \u001b[0;36mpdfRender\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpdf_files\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFILE_PATH\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'*.pdf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#An Array which stores the full text of each document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mdocumentSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpdfparser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_files\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mmydoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdf_files\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdocumentSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m#print(len(documentSet))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-8ec0707f8857>\u001b[0m in \u001b[0;36mpdfparser\u001b[1;34m(pdffileS)\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdummy_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pdfRender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
