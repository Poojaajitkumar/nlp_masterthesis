{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Pooja Ajit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Pooja\n",
      "[nltk_data]     Ajit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to C:\\Users\\Pooja\n",
      "[nltk_data]     Ajit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.image import ImageWriter\n",
    "from pdfminer.layout import LAParams\n",
    "\n",
    "import io\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "from pathlib import *\n",
    "\n",
    "import time\n",
    "\n",
    "from random import randint\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import operator\n",
    "\n",
    "from nltk.chunk import tree2conlltags\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdfRender():\n",
    "    global documentSet\n",
    "    global mydoc\n",
    "    mydoc ={}\n",
    "    pdf_files =[]\n",
    "    allLines =[]\n",
    "    FILE_PATH = Path(r'E:\\MasterThesis\\FinalPapers\\papersforQAModelling\\test')\n",
    "    #FILE_PATH = Path(r'E:\\MasterThesis\\FinalPapers\\papersforQAModelling')\n",
    "    pdf_files = list(FILE_PATH.glob('*.pdf'))\n",
    "    #An Array which stores the full text of each document\n",
    "    documentSet = pdfparser(pdf_files)\n",
    "    #mydoc = dict(zip(pdf_files,documentSet))\n",
    "    #print(len(documentSet))\n",
    "    return documentSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "global pdfName\n",
    "global fileName\n",
    "def pdfparser(pdffileS):\n",
    "    global finalDocumentSet\n",
    "    finalDocumentSet = []\n",
    "    global pdfEx\n",
    "    pdfEx = []\n",
    "    fileName = []\n",
    "    global fullText\n",
    "    for pdffile in pdffileS:\n",
    "        with open(pdffile, mode='rb') as f:\n",
    "            fullText = np.array([])\n",
    "            pdfName = os.path.basename(pdffile)\n",
    "            print(pdfName)\n",
    "            fileName.append(pdfName)\n",
    "            rsrcmgr = PDFResourceManager()\n",
    "            retstr = io.StringIO()\n",
    "            codec = 'utf-8'\n",
    "            laparams = LAParams()\n",
    "            data =[]\n",
    "            details_page = []\n",
    "            abstract = []\n",
    "            device = TextConverter(rsrcmgr, retstr, laparams=laparams)\n",
    "            # Create a PDF interpreter object.\n",
    "            interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "            # Process each page contained in the document.\n",
    "            count = 0\n",
    "            for page in PDFPage.get_pages(f):\n",
    "                interpreter.process_page(page)\n",
    "                data = retstr.getvalue()\n",
    "                \n",
    "                #Pre-Processing of the text\n",
    "                '''data = data.lower()\n",
    "                data = re.sub(r'\\d+', '', data)\n",
    "                data = re.sub(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+','', data)\n",
    "                data = data.translate(str.maketrans('','', string.punctuation))\n",
    "                data = data.strip()\n",
    "                data = data.replace(\"\\n\", \"\")\n",
    "                data = sent_tokenize(data)'''\n",
    "                details_page.append(data)\n",
    "                \n",
    "            finalDocumentSet.append(details_page)\n",
    "            \n",
    "    return finalDocumentSet\n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complementary-approaches-to-searching-MEDLINE-may-be-sufficient-for-updating-systematic-reviews2016Journal-of-Clinical-Epidemiology.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nComplementary approaches to searching MEDLINE may be\\n\\nsufﬁcient for updating systematic reviews\\n\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\n\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\n\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\n\\n0R6, Canada\\n\\nUnited Kingdom\\n\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\n\\nAccepted 7 March 2016; Published online 11 March 2016\\n\\nAbstract\\n\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\n\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\nexamined for each method.\\n\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\ncombined. Related articles showed least stability over time.\\n\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\n\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\n\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\n\\n1. Introduction\\n\\nSystematic review searches need to have high recall.\\nMechanisms to achieve this usually include expansive\\nBoolean searches of multiple databases. This approach\\nleads to long development times for the searches [1], neces-\\nsitates accessing multiple sources that may not be acces-\\nsible in some institutions [2], and entails time-consuming\\nremoval of duplicate records for articles indexed in more\\nthan one of the databases searched [3].\\n\\nMEDLINE gives excellent coverage of most biomed-\\nical topics, in particular, intervention studies. However,\\n\\nConﬂict of interest: None.\\nFunding: Some of this work was funded by the US Agency for Health-\\ncare Research and Quality, Department of Health and Human Services\\n(contract no. 290-02-0021).\\n\\n* Corresponding author. Tel.: 613-737-7600.\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\n\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\n\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\nlished that only about half the studies included in system-\\natic reviews were identiﬁed through MEDLINE. Recent\\nresearch has demonstrated that a much higher percentage\\nis present in MEDLINE, but sometimes their retrieval is\\nproblematic [5e8].\\n\\nSuccessful retrieval through a Boolean search is oper-\\nator dependent, with search performance being inﬂuenced\\nby skill of the indexer and the searcher. The search of\\nmultiple databases can therefore be helpful. The target re-\\ncords may be indexed differently in the second, third, or\\nsubsequent source searched, increasing the probability\\nof a match between the search terms entered and the in-\\ndexing of the additional records. A text search, querying\\nterms appearing in the title or abstract, may help improve\\nretrieval, as may the use of indexing terms that are broad-\\ner (less speciﬁc) than, or related to, the single best index-\\ning term. These tactics on the part of the searcher lead\\n\\n\\x0c',\n",
       "  'Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nComplementary approaches to searching MEDLINE may be\\n\\nsufﬁcient for updating systematic reviews\\n\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\n\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\n\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\n\\n0R6, Canada\\n\\nUnited Kingdom\\n\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\n\\nAccepted 7 March 2016; Published online 11 March 2016\\n\\nAbstract\\n\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\n\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\nexamined for each method.\\n\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\ncombined. Related articles showed least stability over time.\\n\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\n\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\n\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\n\\n1. Introduction\\n\\nSystematic review searches need to have high recall.\\nMechanisms to achieve this usually include expansive\\nBoolean searches of multiple databases. This approach\\nleads to long development times for the searches [1], neces-\\nsitates accessing multiple sources that may not be acces-\\nsible in some institutions [2], and entails time-consuming\\nremoval of duplicate records for articles indexed in more\\nthan one of the databases searched [3].\\n\\nMEDLINE gives excellent coverage of most biomed-\\nical topics, in particular, intervention studies. However,\\n\\nConﬂict of interest: None.\\nFunding: Some of this work was funded by the US Agency for Health-\\ncare Research and Quality, Department of Health and Human Services\\n(contract no. 290-02-0021).\\n\\n* Corresponding author. Tel.: 613-737-7600.\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\n\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\n\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\nlished that only about half the studies included in system-\\natic reviews were identiﬁed through MEDLINE. Recent\\nresearch has demonstrated that a much higher percentage\\nis present in MEDLINE, but sometimes their retrieval is\\nproblematic [5e8].\\n\\nSuccessful retrieval through a Boolean search is oper-\\nator dependent, with search performance being inﬂuenced\\nby skill of the indexer and the searcher. The search of\\nmultiple databases can therefore be helpful. The target re-\\ncords may be indexed differently in the second, third, or\\nsubsequent source searched, increasing the probability\\nof a match between the search terms entered and the in-\\ndexing of the additional records. A text search, querying\\nterms appearing in the title or abstract, may help improve\\nretrieval, as may the use of indexing terms that are broad-\\ner (less speciﬁc) than, or related to, the single best index-\\ning term. These tactics on the part of the searcher lead\\n\\n\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\n109\\n\\nWhat is new?\\n\\nKey ﬁndings\\n(cid:1) Searches using known relevant studies and the\\nsimilar articles feature of PubMed will identify\\nand rank additional articles of potential relevance.\\n(cid:1) For a given question, if the Boolean search has low\\nrecall, the ranking search tends to have higher\\nrecall, and vice versa. The two approaches comple-\\nment each other.\\n\\n(cid:1) The precision of\\n\\nmethod appears better\\nexhaustive Boolean searches.\\n\\nthis complementary paired\\nthan the precision of\\n\\nWhat this adds to what was known?\\n(cid:1) The paired approach performed well regardless of\\nwhich of two tested similarity searches were used.\\nIt is the use of independent retrieval methods that is\\nimportant.\\n\\nWhat is the implication and what should change\\nnow?\\n(cid:1) Using the simple and universally available PubMed\\nsimilar feature makes this paired approach prac-\\ntical for most systematic review teams.\\n\\n(cid:1) If the paired complementary approach is used, the\\nrecall may be sufﬁcient to consider using only\\nMEDLINE.\\n\\nto large retrievals with low precision [9]. These resulting\\nproblems may be particularly challenging for complex or\\nnewly emerging interventions with highly variant terminol-\\nogy, where alternatives to traditional Boolean searches have\\nbeen sought [10].\\n\\nMost recent systematic review information retrieval\\nresearch has focused on text mining approaches [11e14].\\nThese approaches often harvest an intentionally overinclu-\\nsive set of records and then use machine learning, similarity\\nranking and other techniques to reﬁne the set to identify the\\nmaterial most likely to be relevant, thereby reducing human\\nscreening effort. These methods show promise, but are not\\nyet widely available to reviewers.\\n\\nWe examined one method, support vector machine\\n(SVM), and compared it with a simple and readily available\\nmethod based on the PubMed similar articles feature. We\\ncall this method related articles to distinguish the method\\nfrom the similar articles feature itself. We paired both\\nwith a focused Boolean search within MEDLINE. We\\ntested this approach in an updating context where studies\\nincluded in the original review comprise the reference\\nstandard for SVM and seed articles for the PubMed-\\n\\nrelated articles search. We therefore sought to determine\\nif a focused Boolean search paired with one of the search\\nmethods that does not depend on operator skill could pro-\\nvide consistently complete retrieval of relevant new studies.\\nComparison of a number of searches, including two\\ntested here, has been previously reported [15]. In this cur-\\nrent article, two of the most successful methods in a larger\\nsample of 72 journal-published systematic reviews, clinical\\nquery (CQ), and PubMed-related articles are tested in a\\ncohort of six updated Cochrane reviews, as well as in a pre-\\nviously unreported sample of 10 Agency for Healthcare\\nResearch and Quality (AHRQ) evidence reports. The\\nCochrane reviews provide a true gold standard as updates\\nwere made by the review team based on evidence identiﬁed\\nthrough comprehensive searches; however, the new relevant\\nstudies proved fairly easy to ﬁnd. The replication in the\\nAHRQ cohort, of more complex interventions, provided a\\nmeans to validate the generalizability of the approach\\n[16]. All records were assessed for eligibility by two re-\\nviewers, and this complete screening allowed the precision\\nof the methods to be calculated for the ﬁrst time.\\n\\nOther research [17,18] suggests that searches using the\\nPubMed similar articles feature are effective in increasing\\nrecall of relevant items for reviews or more general clinical\\nsearching when combined with a Boolean-type search of\\nMEDLINE. We tested an additional search method, SVM,\\nin the Cochrane and AHRQ samples to permit comparison\\nwith our PubMed-related articles search and assess whether\\nthe complementary effect generalized beyond the PubMed\\nsimilar articles method.\\n\\nThe aim of this article was to test whether the combined\\napproach of a focused Boolean search paired with a second\\nsearch using the similar articles feature of PubMed or SVM\\ncan yield high recall with reasonable precision.\\n\\n2. Methods\\n\\n2.1. Formation of the study cohorts\\n\\nThis analysis uses a data set created for an updating\\nstudy sponsored by AHRQ [16]. Methods for the selection\\nof the cohorts, search approaches tested, and rigorous\\nmechanisms to screen search results for relevance have\\nbeen previously reported, along with the criteria to deter-\\nmine if a review was in need of update [16].\\n\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\nsearch of the ACP Journal Club database (Ovid) using the\\nstrategy:\\n\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\nwith commentary.\\n\\nAHRQ reports were identiﬁed through the PubMed\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\njrid21544]. Screening was undertaken in two phases with\\ntwo reviewers reaching consensus on eligibility. Screening\\ncontinued until the predetermined sample size was reached.\\n\\n\\x0c',\n",
       "  'Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nComplementary approaches to searching MEDLINE may be\\n\\nsufﬁcient for updating systematic reviews\\n\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\n\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\n\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\n\\n0R6, Canada\\n\\nUnited Kingdom\\n\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\n\\nAccepted 7 March 2016; Published online 11 March 2016\\n\\nAbstract\\n\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\n\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\nexamined for each method.\\n\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\ncombined. Related articles showed least stability over time.\\n\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\n\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\n\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\n\\n1. Introduction\\n\\nSystematic review searches need to have high recall.\\nMechanisms to achieve this usually include expansive\\nBoolean searches of multiple databases. This approach\\nleads to long development times for the searches [1], neces-\\nsitates accessing multiple sources that may not be acces-\\nsible in some institutions [2], and entails time-consuming\\nremoval of duplicate records for articles indexed in more\\nthan one of the databases searched [3].\\n\\nMEDLINE gives excellent coverage of most biomed-\\nical topics, in particular, intervention studies. However,\\n\\nConﬂict of interest: None.\\nFunding: Some of this work was funded by the US Agency for Health-\\ncare Research and Quality, Department of Health and Human Services\\n(contract no. 290-02-0021).\\n\\n* Corresponding author. Tel.: 613-737-7600.\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\n\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\n\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\nlished that only about half the studies included in system-\\natic reviews were identiﬁed through MEDLINE. Recent\\nresearch has demonstrated that a much higher percentage\\nis present in MEDLINE, but sometimes their retrieval is\\nproblematic [5e8].\\n\\nSuccessful retrieval through a Boolean search is oper-\\nator dependent, with search performance being inﬂuenced\\nby skill of the indexer and the searcher. The search of\\nmultiple databases can therefore be helpful. The target re-\\ncords may be indexed differently in the second, third, or\\nsubsequent source searched, increasing the probability\\nof a match between the search terms entered and the in-\\ndexing of the additional records. A text search, querying\\nterms appearing in the title or abstract, may help improve\\nretrieval, as may the use of indexing terms that are broad-\\ner (less speciﬁc) than, or related to, the single best index-\\ning term. These tactics on the part of the searcher lead\\n\\n\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\n109\\n\\nWhat is new?\\n\\nKey ﬁndings\\n(cid:1) Searches using known relevant studies and the\\nsimilar articles feature of PubMed will identify\\nand rank additional articles of potential relevance.\\n(cid:1) For a given question, if the Boolean search has low\\nrecall, the ranking search tends to have higher\\nrecall, and vice versa. The two approaches comple-\\nment each other.\\n\\n(cid:1) The precision of\\n\\nmethod appears better\\nexhaustive Boolean searches.\\n\\nthis complementary paired\\nthan the precision of\\n\\nWhat this adds to what was known?\\n(cid:1) The paired approach performed well regardless of\\nwhich of two tested similarity searches were used.\\nIt is the use of independent retrieval methods that is\\nimportant.\\n\\nWhat is the implication and what should change\\nnow?\\n(cid:1) Using the simple and universally available PubMed\\nsimilar feature makes this paired approach prac-\\ntical for most systematic review teams.\\n\\n(cid:1) If the paired complementary approach is used, the\\nrecall may be sufﬁcient to consider using only\\nMEDLINE.\\n\\nto large retrievals with low precision [9]. These resulting\\nproblems may be particularly challenging for complex or\\nnewly emerging interventions with highly variant terminol-\\nogy, where alternatives to traditional Boolean searches have\\nbeen sought [10].\\n\\nMost recent systematic review information retrieval\\nresearch has focused on text mining approaches [11e14].\\nThese approaches often harvest an intentionally overinclu-\\nsive set of records and then use machine learning, similarity\\nranking and other techniques to reﬁne the set to identify the\\nmaterial most likely to be relevant, thereby reducing human\\nscreening effort. These methods show promise, but are not\\nyet widely available to reviewers.\\n\\nWe examined one method, support vector machine\\n(SVM), and compared it with a simple and readily available\\nmethod based on the PubMed similar articles feature. We\\ncall this method related articles to distinguish the method\\nfrom the similar articles feature itself. We paired both\\nwith a focused Boolean search within MEDLINE. We\\ntested this approach in an updating context where studies\\nincluded in the original review comprise the reference\\nstandard for SVM and seed articles for the PubMed-\\n\\nrelated articles search. We therefore sought to determine\\nif a focused Boolean search paired with one of the search\\nmethods that does not depend on operator skill could pro-\\nvide consistently complete retrieval of relevant new studies.\\nComparison of a number of searches, including two\\ntested here, has been previously reported [15]. In this cur-\\nrent article, two of the most successful methods in a larger\\nsample of 72 journal-published systematic reviews, clinical\\nquery (CQ), and PubMed-related articles are tested in a\\ncohort of six updated Cochrane reviews, as well as in a pre-\\nviously unreported sample of 10 Agency for Healthcare\\nResearch and Quality (AHRQ) evidence reports. The\\nCochrane reviews provide a true gold standard as updates\\nwere made by the review team based on evidence identiﬁed\\nthrough comprehensive searches; however, the new relevant\\nstudies proved fairly easy to ﬁnd. The replication in the\\nAHRQ cohort, of more complex interventions, provided a\\nmeans to validate the generalizability of the approach\\n[16]. All records were assessed for eligibility by two re-\\nviewers, and this complete screening allowed the precision\\nof the methods to be calculated for the ﬁrst time.\\n\\nOther research [17,18] suggests that searches using the\\nPubMed similar articles feature are effective in increasing\\nrecall of relevant items for reviews or more general clinical\\nsearching when combined with a Boolean-type search of\\nMEDLINE. We tested an additional search method, SVM,\\nin the Cochrane and AHRQ samples to permit comparison\\nwith our PubMed-related articles search and assess whether\\nthe complementary effect generalized beyond the PubMed\\nsimilar articles method.\\n\\nThe aim of this article was to test whether the combined\\napproach of a focused Boolean search paired with a second\\nsearch using the similar articles feature of PubMed or SVM\\ncan yield high recall with reasonable precision.\\n\\n2. Methods\\n\\n2.1. Formation of the study cohorts\\n\\nThis analysis uses a data set created for an updating\\nstudy sponsored by AHRQ [16]. Methods for the selection\\nof the cohorts, search approaches tested, and rigorous\\nmechanisms to screen search results for relevance have\\nbeen previously reported, along with the criteria to deter-\\nmine if a review was in need of update [16].\\n\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\nsearch of the ACP Journal Club database (Ovid) using the\\nstrategy:\\n\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\nwith commentary.\\n\\nAHRQ reports were identiﬁed through the PubMed\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\njrid21544]. Screening was undertaken in two phases with\\ntwo reviewers reaching consensus on eligibility. Screening\\ncontinued until the predetermined sample size was reached.\\n\\n\\x0c110\\n\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nFor inclusion, all reviews must have included random-\\nized controlled trials (RCTs) and provided meta-analysis\\nfor at least one outcome. The MEDLINE search strategies\\nhad to be reported in enough detail to permit replication.\\nFifteen AHRQ evidence reports thought\\nlikely to have\\nimportant new evidence were selected and used to validate\\nthe updating methods used in the main cohort.\\n\\nCochrane reviews were selected to meet minimum qual-\\nity and relevance standards, as deﬁned by ACP Journal\\nClub [19]. We also required that the review had been up-\\ndated and that the text of both the original review and an\\nupdated version was available. The search for the original\\nreview must have included MEDLINE and at least one\\nother electronic bibliographic database and one or more\\nnondatabase method such as hand searching or checking\\nreference lists. Such comprehensive searching was likely\\nto identify most relevant\\nliterature and form a useful\\ntraining set of included examples. At least 10 RCTs or\\nquasi-RCTs must have been included in the original review\\nto give an adequate training set for SVM.\\n\\n2.2. Test searches\\n\\n2.2.1. Clinical query\\n\\nBoolean searches were developed by a librarian experi-\\nenced in systematic review searches using a protocol\\n(Appendix A at www.jclinepi.com). Search strategies were\\ndeliberately simple, usually consisting of two or three\\nMedical Subject Heading (MeSH) terms representing the\\npopulation and intervention of the review. This search\\nwas limited by a CQ ﬁlter to ‘‘therapy (best balance of\\nsensitivity\\n‘‘Randomized\\ncontrolled trial.pt. or\\nrandomized.mp. or placebo.mp.’’\\n[20]. Boolean searches were run in OVID MEDLINE.\\n\\nspeciﬁcity)’’\\n\\nthat\\n\\nand\\n\\nis,\\n\\n2.2.2. Related articles\\n\\nSearches using the PubMed similar articles feature were\\nperformed using the PubMed unique identiﬁers (PMID) of\\nthe three newest and three largest studies included in the\\noriginal review as seed articles. There was no replacement\\nin the event of overlap between the largest and newest sets,\\nor if one of those studies was not indexed in MEDLINE.\\n(All studies included in the original reviews were checked\\nfor indexing status in MEDLINE.) The resulting set was\\nlimited to the publication type RCT and date limited to\\nthe period since the search date of the original review.\\n\\n2.2.3. Support vector machine\\n\\nThe SVM searches used for this project have not been\\npreviously described so the methods will be reported in\\nmore detail. SVM is a well established and broadly applied\\nclassiﬁcation method from machine learning [21,22]. A\\nclassiﬁcation task is closely related to an information\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\nthe collection. As SVM is a supervised algorithm,\\nit\\n\\nrequires training examples with known labels. It then places\\nthese training examples in a high-dimensional vector space\\nin such a way that examples of one class are separated from\\nexamples of the other class with the greatest distance to the\\nseparating boundary. Predicting the class of a new instance\\nhappens by placing it in the same space and determining on\\nwhich side of the boundary it falls. The distance to the\\nboundary is an indication of the prediction conﬁdence and\\nis used to rank instances (documents) as is essential in in-\\nformation retrieval tasks or to provide a cutoff threshold.\\n\\nIn our setup, the training set consisted of search results\\nfrom the original review formed with included instances\\nbeing the studies included in the review (the relevant re-\\ntrievals) and excluded instances being the studies found\\nby the search used in the original review but excluded from\\nthe review (irrelevant retrievals). The new examples to be\\nclassiﬁed were articles indexed since the date of the search\\nperformed in the original review.\\n\\nSVM searches were run on MEDLINE data stored\\nlocally at\\nthe National Research Council of Canada.\\nMEDLINE was refreshed before running the searches.\\nThe MEDLINE records were represented as bags of fea-\\ntures, where features consisted of lowercased words and\\nword combinations (up to four words) from the title and ab-\\nstract ﬁelds, full MeSH terms, and contents of all other\\nﬁelds in the MEDLINE record. Features were uniformly\\nweighted. The SVM implementation used was SVM Light\\n[23] used with a linear kernel.\\n\\nThe approach was piloted with one AHRQ evidence\\nreport, and various conﬁgurations were tried, observing\\nthe placement of several known new relevant studies within\\nthe retrieval. The MEDLINE set was ﬁltered with the\\nrevised Highly Sensitive Search Strategy (HSSS) [24]. Pre-\\nliminary testing indicated that this ﬁlter would retrieve\\n99.1% of MEDLINE-indexed relevant new evidence from\\nthe larger cohort of journal-published systematic reviews\\n(n 5 72) and Cochrane reviews (n 5 27). This HSSS ﬁlter\\nwas added primarily to improve processing speed, relative\\nto running the classiﬁer against all of MEDLINE. With\\nthe ﬁlter, processing for one systematic review took about\\n10 minutes. Next, a coarse-grained ﬁlter was applied;\\nincluded and excluded instances were combined in a single\\nset labeled POS. Adjacent PubMed IDs (the next higher\\nPMID to each member of POS, as long as that higher num-\\nber was not itself a member of POS) were combined in a set\\nto represent WORLD. The model was trained on POS vs.\\nWORLD, using words and phrases from title, abstract,\\nauthor names, and journal name. This model was run\\nagainst the records passing the HSSS ﬁlter.\\n\\nFour different subsets of\\n\\nthe SVM retrieval were\\nexamineddone set included all records with a relevance\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\n(SVM200point5). Other sets consisted of all records with\\nrelevance scores of 0.95 or more (SVM95), 0.90 or more\\n(SVM90), and 0.80 or more (SVM80). PubMed ID and\\nrelevance scores were recorded, and PubMed IDs were\\n\\n\\x0c',\n",
       "  'Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nComplementary approaches to searching MEDLINE may be\\n\\nsufﬁcient for updating systematic reviews\\n\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\n\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\n\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\n\\n0R6, Canada\\n\\nUnited Kingdom\\n\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\n\\nAccepted 7 March 2016; Published online 11 March 2016\\n\\nAbstract\\n\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\n\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\nexamined for each method.\\n\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\ncombined. Related articles showed least stability over time.\\n\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\n\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\n\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\n\\n1. Introduction\\n\\nSystematic review searches need to have high recall.\\nMechanisms to achieve this usually include expansive\\nBoolean searches of multiple databases. This approach\\nleads to long development times for the searches [1], neces-\\nsitates accessing multiple sources that may not be acces-\\nsible in some institutions [2], and entails time-consuming\\nremoval of duplicate records for articles indexed in more\\nthan one of the databases searched [3].\\n\\nMEDLINE gives excellent coverage of most biomed-\\nical topics, in particular, intervention studies. However,\\n\\nConﬂict of interest: None.\\nFunding: Some of this work was funded by the US Agency for Health-\\ncare Research and Quality, Department of Health and Human Services\\n(contract no. 290-02-0021).\\n\\n* Corresponding author. Tel.: 613-737-7600.\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\n\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\n\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\nlished that only about half the studies included in system-\\natic reviews were identiﬁed through MEDLINE. Recent\\nresearch has demonstrated that a much higher percentage\\nis present in MEDLINE, but sometimes their retrieval is\\nproblematic [5e8].\\n\\nSuccessful retrieval through a Boolean search is oper-\\nator dependent, with search performance being inﬂuenced\\nby skill of the indexer and the searcher. The search of\\nmultiple databases can therefore be helpful. The target re-\\ncords may be indexed differently in the second, third, or\\nsubsequent source searched, increasing the probability\\nof a match between the search terms entered and the in-\\ndexing of the additional records. A text search, querying\\nterms appearing in the title or abstract, may help improve\\nretrieval, as may the use of indexing terms that are broad-\\ner (less speciﬁc) than, or related to, the single best index-\\ning term. These tactics on the part of the searcher lead\\n\\n\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\n109\\n\\nWhat is new?\\n\\nKey ﬁndings\\n(cid:1) Searches using known relevant studies and the\\nsimilar articles feature of PubMed will identify\\nand rank additional articles of potential relevance.\\n(cid:1) For a given question, if the Boolean search has low\\nrecall, the ranking search tends to have higher\\nrecall, and vice versa. The two approaches comple-\\nment each other.\\n\\n(cid:1) The precision of\\n\\nmethod appears better\\nexhaustive Boolean searches.\\n\\nthis complementary paired\\nthan the precision of\\n\\nWhat this adds to what was known?\\n(cid:1) The paired approach performed well regardless of\\nwhich of two tested similarity searches were used.\\nIt is the use of independent retrieval methods that is\\nimportant.\\n\\nWhat is the implication and what should change\\nnow?\\n(cid:1) Using the simple and universally available PubMed\\nsimilar feature makes this paired approach prac-\\ntical for most systematic review teams.\\n\\n(cid:1) If the paired complementary approach is used, the\\nrecall may be sufﬁcient to consider using only\\nMEDLINE.\\n\\nto large retrievals with low precision [9]. These resulting\\nproblems may be particularly challenging for complex or\\nnewly emerging interventions with highly variant terminol-\\nogy, where alternatives to traditional Boolean searches have\\nbeen sought [10].\\n\\nMost recent systematic review information retrieval\\nresearch has focused on text mining approaches [11e14].\\nThese approaches often harvest an intentionally overinclu-\\nsive set of records and then use machine learning, similarity\\nranking and other techniques to reﬁne the set to identify the\\nmaterial most likely to be relevant, thereby reducing human\\nscreening effort. These methods show promise, but are not\\nyet widely available to reviewers.\\n\\nWe examined one method, support vector machine\\n(SVM), and compared it with a simple and readily available\\nmethod based on the PubMed similar articles feature. We\\ncall this method related articles to distinguish the method\\nfrom the similar articles feature itself. We paired both\\nwith a focused Boolean search within MEDLINE. We\\ntested this approach in an updating context where studies\\nincluded in the original review comprise the reference\\nstandard for SVM and seed articles for the PubMed-\\n\\nrelated articles search. We therefore sought to determine\\nif a focused Boolean search paired with one of the search\\nmethods that does not depend on operator skill could pro-\\nvide consistently complete retrieval of relevant new studies.\\nComparison of a number of searches, including two\\ntested here, has been previously reported [15]. In this cur-\\nrent article, two of the most successful methods in a larger\\nsample of 72 journal-published systematic reviews, clinical\\nquery (CQ), and PubMed-related articles are tested in a\\ncohort of six updated Cochrane reviews, as well as in a pre-\\nviously unreported sample of 10 Agency for Healthcare\\nResearch and Quality (AHRQ) evidence reports. The\\nCochrane reviews provide a true gold standard as updates\\nwere made by the review team based on evidence identiﬁed\\nthrough comprehensive searches; however, the new relevant\\nstudies proved fairly easy to ﬁnd. The replication in the\\nAHRQ cohort, of more complex interventions, provided a\\nmeans to validate the generalizability of the approach\\n[16]. All records were assessed for eligibility by two re-\\nviewers, and this complete screening allowed the precision\\nof the methods to be calculated for the ﬁrst time.\\n\\nOther research [17,18] suggests that searches using the\\nPubMed similar articles feature are effective in increasing\\nrecall of relevant items for reviews or more general clinical\\nsearching when combined with a Boolean-type search of\\nMEDLINE. We tested an additional search method, SVM,\\nin the Cochrane and AHRQ samples to permit comparison\\nwith our PubMed-related articles search and assess whether\\nthe complementary effect generalized beyond the PubMed\\nsimilar articles method.\\n\\nThe aim of this article was to test whether the combined\\napproach of a focused Boolean search paired with a second\\nsearch using the similar articles feature of PubMed or SVM\\ncan yield high recall with reasonable precision.\\n\\n2. Methods\\n\\n2.1. Formation of the study cohorts\\n\\nThis analysis uses a data set created for an updating\\nstudy sponsored by AHRQ [16]. Methods for the selection\\nof the cohorts, search approaches tested, and rigorous\\nmechanisms to screen search results for relevance have\\nbeen previously reported, along with the criteria to deter-\\nmine if a review was in need of update [16].\\n\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\nsearch of the ACP Journal Club database (Ovid) using the\\nstrategy:\\n\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\nwith commentary.\\n\\nAHRQ reports were identiﬁed through the PubMed\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\njrid21544]. Screening was undertaken in two phases with\\ntwo reviewers reaching consensus on eligibility. Screening\\ncontinued until the predetermined sample size was reached.\\n\\n\\x0c110\\n\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nFor inclusion, all reviews must have included random-\\nized controlled trials (RCTs) and provided meta-analysis\\nfor at least one outcome. The MEDLINE search strategies\\nhad to be reported in enough detail to permit replication.\\nFifteen AHRQ evidence reports thought\\nlikely to have\\nimportant new evidence were selected and used to validate\\nthe updating methods used in the main cohort.\\n\\nCochrane reviews were selected to meet minimum qual-\\nity and relevance standards, as deﬁned by ACP Journal\\nClub [19]. We also required that the review had been up-\\ndated and that the text of both the original review and an\\nupdated version was available. The search for the original\\nreview must have included MEDLINE and at least one\\nother electronic bibliographic database and one or more\\nnondatabase method such as hand searching or checking\\nreference lists. Such comprehensive searching was likely\\nto identify most relevant\\nliterature and form a useful\\ntraining set of included examples. At least 10 RCTs or\\nquasi-RCTs must have been included in the original review\\nto give an adequate training set for SVM.\\n\\n2.2. Test searches\\n\\n2.2.1. Clinical query\\n\\nBoolean searches were developed by a librarian experi-\\nenced in systematic review searches using a protocol\\n(Appendix A at www.jclinepi.com). Search strategies were\\ndeliberately simple, usually consisting of two or three\\nMedical Subject Heading (MeSH) terms representing the\\npopulation and intervention of the review. This search\\nwas limited by a CQ ﬁlter to ‘‘therapy (best balance of\\nsensitivity\\n‘‘Randomized\\ncontrolled trial.pt. or\\nrandomized.mp. or placebo.mp.’’\\n[20]. Boolean searches were run in OVID MEDLINE.\\n\\nspeciﬁcity)’’\\n\\nthat\\n\\nand\\n\\nis,\\n\\n2.2.2. Related articles\\n\\nSearches using the PubMed similar articles feature were\\nperformed using the PubMed unique identiﬁers (PMID) of\\nthe three newest and three largest studies included in the\\noriginal review as seed articles. There was no replacement\\nin the event of overlap between the largest and newest sets,\\nor if one of those studies was not indexed in MEDLINE.\\n(All studies included in the original reviews were checked\\nfor indexing status in MEDLINE.) The resulting set was\\nlimited to the publication type RCT and date limited to\\nthe period since the search date of the original review.\\n\\n2.2.3. Support vector machine\\n\\nThe SVM searches used for this project have not been\\npreviously described so the methods will be reported in\\nmore detail. SVM is a well established and broadly applied\\nclassiﬁcation method from machine learning [21,22]. A\\nclassiﬁcation task is closely related to an information\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\nthe collection. As SVM is a supervised algorithm,\\nit\\n\\nrequires training examples with known labels. It then places\\nthese training examples in a high-dimensional vector space\\nin such a way that examples of one class are separated from\\nexamples of the other class with the greatest distance to the\\nseparating boundary. Predicting the class of a new instance\\nhappens by placing it in the same space and determining on\\nwhich side of the boundary it falls. The distance to the\\nboundary is an indication of the prediction conﬁdence and\\nis used to rank instances (documents) as is essential in in-\\nformation retrieval tasks or to provide a cutoff threshold.\\n\\nIn our setup, the training set consisted of search results\\nfrom the original review formed with included instances\\nbeing the studies included in the review (the relevant re-\\ntrievals) and excluded instances being the studies found\\nby the search used in the original review but excluded from\\nthe review (irrelevant retrievals). The new examples to be\\nclassiﬁed were articles indexed since the date of the search\\nperformed in the original review.\\n\\nSVM searches were run on MEDLINE data stored\\nlocally at\\nthe National Research Council of Canada.\\nMEDLINE was refreshed before running the searches.\\nThe MEDLINE records were represented as bags of fea-\\ntures, where features consisted of lowercased words and\\nword combinations (up to four words) from the title and ab-\\nstract ﬁelds, full MeSH terms, and contents of all other\\nﬁelds in the MEDLINE record. Features were uniformly\\nweighted. The SVM implementation used was SVM Light\\n[23] used with a linear kernel.\\n\\nThe approach was piloted with one AHRQ evidence\\nreport, and various conﬁgurations were tried, observing\\nthe placement of several known new relevant studies within\\nthe retrieval. The MEDLINE set was ﬁltered with the\\nrevised Highly Sensitive Search Strategy (HSSS) [24]. Pre-\\nliminary testing indicated that this ﬁlter would retrieve\\n99.1% of MEDLINE-indexed relevant new evidence from\\nthe larger cohort of journal-published systematic reviews\\n(n 5 72) and Cochrane reviews (n 5 27). This HSSS ﬁlter\\nwas added primarily to improve processing speed, relative\\nto running the classiﬁer against all of MEDLINE. With\\nthe ﬁlter, processing for one systematic review took about\\n10 minutes. Next, a coarse-grained ﬁlter was applied;\\nincluded and excluded instances were combined in a single\\nset labeled POS. Adjacent PubMed IDs (the next higher\\nPMID to each member of POS, as long as that higher num-\\nber was not itself a member of POS) were combined in a set\\nto represent WORLD. The model was trained on POS vs.\\nWORLD, using words and phrases from title, abstract,\\nauthor names, and journal name. This model was run\\nagainst the records passing the HSSS ﬁlter.\\n\\nFour different subsets of\\n\\nthe SVM retrieval were\\nexamineddone set included all records with a relevance\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\n(SVM200point5). Other sets consisted of all records with\\nrelevance scores of 0.95 or more (SVM95), 0.90 or more\\n(SVM90), and 0.80 or more (SVM80). PubMed ID and\\nrelevance scores were recorded, and PubMed IDs were\\n\\n\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\n111\\n\\nTable 1. Characteristics of the included reviews\\n\\nCochrane\\nreviews; N\\n\\nAHRQ evidence\\n\\nreports; N\\n\\nincorporated into the database of records for the reviewers\\nto screen. Only the results from SVM200point5 are re-\\nported here and are described as SVM.\\n\\n2.3. Determining performance of the test searches\\n\\nFor the updated Cochrane reviews, reference standard ar-\\nticles were those studies included in the updated review that\\nwere not in the original and which entered MEDLINE after\\nthe search date of the original review. For the AHRQ evi-\\ndence reports, the full retrieval set was screened. Records\\nfound relevant by the consensus of reviewers were consid-\\nered reference standard articles. Recall was the proportion\\nof reference standard articles identiﬁed by the search:\\n\\nNumber of reference standard articles found\\nTotal number of reference standard articles\\n\\nBioVenn software was used to analyze the overlap be-\\ntween the retrieval of relevant articles from the three\\nsearches and to create Venn diagrams [25].\\n\\nAll included reviews were classiﬁed into clinical area\\nbased on factors such as ISI journal classiﬁcation, the Co-\\nchrane Collaboration Review Group where the topic might\\nbe placed and the high level MeSH term under which the pop-\\nulation (condition) would be indexed. Performance of the\\nsearches in different clinical areas was displayed graphically\\nfor the searches both alone and in combinations, to allow ex-\\namination of differences in parallelism, level, and ﬂatness.\\n\\nPrecision is the proportion of all retrieved records that\\n\\nare relevant:\\n\\nNumber of reference standard articles found\\n\\nTotal number of records retrieved\\n\\nThe inverse of precision is the number needed to read to\\nﬁnd one eligible study, thus precision inﬂuences the work\\nof the review. Precision was calculated only for the AHRQ\\nevidence reviews. Because not all candidates retrieved by\\nthe Cochrane searches were assessed, precision could not\\nbe established in that sample.\\n\\n2.4. Determining stability of results over time\\n\\nThe CQ and related article searches, originally run in\\nMarch 2008, were repeated in February 2015 in the AHRQ\\ncohort. Recall of relevant items was compared with the\\noriginal retrievals to determine if any changes introduced\\nby National Library of Medicine might invalidate ﬁndings.\\nAs SVM conﬁguration is under the control of the investiga-\\ntors, it was not retested.\\n\\n3. Results\\n\\nSix Cochrane reviews and 10 AHRQ evidence reports\\nmet all inclusion criteria (Appendix B at www.jclinepi.\\ncom). Characteristics of the included reviews are shown\\nin Table 1.\\n\\n9a\\n1\\n3\\n\\n3\\n\\nd\\n1\\n\\nd\\n1\\n2\\n1\\n1\\n\\n1\\nd\\nd\\n\\nd\\n3\\n3\\n4\\n\\nCharacteristic\\n\\nTherapy evaluated\\n\\nMedications\\nMedical devices\\nProcedures\\n\\nClinical topic area\\n\\nCardiac and cardiovascular\\n\\nsystems\\n\\nCritical care\\nEndocrinology and\\n\\nmetabolism\\n\\nInfectious disease\\nClinical neurology\\nObstetrics and gynecology\\nOncology\\nPeripheral vascular\\n\\ndiseases\\nPsychiatry\\nRespiratory systems\\nUrology and nephrology\\n\\nPublication period\\n\\nMarch 1997eApril 1999\\nMay 1999eJune 2001\\nJuly 2001eAugust 2003\\nSeptember 2003e\\nDecember 2005\\n\\nMedian included trials\\n\\nMedian included\\n\\nparticipants\\n\\nMEDLINE coverage of\\n\\n5\\n1\\nd\\n\\nd\\n\\n1\\nd\\n\\n3\\nd\\nd\\nd\\nd\\n\\nd\\n1\\n1\\n\\n3\\n1\\n2\\nd\\n\\n17 (IQR, 14e20)\\n\\n96 (IQR,\\n\\n31.75e121.5)\\n8,679 (IQR,\\n22,830 (IQR, 14,\\n4,085e50,109)\\n172e49,687)\\n99/107 (92.5%) 969/980 (98.8%)\\n\\nincluded articles\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\n\\nity; IQR, interquartile range.\\n\\na Some AHRQ reviews included more than one class of therapy.\\n\\nRecall of new relevant studies is shown in Table 2. The\\nupdated Cochrane reviews had 20 new studies identiﬁed by\\nthe review authors and added in the updates. Recall of these\\n20 ranged from 1.00 for the related articles method to a low\\nof 0.80 CQs (Table 2). Our team identiﬁed 277 new studies\\nas relevant for inclusion in the 10 evidence reports that\\nwere updated. All test searches showed lower recall for this\\ncohort than for the Cochrane reviews and here the CQ out-\\nperformed both ranking searches (Table 2).\\n\\n3.1. Recall of CQ combined with a ranking method\\n\\nOf the 297 new relevant studies identiﬁed, the combina-\\ntion of CQ and related articles searches identiﬁed 270\\n(overall\\nin\\nthe Cochrane cohort and 0.90 (250/277) in the AHRQ\\ncohort.\\n\\nrecall of 0.91). Recall was 1.00 (20/20)\\n\\nThe combination of CQ and SVM identiﬁed 263 studies\\n(overall recall of 0.89). Recall was 1.00 (20/20) in the\\nCochrane cohort and 0.88 (243/277) in the AHRQ cohort.\\nWhen all three methods were used together, recall was\\n0.997 with 296 of the 297 studies identiﬁed.\\n\\n\\x0c',\n",
       "  'Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nComplementary approaches to searching MEDLINE may be\\n\\nsufﬁcient for updating systematic reviews\\n\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\n\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\n\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\n\\n0R6, Canada\\n\\nUnited Kingdom\\n\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\n\\nAccepted 7 March 2016; Published online 11 March 2016\\n\\nAbstract\\n\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\n\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\nexamined for each method.\\n\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\ncombined. Related articles showed least stability over time.\\n\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\n\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\n\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\n\\n1. Introduction\\n\\nSystematic review searches need to have high recall.\\nMechanisms to achieve this usually include expansive\\nBoolean searches of multiple databases. This approach\\nleads to long development times for the searches [1], neces-\\nsitates accessing multiple sources that may not be acces-\\nsible in some institutions [2], and entails time-consuming\\nremoval of duplicate records for articles indexed in more\\nthan one of the databases searched [3].\\n\\nMEDLINE gives excellent coverage of most biomed-\\nical topics, in particular, intervention studies. However,\\n\\nConﬂict of interest: None.\\nFunding: Some of this work was funded by the US Agency for Health-\\ncare Research and Quality, Department of Health and Human Services\\n(contract no. 290-02-0021).\\n\\n* Corresponding author. Tel.: 613-737-7600.\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\n\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\n\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\nlished that only about half the studies included in system-\\natic reviews were identiﬁed through MEDLINE. Recent\\nresearch has demonstrated that a much higher percentage\\nis present in MEDLINE, but sometimes their retrieval is\\nproblematic [5e8].\\n\\nSuccessful retrieval through a Boolean search is oper-\\nator dependent, with search performance being inﬂuenced\\nby skill of the indexer and the searcher. The search of\\nmultiple databases can therefore be helpful. The target re-\\ncords may be indexed differently in the second, third, or\\nsubsequent source searched, increasing the probability\\nof a match between the search terms entered and the in-\\ndexing of the additional records. A text search, querying\\nterms appearing in the title or abstract, may help improve\\nretrieval, as may the use of indexing terms that are broad-\\ner (less speciﬁc) than, or related to, the single best index-\\ning term. These tactics on the part of the searcher lead\\n\\n\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\n109\\n\\nWhat is new?\\n\\nKey ﬁndings\\n(cid:1) Searches using known relevant studies and the\\nsimilar articles feature of PubMed will identify\\nand rank additional articles of potential relevance.\\n(cid:1) For a given question, if the Boolean search has low\\nrecall, the ranking search tends to have higher\\nrecall, and vice versa. The two approaches comple-\\nment each other.\\n\\n(cid:1) The precision of\\n\\nmethod appears better\\nexhaustive Boolean searches.\\n\\nthis complementary paired\\nthan the precision of\\n\\nWhat this adds to what was known?\\n(cid:1) The paired approach performed well regardless of\\nwhich of two tested similarity searches were used.\\nIt is the use of independent retrieval methods that is\\nimportant.\\n\\nWhat is the implication and what should change\\nnow?\\n(cid:1) Using the simple and universally available PubMed\\nsimilar feature makes this paired approach prac-\\ntical for most systematic review teams.\\n\\n(cid:1) If the paired complementary approach is used, the\\nrecall may be sufﬁcient to consider using only\\nMEDLINE.\\n\\nto large retrievals with low precision [9]. These resulting\\nproblems may be particularly challenging for complex or\\nnewly emerging interventions with highly variant terminol-\\nogy, where alternatives to traditional Boolean searches have\\nbeen sought [10].\\n\\nMost recent systematic review information retrieval\\nresearch has focused on text mining approaches [11e14].\\nThese approaches often harvest an intentionally overinclu-\\nsive set of records and then use machine learning, similarity\\nranking and other techniques to reﬁne the set to identify the\\nmaterial most likely to be relevant, thereby reducing human\\nscreening effort. These methods show promise, but are not\\nyet widely available to reviewers.\\n\\nWe examined one method, support vector machine\\n(SVM), and compared it with a simple and readily available\\nmethod based on the PubMed similar articles feature. We\\ncall this method related articles to distinguish the method\\nfrom the similar articles feature itself. We paired both\\nwith a focused Boolean search within MEDLINE. We\\ntested this approach in an updating context where studies\\nincluded in the original review comprise the reference\\nstandard for SVM and seed articles for the PubMed-\\n\\nrelated articles search. We therefore sought to determine\\nif a focused Boolean search paired with one of the search\\nmethods that does not depend on operator skill could pro-\\nvide consistently complete retrieval of relevant new studies.\\nComparison of a number of searches, including two\\ntested here, has been previously reported [15]. In this cur-\\nrent article, two of the most successful methods in a larger\\nsample of 72 journal-published systematic reviews, clinical\\nquery (CQ), and PubMed-related articles are tested in a\\ncohort of six updated Cochrane reviews, as well as in a pre-\\nviously unreported sample of 10 Agency for Healthcare\\nResearch and Quality (AHRQ) evidence reports. The\\nCochrane reviews provide a true gold standard as updates\\nwere made by the review team based on evidence identiﬁed\\nthrough comprehensive searches; however, the new relevant\\nstudies proved fairly easy to ﬁnd. The replication in the\\nAHRQ cohort, of more complex interventions, provided a\\nmeans to validate the generalizability of the approach\\n[16]. All records were assessed for eligibility by two re-\\nviewers, and this complete screening allowed the precision\\nof the methods to be calculated for the ﬁrst time.\\n\\nOther research [17,18] suggests that searches using the\\nPubMed similar articles feature are effective in increasing\\nrecall of relevant items for reviews or more general clinical\\nsearching when combined with a Boolean-type search of\\nMEDLINE. We tested an additional search method, SVM,\\nin the Cochrane and AHRQ samples to permit comparison\\nwith our PubMed-related articles search and assess whether\\nthe complementary effect generalized beyond the PubMed\\nsimilar articles method.\\n\\nThe aim of this article was to test whether the combined\\napproach of a focused Boolean search paired with a second\\nsearch using the similar articles feature of PubMed or SVM\\ncan yield high recall with reasonable precision.\\n\\n2. Methods\\n\\n2.1. Formation of the study cohorts\\n\\nThis analysis uses a data set created for an updating\\nstudy sponsored by AHRQ [16]. Methods for the selection\\nof the cohorts, search approaches tested, and rigorous\\nmechanisms to screen search results for relevance have\\nbeen previously reported, along with the criteria to deter-\\nmine if a review was in need of update [16].\\n\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\nsearch of the ACP Journal Club database (Ovid) using the\\nstrategy:\\n\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\nwith commentary.\\n\\nAHRQ reports were identiﬁed through the PubMed\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\njrid21544]. Screening was undertaken in two phases with\\ntwo reviewers reaching consensus on eligibility. Screening\\ncontinued until the predetermined sample size was reached.\\n\\n\\x0c110\\n\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nFor inclusion, all reviews must have included random-\\nized controlled trials (RCTs) and provided meta-analysis\\nfor at least one outcome. The MEDLINE search strategies\\nhad to be reported in enough detail to permit replication.\\nFifteen AHRQ evidence reports thought\\nlikely to have\\nimportant new evidence were selected and used to validate\\nthe updating methods used in the main cohort.\\n\\nCochrane reviews were selected to meet minimum qual-\\nity and relevance standards, as deﬁned by ACP Journal\\nClub [19]. We also required that the review had been up-\\ndated and that the text of both the original review and an\\nupdated version was available. The search for the original\\nreview must have included MEDLINE and at least one\\nother electronic bibliographic database and one or more\\nnondatabase method such as hand searching or checking\\nreference lists. Such comprehensive searching was likely\\nto identify most relevant\\nliterature and form a useful\\ntraining set of included examples. At least 10 RCTs or\\nquasi-RCTs must have been included in the original review\\nto give an adequate training set for SVM.\\n\\n2.2. Test searches\\n\\n2.2.1. Clinical query\\n\\nBoolean searches were developed by a librarian experi-\\nenced in systematic review searches using a protocol\\n(Appendix A at www.jclinepi.com). Search strategies were\\ndeliberately simple, usually consisting of two or three\\nMedical Subject Heading (MeSH) terms representing the\\npopulation and intervention of the review. This search\\nwas limited by a CQ ﬁlter to ‘‘therapy (best balance of\\nsensitivity\\n‘‘Randomized\\ncontrolled trial.pt. or\\nrandomized.mp. or placebo.mp.’’\\n[20]. Boolean searches were run in OVID MEDLINE.\\n\\nspeciﬁcity)’’\\n\\nthat\\n\\nand\\n\\nis,\\n\\n2.2.2. Related articles\\n\\nSearches using the PubMed similar articles feature were\\nperformed using the PubMed unique identiﬁers (PMID) of\\nthe three newest and three largest studies included in the\\noriginal review as seed articles. There was no replacement\\nin the event of overlap between the largest and newest sets,\\nor if one of those studies was not indexed in MEDLINE.\\n(All studies included in the original reviews were checked\\nfor indexing status in MEDLINE.) The resulting set was\\nlimited to the publication type RCT and date limited to\\nthe period since the search date of the original review.\\n\\n2.2.3. Support vector machine\\n\\nThe SVM searches used for this project have not been\\npreviously described so the methods will be reported in\\nmore detail. SVM is a well established and broadly applied\\nclassiﬁcation method from machine learning [21,22]. A\\nclassiﬁcation task is closely related to an information\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\nthe collection. As SVM is a supervised algorithm,\\nit\\n\\nrequires training examples with known labels. It then places\\nthese training examples in a high-dimensional vector space\\nin such a way that examples of one class are separated from\\nexamples of the other class with the greatest distance to the\\nseparating boundary. Predicting the class of a new instance\\nhappens by placing it in the same space and determining on\\nwhich side of the boundary it falls. The distance to the\\nboundary is an indication of the prediction conﬁdence and\\nis used to rank instances (documents) as is essential in in-\\nformation retrieval tasks or to provide a cutoff threshold.\\n\\nIn our setup, the training set consisted of search results\\nfrom the original review formed with included instances\\nbeing the studies included in the review (the relevant re-\\ntrievals) and excluded instances being the studies found\\nby the search used in the original review but excluded from\\nthe review (irrelevant retrievals). The new examples to be\\nclassiﬁed were articles indexed since the date of the search\\nperformed in the original review.\\n\\nSVM searches were run on MEDLINE data stored\\nlocally at\\nthe National Research Council of Canada.\\nMEDLINE was refreshed before running the searches.\\nThe MEDLINE records were represented as bags of fea-\\ntures, where features consisted of lowercased words and\\nword combinations (up to four words) from the title and ab-\\nstract ﬁelds, full MeSH terms, and contents of all other\\nﬁelds in the MEDLINE record. Features were uniformly\\nweighted. The SVM implementation used was SVM Light\\n[23] used with a linear kernel.\\n\\nThe approach was piloted with one AHRQ evidence\\nreport, and various conﬁgurations were tried, observing\\nthe placement of several known new relevant studies within\\nthe retrieval. The MEDLINE set was ﬁltered with the\\nrevised Highly Sensitive Search Strategy (HSSS) [24]. Pre-\\nliminary testing indicated that this ﬁlter would retrieve\\n99.1% of MEDLINE-indexed relevant new evidence from\\nthe larger cohort of journal-published systematic reviews\\n(n 5 72) and Cochrane reviews (n 5 27). This HSSS ﬁlter\\nwas added primarily to improve processing speed, relative\\nto running the classiﬁer against all of MEDLINE. With\\nthe ﬁlter, processing for one systematic review took about\\n10 minutes. Next, a coarse-grained ﬁlter was applied;\\nincluded and excluded instances were combined in a single\\nset labeled POS. Adjacent PubMed IDs (the next higher\\nPMID to each member of POS, as long as that higher num-\\nber was not itself a member of POS) were combined in a set\\nto represent WORLD. The model was trained on POS vs.\\nWORLD, using words and phrases from title, abstract,\\nauthor names, and journal name. This model was run\\nagainst the records passing the HSSS ﬁlter.\\n\\nFour different subsets of\\n\\nthe SVM retrieval were\\nexamineddone set included all records with a relevance\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\n(SVM200point5). Other sets consisted of all records with\\nrelevance scores of 0.95 or more (SVM95), 0.90 or more\\n(SVM90), and 0.80 or more (SVM80). PubMed ID and\\nrelevance scores were recorded, and PubMed IDs were\\n\\n\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\n111\\n\\nTable 1. Characteristics of the included reviews\\n\\nCochrane\\nreviews; N\\n\\nAHRQ evidence\\n\\nreports; N\\n\\nincorporated into the database of records for the reviewers\\nto screen. Only the results from SVM200point5 are re-\\nported here and are described as SVM.\\n\\n2.3. Determining performance of the test searches\\n\\nFor the updated Cochrane reviews, reference standard ar-\\nticles were those studies included in the updated review that\\nwere not in the original and which entered MEDLINE after\\nthe search date of the original review. For the AHRQ evi-\\ndence reports, the full retrieval set was screened. Records\\nfound relevant by the consensus of reviewers were consid-\\nered reference standard articles. Recall was the proportion\\nof reference standard articles identiﬁed by the search:\\n\\nNumber of reference standard articles found\\nTotal number of reference standard articles\\n\\nBioVenn software was used to analyze the overlap be-\\ntween the retrieval of relevant articles from the three\\nsearches and to create Venn diagrams [25].\\n\\nAll included reviews were classiﬁed into clinical area\\nbased on factors such as ISI journal classiﬁcation, the Co-\\nchrane Collaboration Review Group where the topic might\\nbe placed and the high level MeSH term under which the pop-\\nulation (condition) would be indexed. Performance of the\\nsearches in different clinical areas was displayed graphically\\nfor the searches both alone and in combinations, to allow ex-\\namination of differences in parallelism, level, and ﬂatness.\\n\\nPrecision is the proportion of all retrieved records that\\n\\nare relevant:\\n\\nNumber of reference standard articles found\\n\\nTotal number of records retrieved\\n\\nThe inverse of precision is the number needed to read to\\nﬁnd one eligible study, thus precision inﬂuences the work\\nof the review. Precision was calculated only for the AHRQ\\nevidence reviews. Because not all candidates retrieved by\\nthe Cochrane searches were assessed, precision could not\\nbe established in that sample.\\n\\n2.4. Determining stability of results over time\\n\\nThe CQ and related article searches, originally run in\\nMarch 2008, were repeated in February 2015 in the AHRQ\\ncohort. Recall of relevant items was compared with the\\noriginal retrievals to determine if any changes introduced\\nby National Library of Medicine might invalidate ﬁndings.\\nAs SVM conﬁguration is under the control of the investiga-\\ntors, it was not retested.\\n\\n3. Results\\n\\nSix Cochrane reviews and 10 AHRQ evidence reports\\nmet all inclusion criteria (Appendix B at www.jclinepi.\\ncom). Characteristics of the included reviews are shown\\nin Table 1.\\n\\n9a\\n1\\n3\\n\\n3\\n\\nd\\n1\\n\\nd\\n1\\n2\\n1\\n1\\n\\n1\\nd\\nd\\n\\nd\\n3\\n3\\n4\\n\\nCharacteristic\\n\\nTherapy evaluated\\n\\nMedications\\nMedical devices\\nProcedures\\n\\nClinical topic area\\n\\nCardiac and cardiovascular\\n\\nsystems\\n\\nCritical care\\nEndocrinology and\\n\\nmetabolism\\n\\nInfectious disease\\nClinical neurology\\nObstetrics and gynecology\\nOncology\\nPeripheral vascular\\n\\ndiseases\\nPsychiatry\\nRespiratory systems\\nUrology and nephrology\\n\\nPublication period\\n\\nMarch 1997eApril 1999\\nMay 1999eJune 2001\\nJuly 2001eAugust 2003\\nSeptember 2003e\\nDecember 2005\\n\\nMedian included trials\\n\\nMedian included\\n\\nparticipants\\n\\nMEDLINE coverage of\\n\\n5\\n1\\nd\\n\\nd\\n\\n1\\nd\\n\\n3\\nd\\nd\\nd\\nd\\n\\nd\\n1\\n1\\n\\n3\\n1\\n2\\nd\\n\\n17 (IQR, 14e20)\\n\\n96 (IQR,\\n\\n31.75e121.5)\\n8,679 (IQR,\\n22,830 (IQR, 14,\\n4,085e50,109)\\n172e49,687)\\n99/107 (92.5%) 969/980 (98.8%)\\n\\nincluded articles\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\n\\nity; IQR, interquartile range.\\n\\na Some AHRQ reviews included more than one class of therapy.\\n\\nRecall of new relevant studies is shown in Table 2. The\\nupdated Cochrane reviews had 20 new studies identiﬁed by\\nthe review authors and added in the updates. Recall of these\\n20 ranged from 1.00 for the related articles method to a low\\nof 0.80 CQs (Table 2). Our team identiﬁed 277 new studies\\nas relevant for inclusion in the 10 evidence reports that\\nwere updated. All test searches showed lower recall for this\\ncohort than for the Cochrane reviews and here the CQ out-\\nperformed both ranking searches (Table 2).\\n\\n3.1. Recall of CQ combined with a ranking method\\n\\nOf the 297 new relevant studies identiﬁed, the combina-\\ntion of CQ and related articles searches identiﬁed 270\\n(overall\\nin\\nthe Cochrane cohort and 0.90 (250/277) in the AHRQ\\ncohort.\\n\\nrecall of 0.91). Recall was 1.00 (20/20)\\n\\nThe combination of CQ and SVM identiﬁed 263 studies\\n(overall recall of 0.89). Recall was 1.00 (20/20) in the\\nCochrane cohort and 0.88 (243/277) in the AHRQ cohort.\\nWhen all three methods were used together, recall was\\n0.997 with 296 of the 297 studies identiﬁed.\\n\\n\\x0c112\\n\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nTable 2. Recall of eligible studies by the search methods\\n\\nCochrane reviews\\n\\nAHRQ evidence\\n\\nreports\\n\\nRetrieval method\\n\\nClinical query\\nRelated articles\\nSVM\\nTotal\\n\\nN\\n\\n16\\n20\\n19\\n20\\n\\nRecall\\n\\n0.80\\n1.00\\n0.95\\n\\nN\\n\\n188\\n176\\n129\\n277\\n\\nRecall\\n\\n0.68\\n0.64\\n0.47\\n\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\n\\nity; SVM, support vector machine.\\n\\nAcross the two cohorts, the overlap and unique component from\\nthe retrieval of relevant records by the test searches was examined\\n(Fig. 1).\\n\\n3.2. Consistency across clinical areas\\n\\nThirteen clinical areas were represented in the larger\\nstudy that included 72 journal-published reviews, but only\\neight of those clinical areas had two or more new relevant\\nstudies in the smaller Cochrane and AHRQ cohorts. Fig. 2\\nshows recall of new studies by the three types of searches,\\nfor these eight clinical areas.\\n\\nCombined recall of the CQ Boolean search paired with a\\nranking search is shown in Fig. 3. The combination showed\\ncomplete recall of relevant new studies in three of eight\\nareas when CQ was paired with related articles and recall\\nof 0.67 or higher in all areas. Four of eight clinical areas\\nhad complete recall for the combination of CQ and SVM,\\nand recall was 0.80 or higher in all areas. Even for periph-\\neral vascular disease, where all three searches performed\\n\\nFig. 2. Recall of new studies by clinical area for each search method.\\nAbbreviations: SVM, support vector machine.\\n\\nfairly poorly individually, recall was 0.76 for the CQ/\\nrelated articles and 0.90 in the CQ/SVM pairing.\\n\\n3.3. Search precision\\n\\nPrecision, across the 10 AHRQ reviews, was 0.11 for the\\nCQ, 0.22 for related articles, and 0.19 for SVM (Table 3).\\n\\nFig. 1. Overlapping and unique retrieval of relevant studies by each\\nsearch method. The size of the circle is proportional to the size of\\nthe retrieval, but the overlap and unique portions are approximations.\\nExact ﬁgures for the components are labeled. Abbreviations: SVM,\\nsupport vector machine.\\n\\nFig. 3. Recall of new studies by clinical area for search methods in\\ncombination. Abbreviations: CQ, clinical query; SVM, support vector\\nmachine.\\n\\n\\x0c',\n",
       "  'Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nComplementary approaches to searching MEDLINE may be\\n\\nsufﬁcient for updating systematic reviews\\n\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\n\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\n\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\n\\n0R6, Canada\\n\\nUnited Kingdom\\n\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\n\\nAccepted 7 March 2016; Published online 11 March 2016\\n\\nAbstract\\n\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\n\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\nexamined for each method.\\n\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\ncombined. Related articles showed least stability over time.\\n\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\n\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\n\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\n\\n1. Introduction\\n\\nSystematic review searches need to have high recall.\\nMechanisms to achieve this usually include expansive\\nBoolean searches of multiple databases. This approach\\nleads to long development times for the searches [1], neces-\\nsitates accessing multiple sources that may not be acces-\\nsible in some institutions [2], and entails time-consuming\\nremoval of duplicate records for articles indexed in more\\nthan one of the databases searched [3].\\n\\nMEDLINE gives excellent coverage of most biomed-\\nical topics, in particular, intervention studies. However,\\n\\nConﬂict of interest: None.\\nFunding: Some of this work was funded by the US Agency for Health-\\ncare Research and Quality, Department of Health and Human Services\\n(contract no. 290-02-0021).\\n\\n* Corresponding author. Tel.: 613-737-7600.\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\n\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\n\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\nlished that only about half the studies included in system-\\natic reviews were identiﬁed through MEDLINE. Recent\\nresearch has demonstrated that a much higher percentage\\nis present in MEDLINE, but sometimes their retrieval is\\nproblematic [5e8].\\n\\nSuccessful retrieval through a Boolean search is oper-\\nator dependent, with search performance being inﬂuenced\\nby skill of the indexer and the searcher. The search of\\nmultiple databases can therefore be helpful. The target re-\\ncords may be indexed differently in the second, third, or\\nsubsequent source searched, increasing the probability\\nof a match between the search terms entered and the in-\\ndexing of the additional records. A text search, querying\\nterms appearing in the title or abstract, may help improve\\nretrieval, as may the use of indexing terms that are broad-\\ner (less speciﬁc) than, or related to, the single best index-\\ning term. These tactics on the part of the searcher lead\\n\\n\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\n109\\n\\nWhat is new?\\n\\nKey ﬁndings\\n(cid:1) Searches using known relevant studies and the\\nsimilar articles feature of PubMed will identify\\nand rank additional articles of potential relevance.\\n(cid:1) For a given question, if the Boolean search has low\\nrecall, the ranking search tends to have higher\\nrecall, and vice versa. The two approaches comple-\\nment each other.\\n\\n(cid:1) The precision of\\n\\nmethod appears better\\nexhaustive Boolean searches.\\n\\nthis complementary paired\\nthan the precision of\\n\\nWhat this adds to what was known?\\n(cid:1) The paired approach performed well regardless of\\nwhich of two tested similarity searches were used.\\nIt is the use of independent retrieval methods that is\\nimportant.\\n\\nWhat is the implication and what should change\\nnow?\\n(cid:1) Using the simple and universally available PubMed\\nsimilar feature makes this paired approach prac-\\ntical for most systematic review teams.\\n\\n(cid:1) If the paired complementary approach is used, the\\nrecall may be sufﬁcient to consider using only\\nMEDLINE.\\n\\nto large retrievals with low precision [9]. These resulting\\nproblems may be particularly challenging for complex or\\nnewly emerging interventions with highly variant terminol-\\nogy, where alternatives to traditional Boolean searches have\\nbeen sought [10].\\n\\nMost recent systematic review information retrieval\\nresearch has focused on text mining approaches [11e14].\\nThese approaches often harvest an intentionally overinclu-\\nsive set of records and then use machine learning, similarity\\nranking and other techniques to reﬁne the set to identify the\\nmaterial most likely to be relevant, thereby reducing human\\nscreening effort. These methods show promise, but are not\\nyet widely available to reviewers.\\n\\nWe examined one method, support vector machine\\n(SVM), and compared it with a simple and readily available\\nmethod based on the PubMed similar articles feature. We\\ncall this method related articles to distinguish the method\\nfrom the similar articles feature itself. We paired both\\nwith a focused Boolean search within MEDLINE. We\\ntested this approach in an updating context where studies\\nincluded in the original review comprise the reference\\nstandard for SVM and seed articles for the PubMed-\\n\\nrelated articles search. We therefore sought to determine\\nif a focused Boolean search paired with one of the search\\nmethods that does not depend on operator skill could pro-\\nvide consistently complete retrieval of relevant new studies.\\nComparison of a number of searches, including two\\ntested here, has been previously reported [15]. In this cur-\\nrent article, two of the most successful methods in a larger\\nsample of 72 journal-published systematic reviews, clinical\\nquery (CQ), and PubMed-related articles are tested in a\\ncohort of six updated Cochrane reviews, as well as in a pre-\\nviously unreported sample of 10 Agency for Healthcare\\nResearch and Quality (AHRQ) evidence reports. The\\nCochrane reviews provide a true gold standard as updates\\nwere made by the review team based on evidence identiﬁed\\nthrough comprehensive searches; however, the new relevant\\nstudies proved fairly easy to ﬁnd. The replication in the\\nAHRQ cohort, of more complex interventions, provided a\\nmeans to validate the generalizability of the approach\\n[16]. All records were assessed for eligibility by two re-\\nviewers, and this complete screening allowed the precision\\nof the methods to be calculated for the ﬁrst time.\\n\\nOther research [17,18] suggests that searches using the\\nPubMed similar articles feature are effective in increasing\\nrecall of relevant items for reviews or more general clinical\\nsearching when combined with a Boolean-type search of\\nMEDLINE. We tested an additional search method, SVM,\\nin the Cochrane and AHRQ samples to permit comparison\\nwith our PubMed-related articles search and assess whether\\nthe complementary effect generalized beyond the PubMed\\nsimilar articles method.\\n\\nThe aim of this article was to test whether the combined\\napproach of a focused Boolean search paired with a second\\nsearch using the similar articles feature of PubMed or SVM\\ncan yield high recall with reasonable precision.\\n\\n2. Methods\\n\\n2.1. Formation of the study cohorts\\n\\nThis analysis uses a data set created for an updating\\nstudy sponsored by AHRQ [16]. Methods for the selection\\nof the cohorts, search approaches tested, and rigorous\\nmechanisms to screen search results for relevance have\\nbeen previously reported, along with the criteria to deter-\\nmine if a review was in need of update [16].\\n\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\nsearch of the ACP Journal Club database (Ovid) using the\\nstrategy:\\n\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\nwith commentary.\\n\\nAHRQ reports were identiﬁed through the PubMed\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\njrid21544]. Screening was undertaken in two phases with\\ntwo reviewers reaching consensus on eligibility. Screening\\ncontinued until the predetermined sample size was reached.\\n\\n\\x0c110\\n\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nFor inclusion, all reviews must have included random-\\nized controlled trials (RCTs) and provided meta-analysis\\nfor at least one outcome. The MEDLINE search strategies\\nhad to be reported in enough detail to permit replication.\\nFifteen AHRQ evidence reports thought\\nlikely to have\\nimportant new evidence were selected and used to validate\\nthe updating methods used in the main cohort.\\n\\nCochrane reviews were selected to meet minimum qual-\\nity and relevance standards, as deﬁned by ACP Journal\\nClub [19]. We also required that the review had been up-\\ndated and that the text of both the original review and an\\nupdated version was available. The search for the original\\nreview must have included MEDLINE and at least one\\nother electronic bibliographic database and one or more\\nnondatabase method such as hand searching or checking\\nreference lists. Such comprehensive searching was likely\\nto identify most relevant\\nliterature and form a useful\\ntraining set of included examples. At least 10 RCTs or\\nquasi-RCTs must have been included in the original review\\nto give an adequate training set for SVM.\\n\\n2.2. Test searches\\n\\n2.2.1. Clinical query\\n\\nBoolean searches were developed by a librarian experi-\\nenced in systematic review searches using a protocol\\n(Appendix A at www.jclinepi.com). Search strategies were\\ndeliberately simple, usually consisting of two or three\\nMedical Subject Heading (MeSH) terms representing the\\npopulation and intervention of the review. This search\\nwas limited by a CQ ﬁlter to ‘‘therapy (best balance of\\nsensitivity\\n‘‘Randomized\\ncontrolled trial.pt. or\\nrandomized.mp. or placebo.mp.’’\\n[20]. Boolean searches were run in OVID MEDLINE.\\n\\nspeciﬁcity)’’\\n\\nthat\\n\\nand\\n\\nis,\\n\\n2.2.2. Related articles\\n\\nSearches using the PubMed similar articles feature were\\nperformed using the PubMed unique identiﬁers (PMID) of\\nthe three newest and three largest studies included in the\\noriginal review as seed articles. There was no replacement\\nin the event of overlap between the largest and newest sets,\\nor if one of those studies was not indexed in MEDLINE.\\n(All studies included in the original reviews were checked\\nfor indexing status in MEDLINE.) The resulting set was\\nlimited to the publication type RCT and date limited to\\nthe period since the search date of the original review.\\n\\n2.2.3. Support vector machine\\n\\nThe SVM searches used for this project have not been\\npreviously described so the methods will be reported in\\nmore detail. SVM is a well established and broadly applied\\nclassiﬁcation method from machine learning [21,22]. A\\nclassiﬁcation task is closely related to an information\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\nthe collection. As SVM is a supervised algorithm,\\nit\\n\\nrequires training examples with known labels. It then places\\nthese training examples in a high-dimensional vector space\\nin such a way that examples of one class are separated from\\nexamples of the other class with the greatest distance to the\\nseparating boundary. Predicting the class of a new instance\\nhappens by placing it in the same space and determining on\\nwhich side of the boundary it falls. The distance to the\\nboundary is an indication of the prediction conﬁdence and\\nis used to rank instances (documents) as is essential in in-\\nformation retrieval tasks or to provide a cutoff threshold.\\n\\nIn our setup, the training set consisted of search results\\nfrom the original review formed with included instances\\nbeing the studies included in the review (the relevant re-\\ntrievals) and excluded instances being the studies found\\nby the search used in the original review but excluded from\\nthe review (irrelevant retrievals). The new examples to be\\nclassiﬁed were articles indexed since the date of the search\\nperformed in the original review.\\n\\nSVM searches were run on MEDLINE data stored\\nlocally at\\nthe National Research Council of Canada.\\nMEDLINE was refreshed before running the searches.\\nThe MEDLINE records were represented as bags of fea-\\ntures, where features consisted of lowercased words and\\nword combinations (up to four words) from the title and ab-\\nstract ﬁelds, full MeSH terms, and contents of all other\\nﬁelds in the MEDLINE record. Features were uniformly\\nweighted. The SVM implementation used was SVM Light\\n[23] used with a linear kernel.\\n\\nThe approach was piloted with one AHRQ evidence\\nreport, and various conﬁgurations were tried, observing\\nthe placement of several known new relevant studies within\\nthe retrieval. The MEDLINE set was ﬁltered with the\\nrevised Highly Sensitive Search Strategy (HSSS) [24]. Pre-\\nliminary testing indicated that this ﬁlter would retrieve\\n99.1% of MEDLINE-indexed relevant new evidence from\\nthe larger cohort of journal-published systematic reviews\\n(n 5 72) and Cochrane reviews (n 5 27). This HSSS ﬁlter\\nwas added primarily to improve processing speed, relative\\nto running the classiﬁer against all of MEDLINE. With\\nthe ﬁlter, processing for one systematic review took about\\n10 minutes. Next, a coarse-grained ﬁlter was applied;\\nincluded and excluded instances were combined in a single\\nset labeled POS. Adjacent PubMed IDs (the next higher\\nPMID to each member of POS, as long as that higher num-\\nber was not itself a member of POS) were combined in a set\\nto represent WORLD. The model was trained on POS vs.\\nWORLD, using words and phrases from title, abstract,\\nauthor names, and journal name. This model was run\\nagainst the records passing the HSSS ﬁlter.\\n\\nFour different subsets of\\n\\nthe SVM retrieval were\\nexamineddone set included all records with a relevance\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\n(SVM200point5). Other sets consisted of all records with\\nrelevance scores of 0.95 or more (SVM95), 0.90 or more\\n(SVM90), and 0.80 or more (SVM80). PubMed ID and\\nrelevance scores were recorded, and PubMed IDs were\\n\\n\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\n111\\n\\nTable 1. Characteristics of the included reviews\\n\\nCochrane\\nreviews; N\\n\\nAHRQ evidence\\n\\nreports; N\\n\\nincorporated into the database of records for the reviewers\\nto screen. Only the results from SVM200point5 are re-\\nported here and are described as SVM.\\n\\n2.3. Determining performance of the test searches\\n\\nFor the updated Cochrane reviews, reference standard ar-\\nticles were those studies included in the updated review that\\nwere not in the original and which entered MEDLINE after\\nthe search date of the original review. For the AHRQ evi-\\ndence reports, the full retrieval set was screened. Records\\nfound relevant by the consensus of reviewers were consid-\\nered reference standard articles. Recall was the proportion\\nof reference standard articles identiﬁed by the search:\\n\\nNumber of reference standard articles found\\nTotal number of reference standard articles\\n\\nBioVenn software was used to analyze the overlap be-\\ntween the retrieval of relevant articles from the three\\nsearches and to create Venn diagrams [25].\\n\\nAll included reviews were classiﬁed into clinical area\\nbased on factors such as ISI journal classiﬁcation, the Co-\\nchrane Collaboration Review Group where the topic might\\nbe placed and the high level MeSH term under which the pop-\\nulation (condition) would be indexed. Performance of the\\nsearches in different clinical areas was displayed graphically\\nfor the searches both alone and in combinations, to allow ex-\\namination of differences in parallelism, level, and ﬂatness.\\n\\nPrecision is the proportion of all retrieved records that\\n\\nare relevant:\\n\\nNumber of reference standard articles found\\n\\nTotal number of records retrieved\\n\\nThe inverse of precision is the number needed to read to\\nﬁnd one eligible study, thus precision inﬂuences the work\\nof the review. Precision was calculated only for the AHRQ\\nevidence reviews. Because not all candidates retrieved by\\nthe Cochrane searches were assessed, precision could not\\nbe established in that sample.\\n\\n2.4. Determining stability of results over time\\n\\nThe CQ and related article searches, originally run in\\nMarch 2008, were repeated in February 2015 in the AHRQ\\ncohort. Recall of relevant items was compared with the\\noriginal retrievals to determine if any changes introduced\\nby National Library of Medicine might invalidate ﬁndings.\\nAs SVM conﬁguration is under the control of the investiga-\\ntors, it was not retested.\\n\\n3. Results\\n\\nSix Cochrane reviews and 10 AHRQ evidence reports\\nmet all inclusion criteria (Appendix B at www.jclinepi.\\ncom). Characteristics of the included reviews are shown\\nin Table 1.\\n\\n9a\\n1\\n3\\n\\n3\\n\\nd\\n1\\n\\nd\\n1\\n2\\n1\\n1\\n\\n1\\nd\\nd\\n\\nd\\n3\\n3\\n4\\n\\nCharacteristic\\n\\nTherapy evaluated\\n\\nMedications\\nMedical devices\\nProcedures\\n\\nClinical topic area\\n\\nCardiac and cardiovascular\\n\\nsystems\\n\\nCritical care\\nEndocrinology and\\n\\nmetabolism\\n\\nInfectious disease\\nClinical neurology\\nObstetrics and gynecology\\nOncology\\nPeripheral vascular\\n\\ndiseases\\nPsychiatry\\nRespiratory systems\\nUrology and nephrology\\n\\nPublication period\\n\\nMarch 1997eApril 1999\\nMay 1999eJune 2001\\nJuly 2001eAugust 2003\\nSeptember 2003e\\nDecember 2005\\n\\nMedian included trials\\n\\nMedian included\\n\\nparticipants\\n\\nMEDLINE coverage of\\n\\n5\\n1\\nd\\n\\nd\\n\\n1\\nd\\n\\n3\\nd\\nd\\nd\\nd\\n\\nd\\n1\\n1\\n\\n3\\n1\\n2\\nd\\n\\n17 (IQR, 14e20)\\n\\n96 (IQR,\\n\\n31.75e121.5)\\n8,679 (IQR,\\n22,830 (IQR, 14,\\n4,085e50,109)\\n172e49,687)\\n99/107 (92.5%) 969/980 (98.8%)\\n\\nincluded articles\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\n\\nity; IQR, interquartile range.\\n\\na Some AHRQ reviews included more than one class of therapy.\\n\\nRecall of new relevant studies is shown in Table 2. The\\nupdated Cochrane reviews had 20 new studies identiﬁed by\\nthe review authors and added in the updates. Recall of these\\n20 ranged from 1.00 for the related articles method to a low\\nof 0.80 CQs (Table 2). Our team identiﬁed 277 new studies\\nas relevant for inclusion in the 10 evidence reports that\\nwere updated. All test searches showed lower recall for this\\ncohort than for the Cochrane reviews and here the CQ out-\\nperformed both ranking searches (Table 2).\\n\\n3.1. Recall of CQ combined with a ranking method\\n\\nOf the 297 new relevant studies identiﬁed, the combina-\\ntion of CQ and related articles searches identiﬁed 270\\n(overall\\nin\\nthe Cochrane cohort and 0.90 (250/277) in the AHRQ\\ncohort.\\n\\nrecall of 0.91). Recall was 1.00 (20/20)\\n\\nThe combination of CQ and SVM identiﬁed 263 studies\\n(overall recall of 0.89). Recall was 1.00 (20/20) in the\\nCochrane cohort and 0.88 (243/277) in the AHRQ cohort.\\nWhen all three methods were used together, recall was\\n0.997 with 296 of the 297 studies identiﬁed.\\n\\n\\x0c112\\n\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nTable 2. Recall of eligible studies by the search methods\\n\\nCochrane reviews\\n\\nAHRQ evidence\\n\\nreports\\n\\nRetrieval method\\n\\nClinical query\\nRelated articles\\nSVM\\nTotal\\n\\nN\\n\\n16\\n20\\n19\\n20\\n\\nRecall\\n\\n0.80\\n1.00\\n0.95\\n\\nN\\n\\n188\\n176\\n129\\n277\\n\\nRecall\\n\\n0.68\\n0.64\\n0.47\\n\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\n\\nity; SVM, support vector machine.\\n\\nAcross the two cohorts, the overlap and unique component from\\nthe retrieval of relevant records by the test searches was examined\\n(Fig. 1).\\n\\n3.2. Consistency across clinical areas\\n\\nThirteen clinical areas were represented in the larger\\nstudy that included 72 journal-published reviews, but only\\neight of those clinical areas had two or more new relevant\\nstudies in the smaller Cochrane and AHRQ cohorts. Fig. 2\\nshows recall of new studies by the three types of searches,\\nfor these eight clinical areas.\\n\\nCombined recall of the CQ Boolean search paired with a\\nranking search is shown in Fig. 3. The combination showed\\ncomplete recall of relevant new studies in three of eight\\nareas when CQ was paired with related articles and recall\\nof 0.67 or higher in all areas. Four of eight clinical areas\\nhad complete recall for the combination of CQ and SVM,\\nand recall was 0.80 or higher in all areas. Even for periph-\\neral vascular disease, where all three searches performed\\n\\nFig. 2. Recall of new studies by clinical area for each search method.\\nAbbreviations: SVM, support vector machine.\\n\\nfairly poorly individually, recall was 0.76 for the CQ/\\nrelated articles and 0.90 in the CQ/SVM pairing.\\n\\n3.3. Search precision\\n\\nPrecision, across the 10 AHRQ reviews, was 0.11 for the\\nCQ, 0.22 for related articles, and 0.19 for SVM (Table 3).\\n\\nFig. 1. Overlapping and unique retrieval of relevant studies by each\\nsearch method. The size of the circle is proportional to the size of\\nthe retrieval, but the overlap and unique portions are approximations.\\nExact ﬁgures for the components are labeled. Abbreviations: SVM,\\nsupport vector machine.\\n\\nFig. 3. Recall of new studies by clinical area for search methods in\\ncombination. Abbreviations: CQ, clinical query; SVM, support vector\\nmachine.\\n\\n\\x0cTable 3. Overall precision in the AHRQ Sample\\n\\n4. Discussion\\n\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\n113\\n\\nRetrieval method\\n\\nClinical query\\nRelated articles\\nSVM\\nClinical query þ related\\nClinical query þ SVM\\nClinical query þ related\\n\\narticles\\n\\nEligible\\nstudies\\nretrieved\\n\\n187\\n176\\n128\\n250\\n\\n243\\n276\\n\\nNo. of candidates\\n\\nretrieved\\n\\nPrecision\\n\\n1,637\\n814\\n659\\n2,318a\\n\\n2,247a\\n3,264a\\n\\n0.11\\n0.22\\n0.19\\n0.11\\n\\n0.11\\n0.08\\n\\narticles þ SVM\\nAbbreviations: AHRQ, Agency for Healthcare Research and\\n\\nQuality; SVM, support vector machine.\\n\\na Number of candidates after removal of duplicate records.\\n\\nPrecision for the individual reviews ranged from 0.0 to 0.38\\nfor CQs, 0.004 to 0.80 for related articles, and 0.01 to 0.43\\nfor SVM. Overall precision was 0.11 for CQ and either\\nrelated articles or SVM. Overall precision was 0.08 when\\nCQ and both related articles and SVM were used. For\\nranked searches where a ﬁxed number of records will be\\nscreened, that number is always the denominator, so preci-\\nsion will tend to increase as the number of relevant records\\nincreasesdthe maximum retrieval size for SVM was cap-\\nped at 200 records.\\n\\n3.4. Retrieval consistency over time\\n\\nSearches were retested for the AHRQ cohort using\\nPubMed results obtained February 24, 2015. Recall of the\\nrelated articles searches across the 10 AHRQ reports was\\n0.64 originally and 0.50 when repeated (Fig. 4). Of the\\nthree extreme cases, one rose from 0.00 to 0.80, one re-\\nmained at 0.00, and one dropped from 1.00 to 0.60. CQ\\nhad overall\\nrecall of 0.68 originally and 0.70 when\\nrepeated. Changes in extreme cases were minimal. Overall\\ncombined recall of CQ and related article was 0.90 origi-\\nnally,\\nfalling to 0.85 when repeated. Considering the\\nextreme values, all review with combined recall of 1.00\\noriginally remained at 1.00, whereas the one review with\\n0.00 recall in combination showed recall of 0.80 when\\nrepeated.\\n\\n1.0\\n0.9\\n0.8\\n0.7\\n0.6\\n0.5\\n0.4\\n0.3\\n0.2\\n0.1\\n0.0\\n\\nOriginal\\n\\n2015\\n\\nClinical Query\\n\\nRelated Ar(cid:415)cles\\n\\nCombined\\n\\nFig. 4. Recall of new studies for search methods, alone and in combi-\\nnation originally (left) and when retested in 2015 (right).\\n\\nWe expected that\\n\\nthe sophisticated SVM approach\\nwould perform well when paired with more stripped-\\ndown, focused Boolean searches. Indeed, the two together\\nwere able to replace the multidatabase, multimodel\\nsearches used by the original review teams in updating\\nthe Cochrane searches. The new relevant studies for the Co-\\nchrane reviews may have been relatively easy to ﬁnd, but\\nthe pairing was also effective in the AHRQ set, and there\\nshowed better precision than is usually seen with traditional\\nsearches for systematic review of RCTs [9]. The AHRQ set\\nwas formed from the new studies identiﬁed for more com-\\nplex interventions in a rigorous, well-funded study [16].\\n\\nWe were surprised that the related articles approach per-\\nformed almost as well as SVM when used in combination\\nwith the CQ. The related article search has advantages over\\nSVMdit requires far less data preparation, and no special\\nsoftware is needed for its use. This makes it useful not only\\nin updating, but also, if appropriate seed articles can be\\nfound, in original reviews.\\n\\nThat both SVM and related articles sometimes showed\\npoor recall when used alone, but consistently good recall\\nwhen used with a Boolean method suggests that there is\\nreal beneﬁt in using complementary search methods to\\nquerying MEDLINE. The related article method is not very\\ntime consuming, and easily added to other planned search\\nefforts, while the third method used here, SVM, is more\\ntechnical. The combined performance of all three methods\\nwas surprising, but such a setup might become unpractical\\nin day-to-day use.\\n\\nOther investigators have similar ﬁndings. Examination\\nof the supplemental material presented in the Appendix C\\nto the article by Waffenschmidt et al. [18] reveals that\\nacross the 19 reviews tested, the combination of a simple\\nstructured Boolean search (SSBS) and the ﬁrst 50 retrieval\\nof PubMed similar articles showed complete retrieval of all\\nreference standard articles in 14 of 19 reviews and never\\nless than 0.90 recall (Appendix C at www.jclinepi.com pre-\\nsents data from the table by Waffenschmidt graphically).\\nWaffenschmidt concluded ‘‘the combination of these two\\nsearch techniques that are independent of each other seems\\nto compensate the respective weaknesses.’’\\n\\nSSBS search by Waffenschmidt was constructed in\\nPubMed from search terms selected for the indication and\\nintervention with PubMed’s narrow CQ ﬁlter (category:\\ntherapy). Their search using the similar articles feature (Re-\\nlCits) did not use a set of seed articles; rather, the RelCits\\nfunction was applied for each relevant citation previously\\nidentiﬁed in PubMed. Their test articles were the included\\nstudies in 19 systematic reviews of drugs.\\n\\nAgoritsas et al. also described search construction\\nmethods for searches based on CQ and PubMed similar ar-\\nticles, tested for their ability to retrieve the included studies\\nof 30 Cochrane reviews [17]. Although their search con-\\nstruction methods differed from those used here, both\\n\\n\\x0c',\n",
       "  'Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nComplementary approaches to searching MEDLINE may be\\n\\nsufﬁcient for updating systematic reviews\\n\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\n\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\n\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\n\\n0R6, Canada\\n\\nUnited Kingdom\\n\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\n\\nAccepted 7 March 2016; Published online 11 March 2016\\n\\nAbstract\\n\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\n\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\nexamined for each method.\\n\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\ncombined. Related articles showed least stability over time.\\n\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\n\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\n\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\n\\n1. Introduction\\n\\nSystematic review searches need to have high recall.\\nMechanisms to achieve this usually include expansive\\nBoolean searches of multiple databases. This approach\\nleads to long development times for the searches [1], neces-\\nsitates accessing multiple sources that may not be acces-\\nsible in some institutions [2], and entails time-consuming\\nremoval of duplicate records for articles indexed in more\\nthan one of the databases searched [3].\\n\\nMEDLINE gives excellent coverage of most biomed-\\nical topics, in particular, intervention studies. However,\\n\\nConﬂict of interest: None.\\nFunding: Some of this work was funded by the US Agency for Health-\\ncare Research and Quality, Department of Health and Human Services\\n(contract no. 290-02-0021).\\n\\n* Corresponding author. Tel.: 613-737-7600.\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\n\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\n\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\nlished that only about half the studies included in system-\\natic reviews were identiﬁed through MEDLINE. Recent\\nresearch has demonstrated that a much higher percentage\\nis present in MEDLINE, but sometimes their retrieval is\\nproblematic [5e8].\\n\\nSuccessful retrieval through a Boolean search is oper-\\nator dependent, with search performance being inﬂuenced\\nby skill of the indexer and the searcher. The search of\\nmultiple databases can therefore be helpful. The target re-\\ncords may be indexed differently in the second, third, or\\nsubsequent source searched, increasing the probability\\nof a match between the search terms entered and the in-\\ndexing of the additional records. A text search, querying\\nterms appearing in the title or abstract, may help improve\\nretrieval, as may the use of indexing terms that are broad-\\ner (less speciﬁc) than, or related to, the single best index-\\ning term. These tactics on the part of the searcher lead\\n\\n\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\n109\\n\\nWhat is new?\\n\\nKey ﬁndings\\n(cid:1) Searches using known relevant studies and the\\nsimilar articles feature of PubMed will identify\\nand rank additional articles of potential relevance.\\n(cid:1) For a given question, if the Boolean search has low\\nrecall, the ranking search tends to have higher\\nrecall, and vice versa. The two approaches comple-\\nment each other.\\n\\n(cid:1) The precision of\\n\\nmethod appears better\\nexhaustive Boolean searches.\\n\\nthis complementary paired\\nthan the precision of\\n\\nWhat this adds to what was known?\\n(cid:1) The paired approach performed well regardless of\\nwhich of two tested similarity searches were used.\\nIt is the use of independent retrieval methods that is\\nimportant.\\n\\nWhat is the implication and what should change\\nnow?\\n(cid:1) Using the simple and universally available PubMed\\nsimilar feature makes this paired approach prac-\\ntical for most systematic review teams.\\n\\n(cid:1) If the paired complementary approach is used, the\\nrecall may be sufﬁcient to consider using only\\nMEDLINE.\\n\\nto large retrievals with low precision [9]. These resulting\\nproblems may be particularly challenging for complex or\\nnewly emerging interventions with highly variant terminol-\\nogy, where alternatives to traditional Boolean searches have\\nbeen sought [10].\\n\\nMost recent systematic review information retrieval\\nresearch has focused on text mining approaches [11e14].\\nThese approaches often harvest an intentionally overinclu-\\nsive set of records and then use machine learning, similarity\\nranking and other techniques to reﬁne the set to identify the\\nmaterial most likely to be relevant, thereby reducing human\\nscreening effort. These methods show promise, but are not\\nyet widely available to reviewers.\\n\\nWe examined one method, support vector machine\\n(SVM), and compared it with a simple and readily available\\nmethod based on the PubMed similar articles feature. We\\ncall this method related articles to distinguish the method\\nfrom the similar articles feature itself. We paired both\\nwith a focused Boolean search within MEDLINE. We\\ntested this approach in an updating context where studies\\nincluded in the original review comprise the reference\\nstandard for SVM and seed articles for the PubMed-\\n\\nrelated articles search. We therefore sought to determine\\nif a focused Boolean search paired with one of the search\\nmethods that does not depend on operator skill could pro-\\nvide consistently complete retrieval of relevant new studies.\\nComparison of a number of searches, including two\\ntested here, has been previously reported [15]. In this cur-\\nrent article, two of the most successful methods in a larger\\nsample of 72 journal-published systematic reviews, clinical\\nquery (CQ), and PubMed-related articles are tested in a\\ncohort of six updated Cochrane reviews, as well as in a pre-\\nviously unreported sample of 10 Agency for Healthcare\\nResearch and Quality (AHRQ) evidence reports. The\\nCochrane reviews provide a true gold standard as updates\\nwere made by the review team based on evidence identiﬁed\\nthrough comprehensive searches; however, the new relevant\\nstudies proved fairly easy to ﬁnd. The replication in the\\nAHRQ cohort, of more complex interventions, provided a\\nmeans to validate the generalizability of the approach\\n[16]. All records were assessed for eligibility by two re-\\nviewers, and this complete screening allowed the precision\\nof the methods to be calculated for the ﬁrst time.\\n\\nOther research [17,18] suggests that searches using the\\nPubMed similar articles feature are effective in increasing\\nrecall of relevant items for reviews or more general clinical\\nsearching when combined with a Boolean-type search of\\nMEDLINE. We tested an additional search method, SVM,\\nin the Cochrane and AHRQ samples to permit comparison\\nwith our PubMed-related articles search and assess whether\\nthe complementary effect generalized beyond the PubMed\\nsimilar articles method.\\n\\nThe aim of this article was to test whether the combined\\napproach of a focused Boolean search paired with a second\\nsearch using the similar articles feature of PubMed or SVM\\ncan yield high recall with reasonable precision.\\n\\n2. Methods\\n\\n2.1. Formation of the study cohorts\\n\\nThis analysis uses a data set created for an updating\\nstudy sponsored by AHRQ [16]. Methods for the selection\\nof the cohorts, search approaches tested, and rigorous\\nmechanisms to screen search results for relevance have\\nbeen previously reported, along with the criteria to deter-\\nmine if a review was in need of update [16].\\n\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\nsearch of the ACP Journal Club database (Ovid) using the\\nstrategy:\\n\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\nwith commentary.\\n\\nAHRQ reports were identiﬁed through the PubMed\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\njrid21544]. Screening was undertaken in two phases with\\ntwo reviewers reaching consensus on eligibility. Screening\\ncontinued until the predetermined sample size was reached.\\n\\n\\x0c110\\n\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nFor inclusion, all reviews must have included random-\\nized controlled trials (RCTs) and provided meta-analysis\\nfor at least one outcome. The MEDLINE search strategies\\nhad to be reported in enough detail to permit replication.\\nFifteen AHRQ evidence reports thought\\nlikely to have\\nimportant new evidence were selected and used to validate\\nthe updating methods used in the main cohort.\\n\\nCochrane reviews were selected to meet minimum qual-\\nity and relevance standards, as deﬁned by ACP Journal\\nClub [19]. We also required that the review had been up-\\ndated and that the text of both the original review and an\\nupdated version was available. The search for the original\\nreview must have included MEDLINE and at least one\\nother electronic bibliographic database and one or more\\nnondatabase method such as hand searching or checking\\nreference lists. Such comprehensive searching was likely\\nto identify most relevant\\nliterature and form a useful\\ntraining set of included examples. At least 10 RCTs or\\nquasi-RCTs must have been included in the original review\\nto give an adequate training set for SVM.\\n\\n2.2. Test searches\\n\\n2.2.1. Clinical query\\n\\nBoolean searches were developed by a librarian experi-\\nenced in systematic review searches using a protocol\\n(Appendix A at www.jclinepi.com). Search strategies were\\ndeliberately simple, usually consisting of two or three\\nMedical Subject Heading (MeSH) terms representing the\\npopulation and intervention of the review. This search\\nwas limited by a CQ ﬁlter to ‘‘therapy (best balance of\\nsensitivity\\n‘‘Randomized\\ncontrolled trial.pt. or\\nrandomized.mp. or placebo.mp.’’\\n[20]. Boolean searches were run in OVID MEDLINE.\\n\\nspeciﬁcity)’’\\n\\nthat\\n\\nand\\n\\nis,\\n\\n2.2.2. Related articles\\n\\nSearches using the PubMed similar articles feature were\\nperformed using the PubMed unique identiﬁers (PMID) of\\nthe three newest and three largest studies included in the\\noriginal review as seed articles. There was no replacement\\nin the event of overlap between the largest and newest sets,\\nor if one of those studies was not indexed in MEDLINE.\\n(All studies included in the original reviews were checked\\nfor indexing status in MEDLINE.) The resulting set was\\nlimited to the publication type RCT and date limited to\\nthe period since the search date of the original review.\\n\\n2.2.3. Support vector machine\\n\\nThe SVM searches used for this project have not been\\npreviously described so the methods will be reported in\\nmore detail. SVM is a well established and broadly applied\\nclassiﬁcation method from machine learning [21,22]. A\\nclassiﬁcation task is closely related to an information\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\nthe collection. As SVM is a supervised algorithm,\\nit\\n\\nrequires training examples with known labels. It then places\\nthese training examples in a high-dimensional vector space\\nin such a way that examples of one class are separated from\\nexamples of the other class with the greatest distance to the\\nseparating boundary. Predicting the class of a new instance\\nhappens by placing it in the same space and determining on\\nwhich side of the boundary it falls. The distance to the\\nboundary is an indication of the prediction conﬁdence and\\nis used to rank instances (documents) as is essential in in-\\nformation retrieval tasks or to provide a cutoff threshold.\\n\\nIn our setup, the training set consisted of search results\\nfrom the original review formed with included instances\\nbeing the studies included in the review (the relevant re-\\ntrievals) and excluded instances being the studies found\\nby the search used in the original review but excluded from\\nthe review (irrelevant retrievals). The new examples to be\\nclassiﬁed were articles indexed since the date of the search\\nperformed in the original review.\\n\\nSVM searches were run on MEDLINE data stored\\nlocally at\\nthe National Research Council of Canada.\\nMEDLINE was refreshed before running the searches.\\nThe MEDLINE records were represented as bags of fea-\\ntures, where features consisted of lowercased words and\\nword combinations (up to four words) from the title and ab-\\nstract ﬁelds, full MeSH terms, and contents of all other\\nﬁelds in the MEDLINE record. Features were uniformly\\nweighted. The SVM implementation used was SVM Light\\n[23] used with a linear kernel.\\n\\nThe approach was piloted with one AHRQ evidence\\nreport, and various conﬁgurations were tried, observing\\nthe placement of several known new relevant studies within\\nthe retrieval. The MEDLINE set was ﬁltered with the\\nrevised Highly Sensitive Search Strategy (HSSS) [24]. Pre-\\nliminary testing indicated that this ﬁlter would retrieve\\n99.1% of MEDLINE-indexed relevant new evidence from\\nthe larger cohort of journal-published systematic reviews\\n(n 5 72) and Cochrane reviews (n 5 27). This HSSS ﬁlter\\nwas added primarily to improve processing speed, relative\\nto running the classiﬁer against all of MEDLINE. With\\nthe ﬁlter, processing for one systematic review took about\\n10 minutes. Next, a coarse-grained ﬁlter was applied;\\nincluded and excluded instances were combined in a single\\nset labeled POS. Adjacent PubMed IDs (the next higher\\nPMID to each member of POS, as long as that higher num-\\nber was not itself a member of POS) were combined in a set\\nto represent WORLD. The model was trained on POS vs.\\nWORLD, using words and phrases from title, abstract,\\nauthor names, and journal name. This model was run\\nagainst the records passing the HSSS ﬁlter.\\n\\nFour different subsets of\\n\\nthe SVM retrieval were\\nexamineddone set included all records with a relevance\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\n(SVM200point5). Other sets consisted of all records with\\nrelevance scores of 0.95 or more (SVM95), 0.90 or more\\n(SVM90), and 0.80 or more (SVM80). PubMed ID and\\nrelevance scores were recorded, and PubMed IDs were\\n\\n\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\n111\\n\\nTable 1. Characteristics of the included reviews\\n\\nCochrane\\nreviews; N\\n\\nAHRQ evidence\\n\\nreports; N\\n\\nincorporated into the database of records for the reviewers\\nto screen. Only the results from SVM200point5 are re-\\nported here and are described as SVM.\\n\\n2.3. Determining performance of the test searches\\n\\nFor the updated Cochrane reviews, reference standard ar-\\nticles were those studies included in the updated review that\\nwere not in the original and which entered MEDLINE after\\nthe search date of the original review. For the AHRQ evi-\\ndence reports, the full retrieval set was screened. Records\\nfound relevant by the consensus of reviewers were consid-\\nered reference standard articles. Recall was the proportion\\nof reference standard articles identiﬁed by the search:\\n\\nNumber of reference standard articles found\\nTotal number of reference standard articles\\n\\nBioVenn software was used to analyze the overlap be-\\ntween the retrieval of relevant articles from the three\\nsearches and to create Venn diagrams [25].\\n\\nAll included reviews were classiﬁed into clinical area\\nbased on factors such as ISI journal classiﬁcation, the Co-\\nchrane Collaboration Review Group where the topic might\\nbe placed and the high level MeSH term under which the pop-\\nulation (condition) would be indexed. Performance of the\\nsearches in different clinical areas was displayed graphically\\nfor the searches both alone and in combinations, to allow ex-\\namination of differences in parallelism, level, and ﬂatness.\\n\\nPrecision is the proportion of all retrieved records that\\n\\nare relevant:\\n\\nNumber of reference standard articles found\\n\\nTotal number of records retrieved\\n\\nThe inverse of precision is the number needed to read to\\nﬁnd one eligible study, thus precision inﬂuences the work\\nof the review. Precision was calculated only for the AHRQ\\nevidence reviews. Because not all candidates retrieved by\\nthe Cochrane searches were assessed, precision could not\\nbe established in that sample.\\n\\n2.4. Determining stability of results over time\\n\\nThe CQ and related article searches, originally run in\\nMarch 2008, were repeated in February 2015 in the AHRQ\\ncohort. Recall of relevant items was compared with the\\noriginal retrievals to determine if any changes introduced\\nby National Library of Medicine might invalidate ﬁndings.\\nAs SVM conﬁguration is under the control of the investiga-\\ntors, it was not retested.\\n\\n3. Results\\n\\nSix Cochrane reviews and 10 AHRQ evidence reports\\nmet all inclusion criteria (Appendix B at www.jclinepi.\\ncom). Characteristics of the included reviews are shown\\nin Table 1.\\n\\n9a\\n1\\n3\\n\\n3\\n\\nd\\n1\\n\\nd\\n1\\n2\\n1\\n1\\n\\n1\\nd\\nd\\n\\nd\\n3\\n3\\n4\\n\\nCharacteristic\\n\\nTherapy evaluated\\n\\nMedications\\nMedical devices\\nProcedures\\n\\nClinical topic area\\n\\nCardiac and cardiovascular\\n\\nsystems\\n\\nCritical care\\nEndocrinology and\\n\\nmetabolism\\n\\nInfectious disease\\nClinical neurology\\nObstetrics and gynecology\\nOncology\\nPeripheral vascular\\n\\ndiseases\\nPsychiatry\\nRespiratory systems\\nUrology and nephrology\\n\\nPublication period\\n\\nMarch 1997eApril 1999\\nMay 1999eJune 2001\\nJuly 2001eAugust 2003\\nSeptember 2003e\\nDecember 2005\\n\\nMedian included trials\\n\\nMedian included\\n\\nparticipants\\n\\nMEDLINE coverage of\\n\\n5\\n1\\nd\\n\\nd\\n\\n1\\nd\\n\\n3\\nd\\nd\\nd\\nd\\n\\nd\\n1\\n1\\n\\n3\\n1\\n2\\nd\\n\\n17 (IQR, 14e20)\\n\\n96 (IQR,\\n\\n31.75e121.5)\\n8,679 (IQR,\\n22,830 (IQR, 14,\\n4,085e50,109)\\n172e49,687)\\n99/107 (92.5%) 969/980 (98.8%)\\n\\nincluded articles\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\n\\nity; IQR, interquartile range.\\n\\na Some AHRQ reviews included more than one class of therapy.\\n\\nRecall of new relevant studies is shown in Table 2. The\\nupdated Cochrane reviews had 20 new studies identiﬁed by\\nthe review authors and added in the updates. Recall of these\\n20 ranged from 1.00 for the related articles method to a low\\nof 0.80 CQs (Table 2). Our team identiﬁed 277 new studies\\nas relevant for inclusion in the 10 evidence reports that\\nwere updated. All test searches showed lower recall for this\\ncohort than for the Cochrane reviews and here the CQ out-\\nperformed both ranking searches (Table 2).\\n\\n3.1. Recall of CQ combined with a ranking method\\n\\nOf the 297 new relevant studies identiﬁed, the combina-\\ntion of CQ and related articles searches identiﬁed 270\\n(overall\\nin\\nthe Cochrane cohort and 0.90 (250/277) in the AHRQ\\ncohort.\\n\\nrecall of 0.91). Recall was 1.00 (20/20)\\n\\nThe combination of CQ and SVM identiﬁed 263 studies\\n(overall recall of 0.89). Recall was 1.00 (20/20) in the\\nCochrane cohort and 0.88 (243/277) in the AHRQ cohort.\\nWhen all three methods were used together, recall was\\n0.997 with 296 of the 297 studies identiﬁed.\\n\\n\\x0c112\\n\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nTable 2. Recall of eligible studies by the search methods\\n\\nCochrane reviews\\n\\nAHRQ evidence\\n\\nreports\\n\\nRetrieval method\\n\\nClinical query\\nRelated articles\\nSVM\\nTotal\\n\\nN\\n\\n16\\n20\\n19\\n20\\n\\nRecall\\n\\n0.80\\n1.00\\n0.95\\n\\nN\\n\\n188\\n176\\n129\\n277\\n\\nRecall\\n\\n0.68\\n0.64\\n0.47\\n\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\n\\nity; SVM, support vector machine.\\n\\nAcross the two cohorts, the overlap and unique component from\\nthe retrieval of relevant records by the test searches was examined\\n(Fig. 1).\\n\\n3.2. Consistency across clinical areas\\n\\nThirteen clinical areas were represented in the larger\\nstudy that included 72 journal-published reviews, but only\\neight of those clinical areas had two or more new relevant\\nstudies in the smaller Cochrane and AHRQ cohorts. Fig. 2\\nshows recall of new studies by the three types of searches,\\nfor these eight clinical areas.\\n\\nCombined recall of the CQ Boolean search paired with a\\nranking search is shown in Fig. 3. The combination showed\\ncomplete recall of relevant new studies in three of eight\\nareas when CQ was paired with related articles and recall\\nof 0.67 or higher in all areas. Four of eight clinical areas\\nhad complete recall for the combination of CQ and SVM,\\nand recall was 0.80 or higher in all areas. Even for periph-\\neral vascular disease, where all three searches performed\\n\\nFig. 2. Recall of new studies by clinical area for each search method.\\nAbbreviations: SVM, support vector machine.\\n\\nfairly poorly individually, recall was 0.76 for the CQ/\\nrelated articles and 0.90 in the CQ/SVM pairing.\\n\\n3.3. Search precision\\n\\nPrecision, across the 10 AHRQ reviews, was 0.11 for the\\nCQ, 0.22 for related articles, and 0.19 for SVM (Table 3).\\n\\nFig. 1. Overlapping and unique retrieval of relevant studies by each\\nsearch method. The size of the circle is proportional to the size of\\nthe retrieval, but the overlap and unique portions are approximations.\\nExact ﬁgures for the components are labeled. Abbreviations: SVM,\\nsupport vector machine.\\n\\nFig. 3. Recall of new studies by clinical area for search methods in\\ncombination. Abbreviations: CQ, clinical query; SVM, support vector\\nmachine.\\n\\n\\x0cTable 3. Overall precision in the AHRQ Sample\\n\\n4. Discussion\\n\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\n113\\n\\nRetrieval method\\n\\nClinical query\\nRelated articles\\nSVM\\nClinical query þ related\\nClinical query þ SVM\\nClinical query þ related\\n\\narticles\\n\\nEligible\\nstudies\\nretrieved\\n\\n187\\n176\\n128\\n250\\n\\n243\\n276\\n\\nNo. of candidates\\n\\nretrieved\\n\\nPrecision\\n\\n1,637\\n814\\n659\\n2,318a\\n\\n2,247a\\n3,264a\\n\\n0.11\\n0.22\\n0.19\\n0.11\\n\\n0.11\\n0.08\\n\\narticles þ SVM\\nAbbreviations: AHRQ, Agency for Healthcare Research and\\n\\nQuality; SVM, support vector machine.\\n\\na Number of candidates after removal of duplicate records.\\n\\nPrecision for the individual reviews ranged from 0.0 to 0.38\\nfor CQs, 0.004 to 0.80 for related articles, and 0.01 to 0.43\\nfor SVM. Overall precision was 0.11 for CQ and either\\nrelated articles or SVM. Overall precision was 0.08 when\\nCQ and both related articles and SVM were used. For\\nranked searches where a ﬁxed number of records will be\\nscreened, that number is always the denominator, so preci-\\nsion will tend to increase as the number of relevant records\\nincreasesdthe maximum retrieval size for SVM was cap-\\nped at 200 records.\\n\\n3.4. Retrieval consistency over time\\n\\nSearches were retested for the AHRQ cohort using\\nPubMed results obtained February 24, 2015. Recall of the\\nrelated articles searches across the 10 AHRQ reports was\\n0.64 originally and 0.50 when repeated (Fig. 4). Of the\\nthree extreme cases, one rose from 0.00 to 0.80, one re-\\nmained at 0.00, and one dropped from 1.00 to 0.60. CQ\\nhad overall\\nrecall of 0.68 originally and 0.70 when\\nrepeated. Changes in extreme cases were minimal. Overall\\ncombined recall of CQ and related article was 0.90 origi-\\nnally,\\nfalling to 0.85 when repeated. Considering the\\nextreme values, all review with combined recall of 1.00\\noriginally remained at 1.00, whereas the one review with\\n0.00 recall in combination showed recall of 0.80 when\\nrepeated.\\n\\n1.0\\n0.9\\n0.8\\n0.7\\n0.6\\n0.5\\n0.4\\n0.3\\n0.2\\n0.1\\n0.0\\n\\nOriginal\\n\\n2015\\n\\nClinical Query\\n\\nRelated Ar(cid:415)cles\\n\\nCombined\\n\\nFig. 4. Recall of new studies for search methods, alone and in combi-\\nnation originally (left) and when retested in 2015 (right).\\n\\nWe expected that\\n\\nthe sophisticated SVM approach\\nwould perform well when paired with more stripped-\\ndown, focused Boolean searches. Indeed, the two together\\nwere able to replace the multidatabase, multimodel\\nsearches used by the original review teams in updating\\nthe Cochrane searches. The new relevant studies for the Co-\\nchrane reviews may have been relatively easy to ﬁnd, but\\nthe pairing was also effective in the AHRQ set, and there\\nshowed better precision than is usually seen with traditional\\nsearches for systematic review of RCTs [9]. The AHRQ set\\nwas formed from the new studies identiﬁed for more com-\\nplex interventions in a rigorous, well-funded study [16].\\n\\nWe were surprised that the related articles approach per-\\nformed almost as well as SVM when used in combination\\nwith the CQ. The related article search has advantages over\\nSVMdit requires far less data preparation, and no special\\nsoftware is needed for its use. This makes it useful not only\\nin updating, but also, if appropriate seed articles can be\\nfound, in original reviews.\\n\\nThat both SVM and related articles sometimes showed\\npoor recall when used alone, but consistently good recall\\nwhen used with a Boolean method suggests that there is\\nreal beneﬁt in using complementary search methods to\\nquerying MEDLINE. The related article method is not very\\ntime consuming, and easily added to other planned search\\nefforts, while the third method used here, SVM, is more\\ntechnical. The combined performance of all three methods\\nwas surprising, but such a setup might become unpractical\\nin day-to-day use.\\n\\nOther investigators have similar ﬁndings. Examination\\nof the supplemental material presented in the Appendix C\\nto the article by Waffenschmidt et al. [18] reveals that\\nacross the 19 reviews tested, the combination of a simple\\nstructured Boolean search (SSBS) and the ﬁrst 50 retrieval\\nof PubMed similar articles showed complete retrieval of all\\nreference standard articles in 14 of 19 reviews and never\\nless than 0.90 recall (Appendix C at www.jclinepi.com pre-\\nsents data from the table by Waffenschmidt graphically).\\nWaffenschmidt concluded ‘‘the combination of these two\\nsearch techniques that are independent of each other seems\\nto compensate the respective weaknesses.’’\\n\\nSSBS search by Waffenschmidt was constructed in\\nPubMed from search terms selected for the indication and\\nintervention with PubMed’s narrow CQ ﬁlter (category:\\ntherapy). Their search using the similar articles feature (Re-\\nlCits) did not use a set of seed articles; rather, the RelCits\\nfunction was applied for each relevant citation previously\\nidentiﬁed in PubMed. Their test articles were the included\\nstudies in 19 systematic reviews of drugs.\\n\\nAgoritsas et al. also described search construction\\nmethods for searches based on CQ and PubMed similar ar-\\nticles, tested for their ability to retrieve the included studies\\nof 30 Cochrane reviews [17]. Although their search con-\\nstruction methods differed from those used here, both\\n\\n\\x0c114\\n\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nsearches were derived using standard methods. Their\\napproach to the structured Boolean search used terms from\\nthe population, intervention, and comparison with the CQ,\\nlimited to humans and English. Two clinicians selected the\\nPubMed similar articles seeds from the initial PubMed\\nretrieval. They noted that no one method provided consis-\\ntently high retrieval [17].\\n\\nThus, the robust nature of this pairing of Boolean and\\nnon-Boolean searches has been shown in several contexts,\\ngiving support\\nto the hypothesis that methods such as\\nrelated articles and SVM can compensate for the variation\\ninherent\\nin selecting search terms or assigning subject\\nheadings during indexing.\\n\\nConsidering stability of the searches over time, the\\nminimal change seen in the performance of the CQ is\\nlikely explained by indexing changes of a few records.\\nThe computation appears not to have changed. This sug-\\ngests that indexing changes may also impact SVM results\\nover time; however, this is likely to result in improved\\nperformance, as was seen with the CQ. The computation\\nof similar articles is still described by National Library\\nof Medicine as being based on the algorithm described\\nby Kim et al. [26]. That algorithm would suggest changes\\nover time in the nearest neighbor score as the frequency of\\ncertain terms in the MEDLINE corpus changes. A full\\nexploration of the changes in nearest neighbor scores of\\nsimilar articles over time is beyond the scope of this\\nstudy. It should be noted that when the similar articles\\nfeature was initially studied, it was simple to submit seed\\narticles, and then add limits such as date or RCT publica-\\ntion type. On retesting, such additional limits were more\\ncomplex to apply, and the elink utility seemed to be the\\nmost practical approach to identifying the related articles\\n[27].\\n\\nThe beneﬁt of the complementary searches, relative to\\nBoolean searching alone, was greatest for AHRQ evidence\\nreports and less for the simpler Cochrane reviews, suggest-\\ning that this approach may be particularly useful for more\\ncomplex evidence. There are examples of its utility in the\\nliterature. For example, in a realist review of a multidisci-\\nplinary body of\\nliterature identifying six domains of\\nClinical Practice Guideline implementability, the use of\\nPubMed’s similar articles feature as a third stage of search-\\ning identiﬁed 131 records of which 104 were relevant [28].\\nIn a systematic review of evidence on the links between\\npatient experience and clinical safety and effectiveness,\\nthe authors used PubMed similar articles to snowball on\\narticles\\nidentiﬁed through an EMBASE search to\\novercome the limitations of predeﬁned searches for com-\\nplex evidence [29].\\n\\nThere is always the possibility that a search, or even a\\npair of searches, will fail. One method to detect such\\nfailures is to test whether the search strategies ﬁnd known\\nrelevant items [30]. In the updating case, this is easily done\\nusing the included studies of the original review as a test\\n\\nset, allowing the review to determine the MEDLINE\\ncoverage of their particular topic at the same time.\\n\\n4.1. Limitations\\n\\nThere are two limitations to our proposed strategy. Other\\ndatabases should be searched in the unusual event that\\nnumerous studies, representing more than a small propor-\\ntion of the total N, are not included in MEDLINE. Second,\\nwhen it is important to ﬁnd articles too new to be indexed\\nby MEDLINE, systematic reviewers may wish to conduct a\\nsimple PubMed search limited to the nonindexed subsets\\n[31,32].\\n\\n5. Conclusion\\n\\nThe general approach of a Boolean plus a ranking\\nsearch is effective in MEDLINE retrieval for systematic\\nreviews. Very high levels of identiﬁcation of relevant\\nMEDLINE records, with adequate precision, are possible\\nusing a focused Boolean search complemented by a docu-\\nment similarity or ranking method. The efﬁcacy of a\\nfocused Boolean search paired with a search using the\\nPubMed similar articles feature is in agreement with pre-\\nvious studies [17,18], and this study shows that this com-\\nplementary effect also occurs when SVM is used as the\\nadditional method. The beneﬁt of using two complemen-\\ntary approaches to achieving high recall in MEDLINE is\\na robust effect.\\n\\nPubMed-related articles is a parsimonious method, as it\\nis readily available to all review teams without cost. The\\napproach is robust across clinical domains, and the effect\\nhas now been demonstrated in several samples. The method\\nmay be sufﬁcient for updating systematic reviews of inter-\\nventions and may be used for new reviews of interventions\\nwhen paired with a trials registry search. It is likely to work\\nin any type of search where MEDLINE provides good sub-\\nject coverage even if retrieval through traditional search\\nmethods has been challenging.\\n\\nAcknowledgments\\n\\nMohamed Ansari and Jun Ji undertook screening and\\nrelevance assessment of candidate studies retrieved by the\\ntest\\nsearches, Tamara Rader developed the Boolean\\nsearches, and Raymond Daniel processed related articles\\nsearches and obtained full-text articles. David Moher cosu-\\npervised Margaret Sampson doctoral dissertation and pro-\\nvided invaluable advice and guidance.\\n\\nSupplementary data\\n\\nSupplementary data related to this article can be found at\\n\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004.\\n\\n\\x0c',\n",
       "  'Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nComplementary approaches to searching MEDLINE may be\\n\\nsufﬁcient for updating systematic reviews\\n\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\n\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\n\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\n\\n0R6, Canada\\n\\nUnited Kingdom\\n\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\n\\nAccepted 7 March 2016; Published online 11 March 2016\\n\\nAbstract\\n\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\n\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\nexamined for each method.\\n\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\ncombined. Related articles showed least stability over time.\\n\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\n\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\n\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\n\\n1. Introduction\\n\\nSystematic review searches need to have high recall.\\nMechanisms to achieve this usually include expansive\\nBoolean searches of multiple databases. This approach\\nleads to long development times for the searches [1], neces-\\nsitates accessing multiple sources that may not be acces-\\nsible in some institutions [2], and entails time-consuming\\nremoval of duplicate records for articles indexed in more\\nthan one of the databases searched [3].\\n\\nMEDLINE gives excellent coverage of most biomed-\\nical topics, in particular, intervention studies. However,\\n\\nConﬂict of interest: None.\\nFunding: Some of this work was funded by the US Agency for Health-\\ncare Research and Quality, Department of Health and Human Services\\n(contract no. 290-02-0021).\\n\\n* Corresponding author. Tel.: 613-737-7600.\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\n\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\n\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\nlished that only about half the studies included in system-\\natic reviews were identiﬁed through MEDLINE. Recent\\nresearch has demonstrated that a much higher percentage\\nis present in MEDLINE, but sometimes their retrieval is\\nproblematic [5e8].\\n\\nSuccessful retrieval through a Boolean search is oper-\\nator dependent, with search performance being inﬂuenced\\nby skill of the indexer and the searcher. The search of\\nmultiple databases can therefore be helpful. The target re-\\ncords may be indexed differently in the second, third, or\\nsubsequent source searched, increasing the probability\\nof a match between the search terms entered and the in-\\ndexing of the additional records. A text search, querying\\nterms appearing in the title or abstract, may help improve\\nretrieval, as may the use of indexing terms that are broad-\\ner (less speciﬁc) than, or related to, the single best index-\\ning term. These tactics on the part of the searcher lead\\n\\n\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\n109\\n\\nWhat is new?\\n\\nKey ﬁndings\\n(cid:1) Searches using known relevant studies and the\\nsimilar articles feature of PubMed will identify\\nand rank additional articles of potential relevance.\\n(cid:1) For a given question, if the Boolean search has low\\nrecall, the ranking search tends to have higher\\nrecall, and vice versa. The two approaches comple-\\nment each other.\\n\\n(cid:1) The precision of\\n\\nmethod appears better\\nexhaustive Boolean searches.\\n\\nthis complementary paired\\nthan the precision of\\n\\nWhat this adds to what was known?\\n(cid:1) The paired approach performed well regardless of\\nwhich of two tested similarity searches were used.\\nIt is the use of independent retrieval methods that is\\nimportant.\\n\\nWhat is the implication and what should change\\nnow?\\n(cid:1) Using the simple and universally available PubMed\\nsimilar feature makes this paired approach prac-\\ntical for most systematic review teams.\\n\\n(cid:1) If the paired complementary approach is used, the\\nrecall may be sufﬁcient to consider using only\\nMEDLINE.\\n\\nto large retrievals with low precision [9]. These resulting\\nproblems may be particularly challenging for complex or\\nnewly emerging interventions with highly variant terminol-\\nogy, where alternatives to traditional Boolean searches have\\nbeen sought [10].\\n\\nMost recent systematic review information retrieval\\nresearch has focused on text mining approaches [11e14].\\nThese approaches often harvest an intentionally overinclu-\\nsive set of records and then use machine learning, similarity\\nranking and other techniques to reﬁne the set to identify the\\nmaterial most likely to be relevant, thereby reducing human\\nscreening effort. These methods show promise, but are not\\nyet widely available to reviewers.\\n\\nWe examined one method, support vector machine\\n(SVM), and compared it with a simple and readily available\\nmethod based on the PubMed similar articles feature. We\\ncall this method related articles to distinguish the method\\nfrom the similar articles feature itself. We paired both\\nwith a focused Boolean search within MEDLINE. We\\ntested this approach in an updating context where studies\\nincluded in the original review comprise the reference\\nstandard for SVM and seed articles for the PubMed-\\n\\nrelated articles search. We therefore sought to determine\\nif a focused Boolean search paired with one of the search\\nmethods that does not depend on operator skill could pro-\\nvide consistently complete retrieval of relevant new studies.\\nComparison of a number of searches, including two\\ntested here, has been previously reported [15]. In this cur-\\nrent article, two of the most successful methods in a larger\\nsample of 72 journal-published systematic reviews, clinical\\nquery (CQ), and PubMed-related articles are tested in a\\ncohort of six updated Cochrane reviews, as well as in a pre-\\nviously unreported sample of 10 Agency for Healthcare\\nResearch and Quality (AHRQ) evidence reports. The\\nCochrane reviews provide a true gold standard as updates\\nwere made by the review team based on evidence identiﬁed\\nthrough comprehensive searches; however, the new relevant\\nstudies proved fairly easy to ﬁnd. The replication in the\\nAHRQ cohort, of more complex interventions, provided a\\nmeans to validate the generalizability of the approach\\n[16]. All records were assessed for eligibility by two re-\\nviewers, and this complete screening allowed the precision\\nof the methods to be calculated for the ﬁrst time.\\n\\nOther research [17,18] suggests that searches using the\\nPubMed similar articles feature are effective in increasing\\nrecall of relevant items for reviews or more general clinical\\nsearching when combined with a Boolean-type search of\\nMEDLINE. We tested an additional search method, SVM,\\nin the Cochrane and AHRQ samples to permit comparison\\nwith our PubMed-related articles search and assess whether\\nthe complementary effect generalized beyond the PubMed\\nsimilar articles method.\\n\\nThe aim of this article was to test whether the combined\\napproach of a focused Boolean search paired with a second\\nsearch using the similar articles feature of PubMed or SVM\\ncan yield high recall with reasonable precision.\\n\\n2. Methods\\n\\n2.1. Formation of the study cohorts\\n\\nThis analysis uses a data set created for an updating\\nstudy sponsored by AHRQ [16]. Methods for the selection\\nof the cohorts, search approaches tested, and rigorous\\nmechanisms to screen search results for relevance have\\nbeen previously reported, along with the criteria to deter-\\nmine if a review was in need of update [16].\\n\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\nsearch of the ACP Journal Club database (Ovid) using the\\nstrategy:\\n\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\nwith commentary.\\n\\nAHRQ reports were identiﬁed through the PubMed\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\njrid21544]. Screening was undertaken in two phases with\\ntwo reviewers reaching consensus on eligibility. Screening\\ncontinued until the predetermined sample size was reached.\\n\\n\\x0c110\\n\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nFor inclusion, all reviews must have included random-\\nized controlled trials (RCTs) and provided meta-analysis\\nfor at least one outcome. The MEDLINE search strategies\\nhad to be reported in enough detail to permit replication.\\nFifteen AHRQ evidence reports thought\\nlikely to have\\nimportant new evidence were selected and used to validate\\nthe updating methods used in the main cohort.\\n\\nCochrane reviews were selected to meet minimum qual-\\nity and relevance standards, as deﬁned by ACP Journal\\nClub [19]. We also required that the review had been up-\\ndated and that the text of both the original review and an\\nupdated version was available. The search for the original\\nreview must have included MEDLINE and at least one\\nother electronic bibliographic database and one or more\\nnondatabase method such as hand searching or checking\\nreference lists. Such comprehensive searching was likely\\nto identify most relevant\\nliterature and form a useful\\ntraining set of included examples. At least 10 RCTs or\\nquasi-RCTs must have been included in the original review\\nto give an adequate training set for SVM.\\n\\n2.2. Test searches\\n\\n2.2.1. Clinical query\\n\\nBoolean searches were developed by a librarian experi-\\nenced in systematic review searches using a protocol\\n(Appendix A at www.jclinepi.com). Search strategies were\\ndeliberately simple, usually consisting of two or three\\nMedical Subject Heading (MeSH) terms representing the\\npopulation and intervention of the review. This search\\nwas limited by a CQ ﬁlter to ‘‘therapy (best balance of\\nsensitivity\\n‘‘Randomized\\ncontrolled trial.pt. or\\nrandomized.mp. or placebo.mp.’’\\n[20]. Boolean searches were run in OVID MEDLINE.\\n\\nspeciﬁcity)’’\\n\\nthat\\n\\nand\\n\\nis,\\n\\n2.2.2. Related articles\\n\\nSearches using the PubMed similar articles feature were\\nperformed using the PubMed unique identiﬁers (PMID) of\\nthe three newest and three largest studies included in the\\noriginal review as seed articles. There was no replacement\\nin the event of overlap between the largest and newest sets,\\nor if one of those studies was not indexed in MEDLINE.\\n(All studies included in the original reviews were checked\\nfor indexing status in MEDLINE.) The resulting set was\\nlimited to the publication type RCT and date limited to\\nthe period since the search date of the original review.\\n\\n2.2.3. Support vector machine\\n\\nThe SVM searches used for this project have not been\\npreviously described so the methods will be reported in\\nmore detail. SVM is a well established and broadly applied\\nclassiﬁcation method from machine learning [21,22]. A\\nclassiﬁcation task is closely related to an information\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\nthe collection. As SVM is a supervised algorithm,\\nit\\n\\nrequires training examples with known labels. It then places\\nthese training examples in a high-dimensional vector space\\nin such a way that examples of one class are separated from\\nexamples of the other class with the greatest distance to the\\nseparating boundary. Predicting the class of a new instance\\nhappens by placing it in the same space and determining on\\nwhich side of the boundary it falls. The distance to the\\nboundary is an indication of the prediction conﬁdence and\\nis used to rank instances (documents) as is essential in in-\\nformation retrieval tasks or to provide a cutoff threshold.\\n\\nIn our setup, the training set consisted of search results\\nfrom the original review formed with included instances\\nbeing the studies included in the review (the relevant re-\\ntrievals) and excluded instances being the studies found\\nby the search used in the original review but excluded from\\nthe review (irrelevant retrievals). The new examples to be\\nclassiﬁed were articles indexed since the date of the search\\nperformed in the original review.\\n\\nSVM searches were run on MEDLINE data stored\\nlocally at\\nthe National Research Council of Canada.\\nMEDLINE was refreshed before running the searches.\\nThe MEDLINE records were represented as bags of fea-\\ntures, where features consisted of lowercased words and\\nword combinations (up to four words) from the title and ab-\\nstract ﬁelds, full MeSH terms, and contents of all other\\nﬁelds in the MEDLINE record. Features were uniformly\\nweighted. The SVM implementation used was SVM Light\\n[23] used with a linear kernel.\\n\\nThe approach was piloted with one AHRQ evidence\\nreport, and various conﬁgurations were tried, observing\\nthe placement of several known new relevant studies within\\nthe retrieval. The MEDLINE set was ﬁltered with the\\nrevised Highly Sensitive Search Strategy (HSSS) [24]. Pre-\\nliminary testing indicated that this ﬁlter would retrieve\\n99.1% of MEDLINE-indexed relevant new evidence from\\nthe larger cohort of journal-published systematic reviews\\n(n 5 72) and Cochrane reviews (n 5 27). This HSSS ﬁlter\\nwas added primarily to improve processing speed, relative\\nto running the classiﬁer against all of MEDLINE. With\\nthe ﬁlter, processing for one systematic review took about\\n10 minutes. Next, a coarse-grained ﬁlter was applied;\\nincluded and excluded instances were combined in a single\\nset labeled POS. Adjacent PubMed IDs (the next higher\\nPMID to each member of POS, as long as that higher num-\\nber was not itself a member of POS) were combined in a set\\nto represent WORLD. The model was trained on POS vs.\\nWORLD, using words and phrases from title, abstract,\\nauthor names, and journal name. This model was run\\nagainst the records passing the HSSS ﬁlter.\\n\\nFour different subsets of\\n\\nthe SVM retrieval were\\nexamineddone set included all records with a relevance\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\n(SVM200point5). Other sets consisted of all records with\\nrelevance scores of 0.95 or more (SVM95), 0.90 or more\\n(SVM90), and 0.80 or more (SVM80). PubMed ID and\\nrelevance scores were recorded, and PubMed IDs were\\n\\n\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\n111\\n\\nTable 1. Characteristics of the included reviews\\n\\nCochrane\\nreviews; N\\n\\nAHRQ evidence\\n\\nreports; N\\n\\nincorporated into the database of records for the reviewers\\nto screen. Only the results from SVM200point5 are re-\\nported here and are described as SVM.\\n\\n2.3. Determining performance of the test searches\\n\\nFor the updated Cochrane reviews, reference standard ar-\\nticles were those studies included in the updated review that\\nwere not in the original and which entered MEDLINE after\\nthe search date of the original review. For the AHRQ evi-\\ndence reports, the full retrieval set was screened. Records\\nfound relevant by the consensus of reviewers were consid-\\nered reference standard articles. Recall was the proportion\\nof reference standard articles identiﬁed by the search:\\n\\nNumber of reference standard articles found\\nTotal number of reference standard articles\\n\\nBioVenn software was used to analyze the overlap be-\\ntween the retrieval of relevant articles from the three\\nsearches and to create Venn diagrams [25].\\n\\nAll included reviews were classiﬁed into clinical area\\nbased on factors such as ISI journal classiﬁcation, the Co-\\nchrane Collaboration Review Group where the topic might\\nbe placed and the high level MeSH term under which the pop-\\nulation (condition) would be indexed. Performance of the\\nsearches in different clinical areas was displayed graphically\\nfor the searches both alone and in combinations, to allow ex-\\namination of differences in parallelism, level, and ﬂatness.\\n\\nPrecision is the proportion of all retrieved records that\\n\\nare relevant:\\n\\nNumber of reference standard articles found\\n\\nTotal number of records retrieved\\n\\nThe inverse of precision is the number needed to read to\\nﬁnd one eligible study, thus precision inﬂuences the work\\nof the review. Precision was calculated only for the AHRQ\\nevidence reviews. Because not all candidates retrieved by\\nthe Cochrane searches were assessed, precision could not\\nbe established in that sample.\\n\\n2.4. Determining stability of results over time\\n\\nThe CQ and related article searches, originally run in\\nMarch 2008, were repeated in February 2015 in the AHRQ\\ncohort. Recall of relevant items was compared with the\\noriginal retrievals to determine if any changes introduced\\nby National Library of Medicine might invalidate ﬁndings.\\nAs SVM conﬁguration is under the control of the investiga-\\ntors, it was not retested.\\n\\n3. Results\\n\\nSix Cochrane reviews and 10 AHRQ evidence reports\\nmet all inclusion criteria (Appendix B at www.jclinepi.\\ncom). Characteristics of the included reviews are shown\\nin Table 1.\\n\\n9a\\n1\\n3\\n\\n3\\n\\nd\\n1\\n\\nd\\n1\\n2\\n1\\n1\\n\\n1\\nd\\nd\\n\\nd\\n3\\n3\\n4\\n\\nCharacteristic\\n\\nTherapy evaluated\\n\\nMedications\\nMedical devices\\nProcedures\\n\\nClinical topic area\\n\\nCardiac and cardiovascular\\n\\nsystems\\n\\nCritical care\\nEndocrinology and\\n\\nmetabolism\\n\\nInfectious disease\\nClinical neurology\\nObstetrics and gynecology\\nOncology\\nPeripheral vascular\\n\\ndiseases\\nPsychiatry\\nRespiratory systems\\nUrology and nephrology\\n\\nPublication period\\n\\nMarch 1997eApril 1999\\nMay 1999eJune 2001\\nJuly 2001eAugust 2003\\nSeptember 2003e\\nDecember 2005\\n\\nMedian included trials\\n\\nMedian included\\n\\nparticipants\\n\\nMEDLINE coverage of\\n\\n5\\n1\\nd\\n\\nd\\n\\n1\\nd\\n\\n3\\nd\\nd\\nd\\nd\\n\\nd\\n1\\n1\\n\\n3\\n1\\n2\\nd\\n\\n17 (IQR, 14e20)\\n\\n96 (IQR,\\n\\n31.75e121.5)\\n8,679 (IQR,\\n22,830 (IQR, 14,\\n4,085e50,109)\\n172e49,687)\\n99/107 (92.5%) 969/980 (98.8%)\\n\\nincluded articles\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\n\\nity; IQR, interquartile range.\\n\\na Some AHRQ reviews included more than one class of therapy.\\n\\nRecall of new relevant studies is shown in Table 2. The\\nupdated Cochrane reviews had 20 new studies identiﬁed by\\nthe review authors and added in the updates. Recall of these\\n20 ranged from 1.00 for the related articles method to a low\\nof 0.80 CQs (Table 2). Our team identiﬁed 277 new studies\\nas relevant for inclusion in the 10 evidence reports that\\nwere updated. All test searches showed lower recall for this\\ncohort than for the Cochrane reviews and here the CQ out-\\nperformed both ranking searches (Table 2).\\n\\n3.1. Recall of CQ combined with a ranking method\\n\\nOf the 297 new relevant studies identiﬁed, the combina-\\ntion of CQ and related articles searches identiﬁed 270\\n(overall\\nin\\nthe Cochrane cohort and 0.90 (250/277) in the AHRQ\\ncohort.\\n\\nrecall of 0.91). Recall was 1.00 (20/20)\\n\\nThe combination of CQ and SVM identiﬁed 263 studies\\n(overall recall of 0.89). Recall was 1.00 (20/20) in the\\nCochrane cohort and 0.88 (243/277) in the AHRQ cohort.\\nWhen all three methods were used together, recall was\\n0.997 with 296 of the 297 studies identiﬁed.\\n\\n\\x0c112\\n\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nTable 2. Recall of eligible studies by the search methods\\n\\nCochrane reviews\\n\\nAHRQ evidence\\n\\nreports\\n\\nRetrieval method\\n\\nClinical query\\nRelated articles\\nSVM\\nTotal\\n\\nN\\n\\n16\\n20\\n19\\n20\\n\\nRecall\\n\\n0.80\\n1.00\\n0.95\\n\\nN\\n\\n188\\n176\\n129\\n277\\n\\nRecall\\n\\n0.68\\n0.64\\n0.47\\n\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\n\\nity; SVM, support vector machine.\\n\\nAcross the two cohorts, the overlap and unique component from\\nthe retrieval of relevant records by the test searches was examined\\n(Fig. 1).\\n\\n3.2. Consistency across clinical areas\\n\\nThirteen clinical areas were represented in the larger\\nstudy that included 72 journal-published reviews, but only\\neight of those clinical areas had two or more new relevant\\nstudies in the smaller Cochrane and AHRQ cohorts. Fig. 2\\nshows recall of new studies by the three types of searches,\\nfor these eight clinical areas.\\n\\nCombined recall of the CQ Boolean search paired with a\\nranking search is shown in Fig. 3. The combination showed\\ncomplete recall of relevant new studies in three of eight\\nareas when CQ was paired with related articles and recall\\nof 0.67 or higher in all areas. Four of eight clinical areas\\nhad complete recall for the combination of CQ and SVM,\\nand recall was 0.80 or higher in all areas. Even for periph-\\neral vascular disease, where all three searches performed\\n\\nFig. 2. Recall of new studies by clinical area for each search method.\\nAbbreviations: SVM, support vector machine.\\n\\nfairly poorly individually, recall was 0.76 for the CQ/\\nrelated articles and 0.90 in the CQ/SVM pairing.\\n\\n3.3. Search precision\\n\\nPrecision, across the 10 AHRQ reviews, was 0.11 for the\\nCQ, 0.22 for related articles, and 0.19 for SVM (Table 3).\\n\\nFig. 1. Overlapping and unique retrieval of relevant studies by each\\nsearch method. The size of the circle is proportional to the size of\\nthe retrieval, but the overlap and unique portions are approximations.\\nExact ﬁgures for the components are labeled. Abbreviations: SVM,\\nsupport vector machine.\\n\\nFig. 3. Recall of new studies by clinical area for search methods in\\ncombination. Abbreviations: CQ, clinical query; SVM, support vector\\nmachine.\\n\\n\\x0cTable 3. Overall precision in the AHRQ Sample\\n\\n4. Discussion\\n\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\n113\\n\\nRetrieval method\\n\\nClinical query\\nRelated articles\\nSVM\\nClinical query þ related\\nClinical query þ SVM\\nClinical query þ related\\n\\narticles\\n\\nEligible\\nstudies\\nretrieved\\n\\n187\\n176\\n128\\n250\\n\\n243\\n276\\n\\nNo. of candidates\\n\\nretrieved\\n\\nPrecision\\n\\n1,637\\n814\\n659\\n2,318a\\n\\n2,247a\\n3,264a\\n\\n0.11\\n0.22\\n0.19\\n0.11\\n\\n0.11\\n0.08\\n\\narticles þ SVM\\nAbbreviations: AHRQ, Agency for Healthcare Research and\\n\\nQuality; SVM, support vector machine.\\n\\na Number of candidates after removal of duplicate records.\\n\\nPrecision for the individual reviews ranged from 0.0 to 0.38\\nfor CQs, 0.004 to 0.80 for related articles, and 0.01 to 0.43\\nfor SVM. Overall precision was 0.11 for CQ and either\\nrelated articles or SVM. Overall precision was 0.08 when\\nCQ and both related articles and SVM were used. For\\nranked searches where a ﬁxed number of records will be\\nscreened, that number is always the denominator, so preci-\\nsion will tend to increase as the number of relevant records\\nincreasesdthe maximum retrieval size for SVM was cap-\\nped at 200 records.\\n\\n3.4. Retrieval consistency over time\\n\\nSearches were retested for the AHRQ cohort using\\nPubMed results obtained February 24, 2015. Recall of the\\nrelated articles searches across the 10 AHRQ reports was\\n0.64 originally and 0.50 when repeated (Fig. 4). Of the\\nthree extreme cases, one rose from 0.00 to 0.80, one re-\\nmained at 0.00, and one dropped from 1.00 to 0.60. CQ\\nhad overall\\nrecall of 0.68 originally and 0.70 when\\nrepeated. Changes in extreme cases were minimal. Overall\\ncombined recall of CQ and related article was 0.90 origi-\\nnally,\\nfalling to 0.85 when repeated. Considering the\\nextreme values, all review with combined recall of 1.00\\noriginally remained at 1.00, whereas the one review with\\n0.00 recall in combination showed recall of 0.80 when\\nrepeated.\\n\\n1.0\\n0.9\\n0.8\\n0.7\\n0.6\\n0.5\\n0.4\\n0.3\\n0.2\\n0.1\\n0.0\\n\\nOriginal\\n\\n2015\\n\\nClinical Query\\n\\nRelated Ar(cid:415)cles\\n\\nCombined\\n\\nFig. 4. Recall of new studies for search methods, alone and in combi-\\nnation originally (left) and when retested in 2015 (right).\\n\\nWe expected that\\n\\nthe sophisticated SVM approach\\nwould perform well when paired with more stripped-\\ndown, focused Boolean searches. Indeed, the two together\\nwere able to replace the multidatabase, multimodel\\nsearches used by the original review teams in updating\\nthe Cochrane searches. The new relevant studies for the Co-\\nchrane reviews may have been relatively easy to ﬁnd, but\\nthe pairing was also effective in the AHRQ set, and there\\nshowed better precision than is usually seen with traditional\\nsearches for systematic review of RCTs [9]. The AHRQ set\\nwas formed from the new studies identiﬁed for more com-\\nplex interventions in a rigorous, well-funded study [16].\\n\\nWe were surprised that the related articles approach per-\\nformed almost as well as SVM when used in combination\\nwith the CQ. The related article search has advantages over\\nSVMdit requires far less data preparation, and no special\\nsoftware is needed for its use. This makes it useful not only\\nin updating, but also, if appropriate seed articles can be\\nfound, in original reviews.\\n\\nThat both SVM and related articles sometimes showed\\npoor recall when used alone, but consistently good recall\\nwhen used with a Boolean method suggests that there is\\nreal beneﬁt in using complementary search methods to\\nquerying MEDLINE. The related article method is not very\\ntime consuming, and easily added to other planned search\\nefforts, while the third method used here, SVM, is more\\ntechnical. The combined performance of all three methods\\nwas surprising, but such a setup might become unpractical\\nin day-to-day use.\\n\\nOther investigators have similar ﬁndings. Examination\\nof the supplemental material presented in the Appendix C\\nto the article by Waffenschmidt et al. [18] reveals that\\nacross the 19 reviews tested, the combination of a simple\\nstructured Boolean search (SSBS) and the ﬁrst 50 retrieval\\nof PubMed similar articles showed complete retrieval of all\\nreference standard articles in 14 of 19 reviews and never\\nless than 0.90 recall (Appendix C at www.jclinepi.com pre-\\nsents data from the table by Waffenschmidt graphically).\\nWaffenschmidt concluded ‘‘the combination of these two\\nsearch techniques that are independent of each other seems\\nto compensate the respective weaknesses.’’\\n\\nSSBS search by Waffenschmidt was constructed in\\nPubMed from search terms selected for the indication and\\nintervention with PubMed’s narrow CQ ﬁlter (category:\\ntherapy). Their search using the similar articles feature (Re-\\nlCits) did not use a set of seed articles; rather, the RelCits\\nfunction was applied for each relevant citation previously\\nidentiﬁed in PubMed. Their test articles were the included\\nstudies in 19 systematic reviews of drugs.\\n\\nAgoritsas et al. also described search construction\\nmethods for searches based on CQ and PubMed similar ar-\\nticles, tested for their ability to retrieve the included studies\\nof 30 Cochrane reviews [17]. Although their search con-\\nstruction methods differed from those used here, both\\n\\n\\x0c114\\n\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\nsearches were derived using standard methods. Their\\napproach to the structured Boolean search used terms from\\nthe population, intervention, and comparison with the CQ,\\nlimited to humans and English. Two clinicians selected the\\nPubMed similar articles seeds from the initial PubMed\\nretrieval. They noted that no one method provided consis-\\ntently high retrieval [17].\\n\\nThus, the robust nature of this pairing of Boolean and\\nnon-Boolean searches has been shown in several contexts,\\ngiving support\\nto the hypothesis that methods such as\\nrelated articles and SVM can compensate for the variation\\ninherent\\nin selecting search terms or assigning subject\\nheadings during indexing.\\n\\nConsidering stability of the searches over time, the\\nminimal change seen in the performance of the CQ is\\nlikely explained by indexing changes of a few records.\\nThe computation appears not to have changed. This sug-\\ngests that indexing changes may also impact SVM results\\nover time; however, this is likely to result in improved\\nperformance, as was seen with the CQ. The computation\\nof similar articles is still described by National Library\\nof Medicine as being based on the algorithm described\\nby Kim et al. [26]. That algorithm would suggest changes\\nover time in the nearest neighbor score as the frequency of\\ncertain terms in the MEDLINE corpus changes. A full\\nexploration of the changes in nearest neighbor scores of\\nsimilar articles over time is beyond the scope of this\\nstudy. It should be noted that when the similar articles\\nfeature was initially studied, it was simple to submit seed\\narticles, and then add limits such as date or RCT publica-\\ntion type. On retesting, such additional limits were more\\ncomplex to apply, and the elink utility seemed to be the\\nmost practical approach to identifying the related articles\\n[27].\\n\\nThe beneﬁt of the complementary searches, relative to\\nBoolean searching alone, was greatest for AHRQ evidence\\nreports and less for the simpler Cochrane reviews, suggest-\\ning that this approach may be particularly useful for more\\ncomplex evidence. There are examples of its utility in the\\nliterature. For example, in a realist review of a multidisci-\\nplinary body of\\nliterature identifying six domains of\\nClinical Practice Guideline implementability, the use of\\nPubMed’s similar articles feature as a third stage of search-\\ning identiﬁed 131 records of which 104 were relevant [28].\\nIn a systematic review of evidence on the links between\\npatient experience and clinical safety and effectiveness,\\nthe authors used PubMed similar articles to snowball on\\narticles\\nidentiﬁed through an EMBASE search to\\novercome the limitations of predeﬁned searches for com-\\nplex evidence [29].\\n\\nThere is always the possibility that a search, or even a\\npair of searches, will fail. One method to detect such\\nfailures is to test whether the search strategies ﬁnd known\\nrelevant items [30]. In the updating case, this is easily done\\nusing the included studies of the original review as a test\\n\\nset, allowing the review to determine the MEDLINE\\ncoverage of their particular topic at the same time.\\n\\n4.1. Limitations\\n\\nThere are two limitations to our proposed strategy. Other\\ndatabases should be searched in the unusual event that\\nnumerous studies, representing more than a small propor-\\ntion of the total N, are not included in MEDLINE. Second,\\nwhen it is important to ﬁnd articles too new to be indexed\\nby MEDLINE, systematic reviewers may wish to conduct a\\nsimple PubMed search limited to the nonindexed subsets\\n[31,32].\\n\\n5. Conclusion\\n\\nThe general approach of a Boolean plus a ranking\\nsearch is effective in MEDLINE retrieval for systematic\\nreviews. Very high levels of identiﬁcation of relevant\\nMEDLINE records, with adequate precision, are possible\\nusing a focused Boolean search complemented by a docu-\\nment similarity or ranking method. The efﬁcacy of a\\nfocused Boolean search paired with a search using the\\nPubMed similar articles feature is in agreement with pre-\\nvious studies [17,18], and this study shows that this com-\\nplementary effect also occurs when SVM is used as the\\nadditional method. The beneﬁt of using two complemen-\\ntary approaches to achieving high recall in MEDLINE is\\na robust effect.\\n\\nPubMed-related articles is a parsimonious method, as it\\nis readily available to all review teams without cost. The\\napproach is robust across clinical domains, and the effect\\nhas now been demonstrated in several samples. The method\\nmay be sufﬁcient for updating systematic reviews of inter-\\nventions and may be used for new reviews of interventions\\nwhen paired with a trials registry search. It is likely to work\\nin any type of search where MEDLINE provides good sub-\\nject coverage even if retrieval through traditional search\\nmethods has been challenging.\\n\\nAcknowledgments\\n\\nMohamed Ansari and Jun Ji undertook screening and\\nrelevance assessment of candidate studies retrieved by the\\ntest\\nsearches, Tamara Rader developed the Boolean\\nsearches, and Raymond Daniel processed related articles\\nsearches and obtained full-text articles. David Moher cosu-\\npervised Margaret Sampson doctoral dissertation and pro-\\nvided invaluable advice and guidance.\\n\\nSupplementary data\\n\\nSupplementary data related to this article can be found at\\n\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004.\\n\\n\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\n\\n115\\n\\nReferences\\n\\n[1] Karimi S, Pohl S, Scholer F, Cavedon L, Zobel J. Boolean versus\\nranked querying for biomedical systematic reviews. BMC Med\\nInform Decis Mak 2010;10(1):58.\\n\\n[2] Reveiz L, Ospina E, Zorrilla AFC. Should we consider Embase in\\nLatin America? J Clin Epidemiol 2004;57:866; author reply 867e8.\\n[3] Rathbone J, Carter M, Hoffmann T, Glasziou P. Better duplicate\\ndetection for systematic reviewers: evaluation of Systematic Review\\nAssistant-Deduplication Module. Syst Rev 2015;4(1):6.\\n\\n[4] Dickersin K, Scherer R, Lefebvre C. Identifying relevant studies for\\n\\nsystematic reviews. BMJ 1994;309:1286e91.\\n\\n[5] Bramer WM, Giustini D, Kramer BM, Anderson P. The comparative\\nrecall of Google Scholar versus PubMed in identical searches for\\nbiomedical systematic reviews: a review of searches used in system-\\natic reviews. Syst Rev 2013;2(1):115.\\n\\n[6] Wieland LS, Robinson KA, Dickersin K. Understanding why evi-\\ndence from randomised clinical trials may not be retrieved from Med-\\nline: comparison of indexed and non-indexed records. BMJ 2012;\\n344:d7501.\\n\\n[7] Suarez-Almazor ME, Belseck E, Homik J, Dorgan M, Ramos-\\nRemus C. Identifying clinical trials in the medical literature with\\nelectronic databases: MEDLINE alone is not enough. Control Clin\\nTrials 2000;21:476e87.\\n\\n[8] O’Leary N, Tiernan E, Walsh D, Lucey N, Kirkova J, Davis MP. The\\npitfalls of a systematic MEDLINE review in palliative medicine: symp-\\ntom assessment instruments. Am J Hosp Palliat Care 2007;24:181e4.\\n[9] Sampson M, Tetzlaff J, Urquhart C. Precision of healthcare system-\\natic review searches in a cross-sectional sample. Res Synth Methods\\n2011;2:119e25.\\n\\n[10] Greenhalgh T, Peacock R. Effectiveness and efﬁciency of search\\nmethods in systematic reviews of complex evidence: audit of primary\\nsources. BMJ 2005;331:1064e5.\\n\\n[11] Badgett RG, Dylla DP, Megison SD, Glynn Harmon E. An experimental\\nsearch strategy retrieves more precise results than PubMed and Google\\nfor questions about medical interventions. PeerJ 2015;3:e913.\\n\\n[12] O’Mara-Eves A, Thomas J, McNaught J, Miwa M, Ananiadou S.\\nUsing text mining for study identiﬁcation in systematic reviews: a\\nsystematic review of current approaches. Syst Rev 2015;4(1):5.\\n\\n[13] Bekhuis T, Demner-Fushman D. Towards automating the initial\\nscreening phase of a systematic review. Stud Health Technol Inform\\n[Internet] 2010;160(Pt 1):146e50. Available at http://www.ncbi.nlm.\\nnih.gov/pubmed/20841667. Accessed January 2014.\\n\\n[14] Wallace BC, Trikalinos TA, Lau J, Brodley C, Schmid CH. Semi-auto-\\nmated screening of biomedical citations for systematic reviews. BMC\\nBioinformatics 2010;11(1):55. Available at http://www.citeulike.org/\\nuser/daforerog/article/6595002. Accessed November 2010.\\n\\n[15] Sampson M, Shojania KG, McGowan J, Daniel R, Rader T,\\nIansavichene AE, et al. Surveillance search techniques identiﬁed the\\nneed to update systematic reviews. J Clin Epidemiol 2008;61:755e62.\\n[16] Shojania KG, Sampson M, Ansari MT, Ji J, Garritty C, Doucette S,\\net al. Updating systematic reviewsetechnical review 16. Health\\n(San Francisco). Rockville, MD: Agency for Health Care Policy\\nand Research; 2007.\\n\\n[17] Agoritsas T, Merglen A, Courvoisier DS, Combescure C, Garin N,\\nPerrier A, et al. Sensitivity and predictive value of 15 PubMed search\\n\\nstrategies to answer clinical questions rated against full systematic re-\\nviews. J Med Internet Res 2012;14(3):e85.\\n\\n[18] Waffenschmidt S, Janzen T, Hausner E, Kaiser T. Simple search tech-\\nniques in PubMed are potentially suitable for evaluating the\\ncompleteness of systematic reviews. J Clin Epidemiol 2013;66:\\n660e5.\\n\\n[19] Purpose and procedure [Internet]. ACP Journal Club. [cited 2007 Feb 7].\\nAvailable at http://annals.org/SS/ACPJC_Purpose_and_Procedure.aspx.\\n[20] Hedges Team. Search strategies for MEDLINE in Ovid Syntax and\\nthe PubMed translation. [Internet]. Health Information Research Unit\\nwebsite; 2010. Available at http://hiru.mcmaster.ca/hiru/HIRU_\\nHedges_MEDLINE_Strategies.aspx. Accessed January 2011.\\n\\n[21] Vapnik V. The nature of statistical\\n\\nlearning theory. New York:\\n\\nSpringer; 1995.\\n\\n[22] Joachims T. Text categorization with support vector machines:\\nlearning with many relevant features. Machine Learning: ECML-\\n98. Berlin: Springer; 1998:137e42.\\n\\n[23] Joachims T. Making large-scale SVM learning practical.\\n\\nIn:\\nSch€olkopf B, Burges CJC, Smola AJ, editors. Advances in kernel\\nmethods: support vector\\nlearning [Internet]. MIT Press; 1999.\\nAvailable at http://dl.acm.org/citation.cfm?id5299094. Accessed\\nSeptember 2015.\\n\\n[24] Glanville JM, Lefebvre C, Miles JN, Camosso-Steﬁnovic J. How to\\nidentify randomized controlled trials in MEDLINE: ten years on. J\\nMed Libr Assoc 2006;94:130e6.\\n\\n[25] Hulsen T, de Vlieg J, Alkema W. BioVenn - a web application for the\\nlists using area-\\n\\ncomparison and visualization of biological\\nproportional Venn diagrams. BMC Genomics 2008 Jan;9(1):488.\\n\\n[26] Kim W, Aronson AR, Wilbur WJ. Automatic MeSH term assignment\\n\\nand quality assessment. Proc AMIA Symp 2001;319e23.\\n\\n[27] Sayers E. The E-utilities in-depth: parameters, syntax and more\\n[Internet]. National Center for Biotechnology Information (US);\\n2015. Available at http://www.ncbi.nlm.nih.gov/books/NBK25499/.\\nAccessed March 2015.\\n\\n[28] Kastner M, Bhattacharyya O, Hayden L, Makarski J, Estey E,\\nDurocher L, et al. Guideline uptake is inﬂuenced by six implement-\\nability domains for creating and communicating guidelines: a realist\\nreview. J Clin Epidemiol 2015;68:498e509.\\n\\n[29] Doyle C, Lennox L, Bell D. A systematic review of evidence on the\\nlinks between patient experience and clinical safety and effective-\\nness. BMJ Open [Internet] 2013;3(1). Available at http://bmjopen.\\nbmj.com/content/3/1/e001570.abstract Accessed March 2015.\\n\\n[30] Sampson M, Mcgowan J. Inquisitio validus Index Medicus: a simple\\nmethod of validating MEDLINE systematic review searches. Res\\n[internet] 2011;2:103e9. Available at http://\\nSynth Methods\\nonlinelibrary.wiley.com/doi/10.1002/jrsm.40, Accessed September\\n28, 2015.\\n\\n[31] Duffy S, Misso K, Noake C, Ross J, Stirk L. Supplementary searches\\nof PubMed to improve currency of MEDLINE and MEDLINE\\nIn-Process searches via OvidSP. UK InterTASC Information Special-\\nists’ Sub-Group Workshop, July 9. University of Exeter [Internet];\\n2014. Available at http://www.systematic-reviews.com/. Accessed\\nAugust 2014.\\n\\n[32] Thompson JC, Quigley JM, Halfpenny NJA, Scott DA, Hawkins NS.\\nImportance and methods of searching for E-publications ahead of\\nprint in systematic reviews. Evid Based Med 2016;21:55e9.\\n\\n\\x0c']]"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfRender()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(finalDocumentSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_intro = re.search(r'Introduction(.*?)Theoretical background', str(finalDocumentSet)).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-575-e75927f6522f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mconclusion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mconclusionData\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfinalDocumentSet\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mdata_conclusion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'Conclusion(.*?)Acknowledgements'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconclusionData\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mconclusion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_conclusion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "global conclusion\n",
    "conclusion = []\n",
    "for conclusionData in finalDocumentSet:\n",
    "    data_conclusion = re.search(r'Conclusion(.*?)Acknowledgements', str(conclusionData)).group(1)\n",
    "    conclusion.append(data_conclusion)\n",
    "    \n",
    "print(len(conclusion))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-588-4b260025c619>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_conclusion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'5. Conclusion(.*?)Acknowledgments'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m|\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34mr'Conclusion(.*?)Acknowledgement'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinalDocumentSet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "data_conclusion = re.search(r'5. Conclusion(.*?)Acknowledgments', str(finalDocumentSet)).group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\n\\\\nThe general approach of a Boolean plus a ranking\\\\nsearch is effective in MEDLINE retrieval for systematic\\\\nreviews. Very high levels of identiﬁcation of relevant\\\\nMEDLINE records, with adequate precision, are possible\\\\nusing a focused Boolean search complemented by a docu-\\\\nment similarity or ranking method. The efﬁcacy of a\\\\nfocused Boolean search paired with a search using the\\\\nPubMed similar articles feature is in agreement with pre-\\\\nvious studies [17,18], and this study shows that this com-\\\\nplementary effect also occurs when SVM is used as the\\\\nadditional method. The beneﬁt of using two complemen-\\\\ntary approaches to achieving high recall in MEDLINE is\\\\na robust effect.\\\\n\\\\nPubMed-related articles is a parsimonious method, as it\\\\nis readily available to all review teams without cost. The\\\\napproach is robust across clinical domains, and the effect\\\\nhas now been demonstrated in several samples. The method\\\\nmay be sufﬁcient for updating systematic reviews of inter-\\\\nventions and may be used for new reviews of interventions\\\\nwhen paired with a trials registry search. It is likely to work\\\\nin any type of search where MEDLINE provides good sub-\\\\nject coverage even if retrieval through traditional search\\\\nmethods has been challenging.\\\\n\\\\n'"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\\\n\\\\nSystematic review searches need to have high recall.\\\\nMechanisms to achieve this usually include expansive\\\\nBoolean searches of multiple databases. This approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nMEDLINE gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. However,\\\\n\\\\nConﬂict of interest: None.\\\\nFunding: Some of this work was funded by the US Agency for Health-\\\\ncare Research and Quality, Department of Health and Human Services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* Corresponding author. Tel.: 613-737-7600.\\\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through MEDLINE. Recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in MEDLINE, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nSuccessful retrieval through a Boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. The search of\\\\nmultiple databases can therefore be helpful. The target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. A text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. These tactics on the part of the searcher lead\\\\n\\\\n\\\\x0c', 'Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nComplementary approaches to searching MEDLINE may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\\\n\\\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\\\n\\\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\\\n\\\\n0R6, Canada\\\\n\\\\nUnited Kingdom\\\\n\\\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\\\n\\\\nAccepted 7 March 2016; Published online 11 March 2016\\\\n\\\\nAbstract\\\\n\\\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\\\n\\\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\\\ncombined. Related articles showed least stability over time.\\\\n\\\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\\\n\\\\n1. Introduction\\\\n\\\\nSystematic review searches need to have high recall.\\\\nMechanisms to achieve this usually include expansive\\\\nBoolean searches of multiple databases. This approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nMEDLINE gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. However,\\\\n\\\\nConﬂict of interest: None.\\\\nFunding: Some of this work was funded by the US Agency for Health-\\\\ncare Research and Quality, Department of Health and Human Services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* Corresponding author. Tel.: 613-737-7600.\\\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through MEDLINE. Recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in MEDLINE, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nSuccessful retrieval through a Boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. The search of\\\\nmultiple databases can therefore be helpful. The target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. A text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. These tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nWhat is new?\\\\n\\\\nKey ﬁndings\\\\n(cid:1) Searches using known relevant studies and the\\\\nsimilar articles feature of PubMed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) For a given question, if the Boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. The two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) The precision of\\\\n\\\\nmethod appears better\\\\nexhaustive Boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nWhat this adds to what was known?\\\\n(cid:1) The paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nIt is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nWhat is the implication and what should change\\\\nnow?\\\\n(cid:1) Using the simple and universally available PubMed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) If the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nMEDLINE.\\\\n\\\\nto large retrievals with low precision [9]. These resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional Boolean searches have\\\\nbeen sought [10].\\\\n\\\\nMost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nThese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. These methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nWe examined one method, support vector machine\\\\n(SVM), and compared it with a simple and readily available\\\\nmethod based on the PubMed similar articles feature. We\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. We paired both\\\\nwith a focused Boolean search within MEDLINE. We\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for SVM and seed articles for the PubMed-\\\\n\\\\nrelated articles search. We therefore sought to determine\\\\nif a focused Boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\nComparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. In this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (CQ), and PubMed-related articles are tested in a\\\\ncohort of six updated Cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 Agency for Healthcare\\\\nResearch and Quality (AHRQ) evidence reports. The\\\\nCochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. The replication in the\\\\nAHRQ cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. All records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nOther research [17,18] suggests that searches using the\\\\nPubMed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a Boolean-type search of\\\\nMEDLINE. We tested an additional search method, SVM,\\\\nin the Cochrane and AHRQ samples to permit comparison\\\\nwith our PubMed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the PubMed\\\\nsimilar articles method.\\\\n\\\\nThe aim of this article was to test whether the combined\\\\napproach of a focused Boolean search paired with a second\\\\nsearch using the similar articles feature of PubMed or SVM\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. Methods\\\\n\\\\n2.1. Formation of the study cohorts\\\\n\\\\nThis analysis uses a data set created for an updating\\\\nstudy sponsored by AHRQ [16]. Methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\\\nsearch of the ACP Journal Club database (Ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nAHRQ reports were identiﬁed through the PubMed\\\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\\\njrid21544]. Screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. Screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c', 'Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nComplementary approaches to searching MEDLINE may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\\\n\\\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\\\n\\\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\\\n\\\\n0R6, Canada\\\\n\\\\nUnited Kingdom\\\\n\\\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\\\n\\\\nAccepted 7 March 2016; Published online 11 March 2016\\\\n\\\\nAbstract\\\\n\\\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\\\n\\\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\\\ncombined. Related articles showed least stability over time.\\\\n\\\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\\\n\\\\n1. Introduction\\\\n\\\\nSystematic review searches need to have high recall.\\\\nMechanisms to achieve this usually include expansive\\\\nBoolean searches of multiple databases. This approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nMEDLINE gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. However,\\\\n\\\\nConﬂict of interest: None.\\\\nFunding: Some of this work was funded by the US Agency for Health-\\\\ncare Research and Quality, Department of Health and Human Services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* Corresponding author. Tel.: 613-737-7600.\\\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through MEDLINE. Recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in MEDLINE, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nSuccessful retrieval through a Boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. The search of\\\\nmultiple databases can therefore be helpful. The target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. A text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. These tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nWhat is new?\\\\n\\\\nKey ﬁndings\\\\n(cid:1) Searches using known relevant studies and the\\\\nsimilar articles feature of PubMed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) For a given question, if the Boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. The two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) The precision of\\\\n\\\\nmethod appears better\\\\nexhaustive Boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nWhat this adds to what was known?\\\\n(cid:1) The paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nIt is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nWhat is the implication and what should change\\\\nnow?\\\\n(cid:1) Using the simple and universally available PubMed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) If the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nMEDLINE.\\\\n\\\\nto large retrievals with low precision [9]. These resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional Boolean searches have\\\\nbeen sought [10].\\\\n\\\\nMost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nThese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. These methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nWe examined one method, support vector machine\\\\n(SVM), and compared it with a simple and readily available\\\\nmethod based on the PubMed similar articles feature. We\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. We paired both\\\\nwith a focused Boolean search within MEDLINE. We\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for SVM and seed articles for the PubMed-\\\\n\\\\nrelated articles search. We therefore sought to determine\\\\nif a focused Boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\nComparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. In this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (CQ), and PubMed-related articles are tested in a\\\\ncohort of six updated Cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 Agency for Healthcare\\\\nResearch and Quality (AHRQ) evidence reports. The\\\\nCochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. The replication in the\\\\nAHRQ cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. All records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nOther research [17,18] suggests that searches using the\\\\nPubMed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a Boolean-type search of\\\\nMEDLINE. We tested an additional search method, SVM,\\\\nin the Cochrane and AHRQ samples to permit comparison\\\\nwith our PubMed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the PubMed\\\\nsimilar articles method.\\\\n\\\\nThe aim of this article was to test whether the combined\\\\napproach of a focused Boolean search paired with a second\\\\nsearch using the similar articles feature of PubMed or SVM\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. Methods\\\\n\\\\n2.1. Formation of the study cohorts\\\\n\\\\nThis analysis uses a data set created for an updating\\\\nstudy sponsored by AHRQ [16]. Methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\\\nsearch of the ACP Journal Club database (Ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nAHRQ reports were identiﬁed through the PubMed\\\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\\\njrid21544]. Screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. Screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c110\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nFor inclusion, all reviews must have included random-\\\\nized controlled trials (RCTs) and provided meta-analysis\\\\nfor at least one outcome. The MEDLINE search strategies\\\\nhad to be reported in enough detail to permit replication.\\\\nFifteen AHRQ evidence reports thought\\\\nlikely to have\\\\nimportant new evidence were selected and used to validate\\\\nthe updating methods used in the main cohort.\\\\n\\\\nCochrane reviews were selected to meet minimum qual-\\\\nity and relevance standards, as deﬁned by ACP Journal\\\\nClub [19]. We also required that the review had been up-\\\\ndated and that the text of both the original review and an\\\\nupdated version was available. The search for the original\\\\nreview must have included MEDLINE and at least one\\\\nother electronic bibliographic database and one or more\\\\nnondatabase method such as hand searching or checking\\\\nreference lists. Such comprehensive searching was likely\\\\nto identify most relevant\\\\nliterature and form a useful\\\\ntraining set of included examples. At least 10 RCTs or\\\\nquasi-RCTs must have been included in the original review\\\\nto give an adequate training set for SVM.\\\\n\\\\n2.2. Test searches\\\\n\\\\n2.2.1. Clinical query\\\\n\\\\nBoolean searches were developed by a librarian experi-\\\\nenced in systematic review searches using a protocol\\\\n(Appendix A at www.jclinepi.com). Search strategies were\\\\ndeliberately simple, usually consisting of two or three\\\\nMedical Subject Heading (MeSH) terms representing the\\\\npopulation and intervention of the review. This search\\\\nwas limited by a CQ ﬁlter to ‘‘therapy (best balance of\\\\nsensitivity\\\\n‘‘Randomized\\\\ncontrolled trial.pt. or\\\\nrandomized.mp. or placebo.mp.’’\\\\n[20]. Boolean searches were run in OVID MEDLINE.\\\\n\\\\nspeciﬁcity)’’\\\\n\\\\nthat\\\\n\\\\nand\\\\n\\\\nis,\\\\n\\\\n2.2.2. Related articles\\\\n\\\\nSearches using the PubMed similar articles feature were\\\\nperformed using the PubMed unique identiﬁers (PMID) of\\\\nthe three newest and three largest studies included in the\\\\noriginal review as seed articles. There was no replacement\\\\nin the event of overlap between the largest and newest sets,\\\\nor if one of those studies was not indexed in MEDLINE.\\\\n(All studies included in the original reviews were checked\\\\nfor indexing status in MEDLINE.) The resulting set was\\\\nlimited to the publication type RCT and date limited to\\\\nthe period since the search date of the original review.\\\\n\\\\n2.2.3. Support vector machine\\\\n\\\\nThe SVM searches used for this project have not been\\\\npreviously described so the methods will be reported in\\\\nmore detail. SVM is a well established and broadly applied\\\\nclassiﬁcation method from machine learning [21,22]. A\\\\nclassiﬁcation task is closely related to an information\\\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\\\nthe collection. As SVM is a supervised algorithm,\\\\nit\\\\n\\\\nrequires training examples with known labels. It then places\\\\nthese training examples in a high-dimensional vector space\\\\nin such a way that examples of one class are separated from\\\\nexamples of the other class with the greatest distance to the\\\\nseparating boundary. Predicting the class of a new instance\\\\nhappens by placing it in the same space and determining on\\\\nwhich side of the boundary it falls. The distance to the\\\\nboundary is an indication of the prediction conﬁdence and\\\\nis used to rank instances (documents) as is essential in in-\\\\nformation retrieval tasks or to provide a cutoff threshold.\\\\n\\\\nIn our setup, the training set consisted of search results\\\\nfrom the original review formed with included instances\\\\nbeing the studies included in the review (the relevant re-\\\\ntrievals) and excluded instances being the studies found\\\\nby the search used in the original review but excluded from\\\\nthe review (irrelevant retrievals). The new examples to be\\\\nclassiﬁed were articles indexed since the date of the search\\\\nperformed in the original review.\\\\n\\\\nSVM searches were run on MEDLINE data stored\\\\nlocally at\\\\nthe National Research Council of Canada.\\\\nMEDLINE was refreshed before running the searches.\\\\nThe MEDLINE records were represented as bags of fea-\\\\ntures, where features consisted of lowercased words and\\\\nword combinations (up to four words) from the title and ab-\\\\nstract ﬁelds, full MeSH terms, and contents of all other\\\\nﬁelds in the MEDLINE record. Features were uniformly\\\\nweighted. The SVM implementation used was SVM Light\\\\n[23] used with a linear kernel.\\\\n\\\\nThe approach was piloted with one AHRQ evidence\\\\nreport, and various conﬁgurations were tried, observing\\\\nthe placement of several known new relevant studies within\\\\nthe retrieval. The MEDLINE set was ﬁltered with the\\\\nrevised Highly Sensitive Search Strategy (HSSS) [24]. Pre-\\\\nliminary testing indicated that this ﬁlter would retrieve\\\\n99.1% of MEDLINE-indexed relevant new evidence from\\\\nthe larger cohort of journal-published systematic reviews\\\\n(n 5 72) and Cochrane reviews (n 5 27). This HSSS ﬁlter\\\\nwas added primarily to improve processing speed, relative\\\\nto running the classiﬁer against all of MEDLINE. With\\\\nthe ﬁlter, processing for one systematic review took about\\\\n10 minutes. Next, a coarse-grained ﬁlter was applied;\\\\nincluded and excluded instances were combined in a single\\\\nset labeled POS. Adjacent PubMed IDs (the next higher\\\\nPMID to each member of POS, as long as that higher num-\\\\nber was not itself a member of POS) were combined in a set\\\\nto represent WORLD. The model was trained on POS vs.\\\\nWORLD, using words and phrases from title, abstract,\\\\nauthor names, and journal name. This model was run\\\\nagainst the records passing the HSSS ﬁlter.\\\\n\\\\nFour different subsets of\\\\n\\\\nthe SVM retrieval were\\\\nexamineddone set included all records with a relevance\\\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\\\n(SVM200point5). Other sets consisted of all records with\\\\nrelevance scores of 0.95 or more (SVM95), 0.90 or more\\\\n(SVM90), and 0.80 or more (SVM80). PubMed ID and\\\\nrelevance scores were recorded, and PubMed IDs were\\\\n\\\\n\\\\x0c', 'Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nComplementary approaches to searching MEDLINE may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\\\n\\\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\\\n\\\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\\\n\\\\n0R6, Canada\\\\n\\\\nUnited Kingdom\\\\n\\\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\\\n\\\\nAccepted 7 March 2016; Published online 11 March 2016\\\\n\\\\nAbstract\\\\n\\\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\\\n\\\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\\\ncombined. Related articles showed least stability over time.\\\\n\\\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\\\n\\\\n1. Introduction\\\\n\\\\nSystematic review searches need to have high recall.\\\\nMechanisms to achieve this usually include expansive\\\\nBoolean searches of multiple databases. This approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nMEDLINE gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. However,\\\\n\\\\nConﬂict of interest: None.\\\\nFunding: Some of this work was funded by the US Agency for Health-\\\\ncare Research and Quality, Department of Health and Human Services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* Corresponding author. Tel.: 613-737-7600.\\\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through MEDLINE. Recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in MEDLINE, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nSuccessful retrieval through a Boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. The search of\\\\nmultiple databases can therefore be helpful. The target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. A text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. These tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nWhat is new?\\\\n\\\\nKey ﬁndings\\\\n(cid:1) Searches using known relevant studies and the\\\\nsimilar articles feature of PubMed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) For a given question, if the Boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. The two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) The precision of\\\\n\\\\nmethod appears better\\\\nexhaustive Boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nWhat this adds to what was known?\\\\n(cid:1) The paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nIt is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nWhat is the implication and what should change\\\\nnow?\\\\n(cid:1) Using the simple and universally available PubMed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) If the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nMEDLINE.\\\\n\\\\nto large retrievals with low precision [9]. These resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional Boolean searches have\\\\nbeen sought [10].\\\\n\\\\nMost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nThese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. These methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nWe examined one method, support vector machine\\\\n(SVM), and compared it with a simple and readily available\\\\nmethod based on the PubMed similar articles feature. We\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. We paired both\\\\nwith a focused Boolean search within MEDLINE. We\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for SVM and seed articles for the PubMed-\\\\n\\\\nrelated articles search. We therefore sought to determine\\\\nif a focused Boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\nComparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. In this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (CQ), and PubMed-related articles are tested in a\\\\ncohort of six updated Cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 Agency for Healthcare\\\\nResearch and Quality (AHRQ) evidence reports. The\\\\nCochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. The replication in the\\\\nAHRQ cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. All records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nOther research [17,18] suggests that searches using the\\\\nPubMed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a Boolean-type search of\\\\nMEDLINE. We tested an additional search method, SVM,\\\\nin the Cochrane and AHRQ samples to permit comparison\\\\nwith our PubMed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the PubMed\\\\nsimilar articles method.\\\\n\\\\nThe aim of this article was to test whether the combined\\\\napproach of a focused Boolean search paired with a second\\\\nsearch using the similar articles feature of PubMed or SVM\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. Methods\\\\n\\\\n2.1. Formation of the study cohorts\\\\n\\\\nThis analysis uses a data set created for an updating\\\\nstudy sponsored by AHRQ [16]. Methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\\\nsearch of the ACP Journal Club database (Ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nAHRQ reports were identiﬁed through the PubMed\\\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\\\njrid21544]. Screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. Screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c110\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nFor inclusion, all reviews must have included random-\\\\nized controlled trials (RCTs) and provided meta-analysis\\\\nfor at least one outcome. The MEDLINE search strategies\\\\nhad to be reported in enough detail to permit replication.\\\\nFifteen AHRQ evidence reports thought\\\\nlikely to have\\\\nimportant new evidence were selected and used to validate\\\\nthe updating methods used in the main cohort.\\\\n\\\\nCochrane reviews were selected to meet minimum qual-\\\\nity and relevance standards, as deﬁned by ACP Journal\\\\nClub [19]. We also required that the review had been up-\\\\ndated and that the text of both the original review and an\\\\nupdated version was available. The search for the original\\\\nreview must have included MEDLINE and at least one\\\\nother electronic bibliographic database and one or more\\\\nnondatabase method such as hand searching or checking\\\\nreference lists. Such comprehensive searching was likely\\\\nto identify most relevant\\\\nliterature and form a useful\\\\ntraining set of included examples. At least 10 RCTs or\\\\nquasi-RCTs must have been included in the original review\\\\nto give an adequate training set for SVM.\\\\n\\\\n2.2. Test searches\\\\n\\\\n2.2.1. Clinical query\\\\n\\\\nBoolean searches were developed by a librarian experi-\\\\nenced in systematic review searches using a protocol\\\\n(Appendix A at www.jclinepi.com). Search strategies were\\\\ndeliberately simple, usually consisting of two or three\\\\nMedical Subject Heading (MeSH) terms representing the\\\\npopulation and intervention of the review. This search\\\\nwas limited by a CQ ﬁlter to ‘‘therapy (best balance of\\\\nsensitivity\\\\n‘‘Randomized\\\\ncontrolled trial.pt. or\\\\nrandomized.mp. or placebo.mp.’’\\\\n[20]. Boolean searches were run in OVID MEDLINE.\\\\n\\\\nspeciﬁcity)’’\\\\n\\\\nthat\\\\n\\\\nand\\\\n\\\\nis,\\\\n\\\\n2.2.2. Related articles\\\\n\\\\nSearches using the PubMed similar articles feature were\\\\nperformed using the PubMed unique identiﬁers (PMID) of\\\\nthe three newest and three largest studies included in the\\\\noriginal review as seed articles. There was no replacement\\\\nin the event of overlap between the largest and newest sets,\\\\nor if one of those studies was not indexed in MEDLINE.\\\\n(All studies included in the original reviews were checked\\\\nfor indexing status in MEDLINE.) The resulting set was\\\\nlimited to the publication type RCT and date limited to\\\\nthe period since the search date of the original review.\\\\n\\\\n2.2.3. Support vector machine\\\\n\\\\nThe SVM searches used for this project have not been\\\\npreviously described so the methods will be reported in\\\\nmore detail. SVM is a well established and broadly applied\\\\nclassiﬁcation method from machine learning [21,22]. A\\\\nclassiﬁcation task is closely related to an information\\\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\\\nthe collection. As SVM is a supervised algorithm,\\\\nit\\\\n\\\\nrequires training examples with known labels. It then places\\\\nthese training examples in a high-dimensional vector space\\\\nin such a way that examples of one class are separated from\\\\nexamples of the other class with the greatest distance to the\\\\nseparating boundary. Predicting the class of a new instance\\\\nhappens by placing it in the same space and determining on\\\\nwhich side of the boundary it falls. The distance to the\\\\nboundary is an indication of the prediction conﬁdence and\\\\nis used to rank instances (documents) as is essential in in-\\\\nformation retrieval tasks or to provide a cutoff threshold.\\\\n\\\\nIn our setup, the training set consisted of search results\\\\nfrom the original review formed with included instances\\\\nbeing the studies included in the review (the relevant re-\\\\ntrievals) and excluded instances being the studies found\\\\nby the search used in the original review but excluded from\\\\nthe review (irrelevant retrievals). The new examples to be\\\\nclassiﬁed were articles indexed since the date of the search\\\\nperformed in the original review.\\\\n\\\\nSVM searches were run on MEDLINE data stored\\\\nlocally at\\\\nthe National Research Council of Canada.\\\\nMEDLINE was refreshed before running the searches.\\\\nThe MEDLINE records were represented as bags of fea-\\\\ntures, where features consisted of lowercased words and\\\\nword combinations (up to four words) from the title and ab-\\\\nstract ﬁelds, full MeSH terms, and contents of all other\\\\nﬁelds in the MEDLINE record. Features were uniformly\\\\nweighted. The SVM implementation used was SVM Light\\\\n[23] used with a linear kernel.\\\\n\\\\nThe approach was piloted with one AHRQ evidence\\\\nreport, and various conﬁgurations were tried, observing\\\\nthe placement of several known new relevant studies within\\\\nthe retrieval. The MEDLINE set was ﬁltered with the\\\\nrevised Highly Sensitive Search Strategy (HSSS) [24]. Pre-\\\\nliminary testing indicated that this ﬁlter would retrieve\\\\n99.1% of MEDLINE-indexed relevant new evidence from\\\\nthe larger cohort of journal-published systematic reviews\\\\n(n 5 72) and Cochrane reviews (n 5 27). This HSSS ﬁlter\\\\nwas added primarily to improve processing speed, relative\\\\nto running the classiﬁer against all of MEDLINE. With\\\\nthe ﬁlter, processing for one systematic review took about\\\\n10 minutes. Next, a coarse-grained ﬁlter was applied;\\\\nincluded and excluded instances were combined in a single\\\\nset labeled POS. Adjacent PubMed IDs (the next higher\\\\nPMID to each member of POS, as long as that higher num-\\\\nber was not itself a member of POS) were combined in a set\\\\nto represent WORLD. The model was trained on POS vs.\\\\nWORLD, using words and phrases from title, abstract,\\\\nauthor names, and journal name. This model was run\\\\nagainst the records passing the HSSS ﬁlter.\\\\n\\\\nFour different subsets of\\\\n\\\\nthe SVM retrieval were\\\\nexamineddone set included all records with a relevance\\\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\\\n(SVM200point5). Other sets consisted of all records with\\\\nrelevance scores of 0.95 or more (SVM95), 0.90 or more\\\\n(SVM90), and 0.80 or more (SVM80). PubMed ID and\\\\nrelevance scores were recorded, and PubMed IDs were\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n111\\\\n\\\\nTable 1. Characteristics of the included reviews\\\\n\\\\nCochrane\\\\nreviews; N\\\\n\\\\nAHRQ evidence\\\\n\\\\nreports; N\\\\n\\\\nincorporated into the database of records for the reviewers\\\\nto screen. Only the results from SVM200point5 are re-\\\\nported here and are described as SVM.\\\\n\\\\n2.3. Determining performance of the test searches\\\\n\\\\nFor the updated Cochrane reviews, reference standard ar-\\\\nticles were those studies included in the updated review that\\\\nwere not in the original and which entered MEDLINE after\\\\nthe search date of the original review. For the AHRQ evi-\\\\ndence reports, the full retrieval set was screened. Records\\\\nfound relevant by the consensus of reviewers were consid-\\\\nered reference standard articles. Recall was the proportion\\\\nof reference standard articles identiﬁed by the search:\\\\n\\\\nNumber of reference standard articles found\\\\nTotal number of reference standard articles\\\\n\\\\nBioVenn software was used to analyze the overlap be-\\\\ntween the retrieval of relevant articles from the three\\\\nsearches and to create Venn diagrams [25].\\\\n\\\\nAll included reviews were classiﬁed into clinical area\\\\nbased on factors such as ISI journal classiﬁcation, the Co-\\\\nchrane Collaboration Review Group where the topic might\\\\nbe placed and the high level MeSH term under which the pop-\\\\nulation (condition) would be indexed. Performance of the\\\\nsearches in different clinical areas was displayed graphically\\\\nfor the searches both alone and in combinations, to allow ex-\\\\namination of differences in parallelism, level, and ﬂatness.\\\\n\\\\nPrecision is the proportion of all retrieved records that\\\\n\\\\nare relevant:\\\\n\\\\nNumber of reference standard articles found\\\\n\\\\nTotal number of records retrieved\\\\n\\\\nThe inverse of precision is the number needed to read to\\\\nﬁnd one eligible study, thus precision inﬂuences the work\\\\nof the review. Precision was calculated only for the AHRQ\\\\nevidence reviews. Because not all candidates retrieved by\\\\nthe Cochrane searches were assessed, precision could not\\\\nbe established in that sample.\\\\n\\\\n2.4. Determining stability of results over time\\\\n\\\\nThe CQ and related article searches, originally run in\\\\nMarch 2008, were repeated in February 2015 in the AHRQ\\\\ncohort. Recall of relevant items was compared with the\\\\noriginal retrievals to determine if any changes introduced\\\\nby National Library of Medicine might invalidate ﬁndings.\\\\nAs SVM conﬁguration is under the control of the investiga-\\\\ntors, it was not retested.\\\\n\\\\n3. Results\\\\n\\\\nSix Cochrane reviews and 10 AHRQ evidence reports\\\\nmet all inclusion criteria (Appendix B at www.jclinepi.\\\\ncom). Characteristics of the included reviews are shown\\\\nin Table 1.\\\\n\\\\n9a\\\\n1\\\\n3\\\\n\\\\n3\\\\n\\\\nd\\\\n1\\\\n\\\\nd\\\\n1\\\\n2\\\\n1\\\\n1\\\\n\\\\n1\\\\nd\\\\nd\\\\n\\\\nd\\\\n3\\\\n3\\\\n4\\\\n\\\\nCharacteristic\\\\n\\\\nTherapy evaluated\\\\n\\\\nMedications\\\\nMedical devices\\\\nProcedures\\\\n\\\\nClinical topic area\\\\n\\\\nCardiac and cardiovascular\\\\n\\\\nsystems\\\\n\\\\nCritical care\\\\nEndocrinology and\\\\n\\\\nmetabolism\\\\n\\\\nInfectious disease\\\\nClinical neurology\\\\nObstetrics and gynecology\\\\nOncology\\\\nPeripheral vascular\\\\n\\\\ndiseases\\\\nPsychiatry\\\\nRespiratory systems\\\\nUrology and nephrology\\\\n\\\\nPublication period\\\\n\\\\nMarch 1997eApril 1999\\\\nMay 1999eJune 2001\\\\nJuly 2001eAugust 2003\\\\nSeptember 2003e\\\\nDecember 2005\\\\n\\\\nMedian included trials\\\\n\\\\nMedian included\\\\n\\\\nparticipants\\\\n\\\\nMEDLINE coverage of\\\\n\\\\n5\\\\n1\\\\nd\\\\n\\\\nd\\\\n\\\\n1\\\\nd\\\\n\\\\n3\\\\nd\\\\nd\\\\nd\\\\nd\\\\n\\\\nd\\\\n1\\\\n1\\\\n\\\\n3\\\\n1\\\\n2\\\\nd\\\\n\\\\n17 (IQR, 14e20)\\\\n\\\\n96 (IQR,\\\\n\\\\n31.75e121.5)\\\\n8,679 (IQR,\\\\n22,830 (IQR, 14,\\\\n4,085e50,109)\\\\n172e49,687)\\\\n99/107 (92.5%) 969/980 (98.8%)\\\\n\\\\nincluded articles\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\\\n\\\\nity; IQR, interquartile range.\\\\n\\\\na Some AHRQ reviews included more than one class of therapy.\\\\n\\\\nRecall of new relevant studies is shown in Table 2. The\\\\nupdated Cochrane reviews had 20 new studies identiﬁed by\\\\nthe review authors and added in the updates. Recall of these\\\\n20 ranged from 1.00 for the related articles method to a low\\\\nof 0.80 CQs (Table 2). Our team identiﬁed 277 new studies\\\\nas relevant for inclusion in the 10 evidence reports that\\\\nwere updated. All test searches showed lower recall for this\\\\ncohort than for the Cochrane reviews and here the CQ out-\\\\nperformed both ranking searches (Table 2).\\\\n\\\\n3.1. Recall of CQ combined with a ranking method\\\\n\\\\nOf the 297 new relevant studies identiﬁed, the combina-\\\\ntion of CQ and related articles searches identiﬁed 270\\\\n(overall\\\\nin\\\\nthe Cochrane cohort and 0.90 (250/277) in the AHRQ\\\\ncohort.\\\\n\\\\nrecall of 0.91). Recall was 1.00 (20/20)\\\\n\\\\nThe combination of CQ and SVM identiﬁed 263 studies\\\\n(overall recall of 0.89). Recall was 1.00 (20/20) in the\\\\nCochrane cohort and 0.88 (243/277) in the AHRQ cohort.\\\\nWhen all three methods were used together, recall was\\\\n0.997 with 296 of the 297 studies identiﬁed.\\\\n\\\\n\\\\x0c', 'Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nComplementary approaches to searching MEDLINE may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\\\n\\\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\\\n\\\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\\\n\\\\n0R6, Canada\\\\n\\\\nUnited Kingdom\\\\n\\\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\\\n\\\\nAccepted 7 March 2016; Published online 11 March 2016\\\\n\\\\nAbstract\\\\n\\\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\\\n\\\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\\\ncombined. Related articles showed least stability over time.\\\\n\\\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\\\n\\\\n1. Introduction\\\\n\\\\nSystematic review searches need to have high recall.\\\\nMechanisms to achieve this usually include expansive\\\\nBoolean searches of multiple databases. This approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nMEDLINE gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. However,\\\\n\\\\nConﬂict of interest: None.\\\\nFunding: Some of this work was funded by the US Agency for Health-\\\\ncare Research and Quality, Department of Health and Human Services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* Corresponding author. Tel.: 613-737-7600.\\\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through MEDLINE. Recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in MEDLINE, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nSuccessful retrieval through a Boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. The search of\\\\nmultiple databases can therefore be helpful. The target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. A text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. These tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nWhat is new?\\\\n\\\\nKey ﬁndings\\\\n(cid:1) Searches using known relevant studies and the\\\\nsimilar articles feature of PubMed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) For a given question, if the Boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. The two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) The precision of\\\\n\\\\nmethod appears better\\\\nexhaustive Boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nWhat this adds to what was known?\\\\n(cid:1) The paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nIt is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nWhat is the implication and what should change\\\\nnow?\\\\n(cid:1) Using the simple and universally available PubMed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) If the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nMEDLINE.\\\\n\\\\nto large retrievals with low precision [9]. These resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional Boolean searches have\\\\nbeen sought [10].\\\\n\\\\nMost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nThese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. These methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nWe examined one method, support vector machine\\\\n(SVM), and compared it with a simple and readily available\\\\nmethod based on the PubMed similar articles feature. We\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. We paired both\\\\nwith a focused Boolean search within MEDLINE. We\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for SVM and seed articles for the PubMed-\\\\n\\\\nrelated articles search. We therefore sought to determine\\\\nif a focused Boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\nComparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. In this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (CQ), and PubMed-related articles are tested in a\\\\ncohort of six updated Cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 Agency for Healthcare\\\\nResearch and Quality (AHRQ) evidence reports. The\\\\nCochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. The replication in the\\\\nAHRQ cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. All records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nOther research [17,18] suggests that searches using the\\\\nPubMed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a Boolean-type search of\\\\nMEDLINE. We tested an additional search method, SVM,\\\\nin the Cochrane and AHRQ samples to permit comparison\\\\nwith our PubMed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the PubMed\\\\nsimilar articles method.\\\\n\\\\nThe aim of this article was to test whether the combined\\\\napproach of a focused Boolean search paired with a second\\\\nsearch using the similar articles feature of PubMed or SVM\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. Methods\\\\n\\\\n2.1. Formation of the study cohorts\\\\n\\\\nThis analysis uses a data set created for an updating\\\\nstudy sponsored by AHRQ [16]. Methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\\\nsearch of the ACP Journal Club database (Ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nAHRQ reports were identiﬁed through the PubMed\\\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\\\njrid21544]. Screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. Screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c110\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nFor inclusion, all reviews must have included random-\\\\nized controlled trials (RCTs) and provided meta-analysis\\\\nfor at least one outcome. The MEDLINE search strategies\\\\nhad to be reported in enough detail to permit replication.\\\\nFifteen AHRQ evidence reports thought\\\\nlikely to have\\\\nimportant new evidence were selected and used to validate\\\\nthe updating methods used in the main cohort.\\\\n\\\\nCochrane reviews were selected to meet minimum qual-\\\\nity and relevance standards, as deﬁned by ACP Journal\\\\nClub [19]. We also required that the review had been up-\\\\ndated and that the text of both the original review and an\\\\nupdated version was available. The search for the original\\\\nreview must have included MEDLINE and at least one\\\\nother electronic bibliographic database and one or more\\\\nnondatabase method such as hand searching or checking\\\\nreference lists. Such comprehensive searching was likely\\\\nto identify most relevant\\\\nliterature and form a useful\\\\ntraining set of included examples. At least 10 RCTs or\\\\nquasi-RCTs must have been included in the original review\\\\nto give an adequate training set for SVM.\\\\n\\\\n2.2. Test searches\\\\n\\\\n2.2.1. Clinical query\\\\n\\\\nBoolean searches were developed by a librarian experi-\\\\nenced in systematic review searches using a protocol\\\\n(Appendix A at www.jclinepi.com). Search strategies were\\\\ndeliberately simple, usually consisting of two or three\\\\nMedical Subject Heading (MeSH) terms representing the\\\\npopulation and intervention of the review. This search\\\\nwas limited by a CQ ﬁlter to ‘‘therapy (best balance of\\\\nsensitivity\\\\n‘‘Randomized\\\\ncontrolled trial.pt. or\\\\nrandomized.mp. or placebo.mp.’’\\\\n[20]. Boolean searches were run in OVID MEDLINE.\\\\n\\\\nspeciﬁcity)’’\\\\n\\\\nthat\\\\n\\\\nand\\\\n\\\\nis,\\\\n\\\\n2.2.2. Related articles\\\\n\\\\nSearches using the PubMed similar articles feature were\\\\nperformed using the PubMed unique identiﬁers (PMID) of\\\\nthe three newest and three largest studies included in the\\\\noriginal review as seed articles. There was no replacement\\\\nin the event of overlap between the largest and newest sets,\\\\nor if one of those studies was not indexed in MEDLINE.\\\\n(All studies included in the original reviews were checked\\\\nfor indexing status in MEDLINE.) The resulting set was\\\\nlimited to the publication type RCT and date limited to\\\\nthe period since the search date of the original review.\\\\n\\\\n2.2.3. Support vector machine\\\\n\\\\nThe SVM searches used for this project have not been\\\\npreviously described so the methods will be reported in\\\\nmore detail. SVM is a well established and broadly applied\\\\nclassiﬁcation method from machine learning [21,22]. A\\\\nclassiﬁcation task is closely related to an information\\\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\\\nthe collection. As SVM is a supervised algorithm,\\\\nit\\\\n\\\\nrequires training examples with known labels. It then places\\\\nthese training examples in a high-dimensional vector space\\\\nin such a way that examples of one class are separated from\\\\nexamples of the other class with the greatest distance to the\\\\nseparating boundary. Predicting the class of a new instance\\\\nhappens by placing it in the same space and determining on\\\\nwhich side of the boundary it falls. The distance to the\\\\nboundary is an indication of the prediction conﬁdence and\\\\nis used to rank instances (documents) as is essential in in-\\\\nformation retrieval tasks or to provide a cutoff threshold.\\\\n\\\\nIn our setup, the training set consisted of search results\\\\nfrom the original review formed with included instances\\\\nbeing the studies included in the review (the relevant re-\\\\ntrievals) and excluded instances being the studies found\\\\nby the search used in the original review but excluded from\\\\nthe review (irrelevant retrievals). The new examples to be\\\\nclassiﬁed were articles indexed since the date of the search\\\\nperformed in the original review.\\\\n\\\\nSVM searches were run on MEDLINE data stored\\\\nlocally at\\\\nthe National Research Council of Canada.\\\\nMEDLINE was refreshed before running the searches.\\\\nThe MEDLINE records were represented as bags of fea-\\\\ntures, where features consisted of lowercased words and\\\\nword combinations (up to four words) from the title and ab-\\\\nstract ﬁelds, full MeSH terms, and contents of all other\\\\nﬁelds in the MEDLINE record. Features were uniformly\\\\nweighted. The SVM implementation used was SVM Light\\\\n[23] used with a linear kernel.\\\\n\\\\nThe approach was piloted with one AHRQ evidence\\\\nreport, and various conﬁgurations were tried, observing\\\\nthe placement of several known new relevant studies within\\\\nthe retrieval. The MEDLINE set was ﬁltered with the\\\\nrevised Highly Sensitive Search Strategy (HSSS) [24]. Pre-\\\\nliminary testing indicated that this ﬁlter would retrieve\\\\n99.1% of MEDLINE-indexed relevant new evidence from\\\\nthe larger cohort of journal-published systematic reviews\\\\n(n 5 72) and Cochrane reviews (n 5 27). This HSSS ﬁlter\\\\nwas added primarily to improve processing speed, relative\\\\nto running the classiﬁer against all of MEDLINE. With\\\\nthe ﬁlter, processing for one systematic review took about\\\\n10 minutes. Next, a coarse-grained ﬁlter was applied;\\\\nincluded and excluded instances were combined in a single\\\\nset labeled POS. Adjacent PubMed IDs (the next higher\\\\nPMID to each member of POS, as long as that higher num-\\\\nber was not itself a member of POS) were combined in a set\\\\nto represent WORLD. The model was trained on POS vs.\\\\nWORLD, using words and phrases from title, abstract,\\\\nauthor names, and journal name. This model was run\\\\nagainst the records passing the HSSS ﬁlter.\\\\n\\\\nFour different subsets of\\\\n\\\\nthe SVM retrieval were\\\\nexamineddone set included all records with a relevance\\\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\\\n(SVM200point5). Other sets consisted of all records with\\\\nrelevance scores of 0.95 or more (SVM95), 0.90 or more\\\\n(SVM90), and 0.80 or more (SVM80). PubMed ID and\\\\nrelevance scores were recorded, and PubMed IDs were\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n111\\\\n\\\\nTable 1. Characteristics of the included reviews\\\\n\\\\nCochrane\\\\nreviews; N\\\\n\\\\nAHRQ evidence\\\\n\\\\nreports; N\\\\n\\\\nincorporated into the database of records for the reviewers\\\\nto screen. Only the results from SVM200point5 are re-\\\\nported here and are described as SVM.\\\\n\\\\n2.3. Determining performance of the test searches\\\\n\\\\nFor the updated Cochrane reviews, reference standard ar-\\\\nticles were those studies included in the updated review that\\\\nwere not in the original and which entered MEDLINE after\\\\nthe search date of the original review. For the AHRQ evi-\\\\ndence reports, the full retrieval set was screened. Records\\\\nfound relevant by the consensus of reviewers were consid-\\\\nered reference standard articles. Recall was the proportion\\\\nof reference standard articles identiﬁed by the search:\\\\n\\\\nNumber of reference standard articles found\\\\nTotal number of reference standard articles\\\\n\\\\nBioVenn software was used to analyze the overlap be-\\\\ntween the retrieval of relevant articles from the three\\\\nsearches and to create Venn diagrams [25].\\\\n\\\\nAll included reviews were classiﬁed into clinical area\\\\nbased on factors such as ISI journal classiﬁcation, the Co-\\\\nchrane Collaboration Review Group where the topic might\\\\nbe placed and the high level MeSH term under which the pop-\\\\nulation (condition) would be indexed. Performance of the\\\\nsearches in different clinical areas was displayed graphically\\\\nfor the searches both alone and in combinations, to allow ex-\\\\namination of differences in parallelism, level, and ﬂatness.\\\\n\\\\nPrecision is the proportion of all retrieved records that\\\\n\\\\nare relevant:\\\\n\\\\nNumber of reference standard articles found\\\\n\\\\nTotal number of records retrieved\\\\n\\\\nThe inverse of precision is the number needed to read to\\\\nﬁnd one eligible study, thus precision inﬂuences the work\\\\nof the review. Precision was calculated only for the AHRQ\\\\nevidence reviews. Because not all candidates retrieved by\\\\nthe Cochrane searches were assessed, precision could not\\\\nbe established in that sample.\\\\n\\\\n2.4. Determining stability of results over time\\\\n\\\\nThe CQ and related article searches, originally run in\\\\nMarch 2008, were repeated in February 2015 in the AHRQ\\\\ncohort. Recall of relevant items was compared with the\\\\noriginal retrievals to determine if any changes introduced\\\\nby National Library of Medicine might invalidate ﬁndings.\\\\nAs SVM conﬁguration is under the control of the investiga-\\\\ntors, it was not retested.\\\\n\\\\n3. Results\\\\n\\\\nSix Cochrane reviews and 10 AHRQ evidence reports\\\\nmet all inclusion criteria (Appendix B at www.jclinepi.\\\\ncom). Characteristics of the included reviews are shown\\\\nin Table 1.\\\\n\\\\n9a\\\\n1\\\\n3\\\\n\\\\n3\\\\n\\\\nd\\\\n1\\\\n\\\\nd\\\\n1\\\\n2\\\\n1\\\\n1\\\\n\\\\n1\\\\nd\\\\nd\\\\n\\\\nd\\\\n3\\\\n3\\\\n4\\\\n\\\\nCharacteristic\\\\n\\\\nTherapy evaluated\\\\n\\\\nMedications\\\\nMedical devices\\\\nProcedures\\\\n\\\\nClinical topic area\\\\n\\\\nCardiac and cardiovascular\\\\n\\\\nsystems\\\\n\\\\nCritical care\\\\nEndocrinology and\\\\n\\\\nmetabolism\\\\n\\\\nInfectious disease\\\\nClinical neurology\\\\nObstetrics and gynecology\\\\nOncology\\\\nPeripheral vascular\\\\n\\\\ndiseases\\\\nPsychiatry\\\\nRespiratory systems\\\\nUrology and nephrology\\\\n\\\\nPublication period\\\\n\\\\nMarch 1997eApril 1999\\\\nMay 1999eJune 2001\\\\nJuly 2001eAugust 2003\\\\nSeptember 2003e\\\\nDecember 2005\\\\n\\\\nMedian included trials\\\\n\\\\nMedian included\\\\n\\\\nparticipants\\\\n\\\\nMEDLINE coverage of\\\\n\\\\n5\\\\n1\\\\nd\\\\n\\\\nd\\\\n\\\\n1\\\\nd\\\\n\\\\n3\\\\nd\\\\nd\\\\nd\\\\nd\\\\n\\\\nd\\\\n1\\\\n1\\\\n\\\\n3\\\\n1\\\\n2\\\\nd\\\\n\\\\n17 (IQR, 14e20)\\\\n\\\\n96 (IQR,\\\\n\\\\n31.75e121.5)\\\\n8,679 (IQR,\\\\n22,830 (IQR, 14,\\\\n4,085e50,109)\\\\n172e49,687)\\\\n99/107 (92.5%) 969/980 (98.8%)\\\\n\\\\nincluded articles\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\\\n\\\\nity; IQR, interquartile range.\\\\n\\\\na Some AHRQ reviews included more than one class of therapy.\\\\n\\\\nRecall of new relevant studies is shown in Table 2. The\\\\nupdated Cochrane reviews had 20 new studies identiﬁed by\\\\nthe review authors and added in the updates. Recall of these\\\\n20 ranged from 1.00 for the related articles method to a low\\\\nof 0.80 CQs (Table 2). Our team identiﬁed 277 new studies\\\\nas relevant for inclusion in the 10 evidence reports that\\\\nwere updated. All test searches showed lower recall for this\\\\ncohort than for the Cochrane reviews and here the CQ out-\\\\nperformed both ranking searches (Table 2).\\\\n\\\\n3.1. Recall of CQ combined with a ranking method\\\\n\\\\nOf the 297 new relevant studies identiﬁed, the combina-\\\\ntion of CQ and related articles searches identiﬁed 270\\\\n(overall\\\\nin\\\\nthe Cochrane cohort and 0.90 (250/277) in the AHRQ\\\\ncohort.\\\\n\\\\nrecall of 0.91). Recall was 1.00 (20/20)\\\\n\\\\nThe combination of CQ and SVM identiﬁed 263 studies\\\\n(overall recall of 0.89). Recall was 1.00 (20/20) in the\\\\nCochrane cohort and 0.88 (243/277) in the AHRQ cohort.\\\\nWhen all three methods were used together, recall was\\\\n0.997 with 296 of the 297 studies identiﬁed.\\\\n\\\\n\\\\x0c112\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nTable 2. Recall of eligible studies by the search methods\\\\n\\\\nCochrane reviews\\\\n\\\\nAHRQ evidence\\\\n\\\\nreports\\\\n\\\\nRetrieval method\\\\n\\\\nClinical query\\\\nRelated articles\\\\nSVM\\\\nTotal\\\\n\\\\nN\\\\n\\\\n16\\\\n20\\\\n19\\\\n20\\\\n\\\\nRecall\\\\n\\\\n0.80\\\\n1.00\\\\n0.95\\\\n\\\\nN\\\\n\\\\n188\\\\n176\\\\n129\\\\n277\\\\n\\\\nRecall\\\\n\\\\n0.68\\\\n0.64\\\\n0.47\\\\n\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\\\n\\\\nity; SVM, support vector machine.\\\\n\\\\nAcross the two cohorts, the overlap and unique component from\\\\nthe retrieval of relevant records by the test searches was examined\\\\n(Fig. 1).\\\\n\\\\n3.2. Consistency across clinical areas\\\\n\\\\nThirteen clinical areas were represented in the larger\\\\nstudy that included 72 journal-published reviews, but only\\\\neight of those clinical areas had two or more new relevant\\\\nstudies in the smaller Cochrane and AHRQ cohorts. Fig. 2\\\\nshows recall of new studies by the three types of searches,\\\\nfor these eight clinical areas.\\\\n\\\\nCombined recall of the CQ Boolean search paired with a\\\\nranking search is shown in Fig. 3. The combination showed\\\\ncomplete recall of relevant new studies in three of eight\\\\nareas when CQ was paired with related articles and recall\\\\nof 0.67 or higher in all areas. Four of eight clinical areas\\\\nhad complete recall for the combination of CQ and SVM,\\\\nand recall was 0.80 or higher in all areas. Even for periph-\\\\neral vascular disease, where all three searches performed\\\\n\\\\nFig. 2. Recall of new studies by clinical area for each search method.\\\\nAbbreviations: SVM, support vector machine.\\\\n\\\\nfairly poorly individually, recall was 0.76 for the CQ/\\\\nrelated articles and 0.90 in the CQ/SVM pairing.\\\\n\\\\n3.3. Search precision\\\\n\\\\nPrecision, across the 10 AHRQ reviews, was 0.11 for the\\\\nCQ, 0.22 for related articles, and 0.19 for SVM (Table 3).\\\\n\\\\nFig. 1. Overlapping and unique retrieval of relevant studies by each\\\\nsearch method. The size of the circle is proportional to the size of\\\\nthe retrieval, but the overlap and unique portions are approximations.\\\\nExact ﬁgures for the components are labeled. Abbreviations: SVM,\\\\nsupport vector machine.\\\\n\\\\nFig. 3. Recall of new studies by clinical area for search methods in\\\\ncombination. Abbreviations: CQ, clinical query; SVM, support vector\\\\nmachine.\\\\n\\\\n\\\\x0c', 'Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nComplementary approaches to searching MEDLINE may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\\\n\\\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\\\n\\\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\\\n\\\\n0R6, Canada\\\\n\\\\nUnited Kingdom\\\\n\\\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\\\n\\\\nAccepted 7 March 2016; Published online 11 March 2016\\\\n\\\\nAbstract\\\\n\\\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\\\n\\\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\\\ncombined. Related articles showed least stability over time.\\\\n\\\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\\\n\\\\n1. Introduction\\\\n\\\\nSystematic review searches need to have high recall.\\\\nMechanisms to achieve this usually include expansive\\\\nBoolean searches of multiple databases. This approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nMEDLINE gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. However,\\\\n\\\\nConﬂict of interest: None.\\\\nFunding: Some of this work was funded by the US Agency for Health-\\\\ncare Research and Quality, Department of Health and Human Services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* Corresponding author. Tel.: 613-737-7600.\\\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through MEDLINE. Recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in MEDLINE, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nSuccessful retrieval through a Boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. The search of\\\\nmultiple databases can therefore be helpful. The target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. A text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. These tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nWhat is new?\\\\n\\\\nKey ﬁndings\\\\n(cid:1) Searches using known relevant studies and the\\\\nsimilar articles feature of PubMed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) For a given question, if the Boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. The two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) The precision of\\\\n\\\\nmethod appears better\\\\nexhaustive Boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nWhat this adds to what was known?\\\\n(cid:1) The paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nIt is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nWhat is the implication and what should change\\\\nnow?\\\\n(cid:1) Using the simple and universally available PubMed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) If the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nMEDLINE.\\\\n\\\\nto large retrievals with low precision [9]. These resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional Boolean searches have\\\\nbeen sought [10].\\\\n\\\\nMost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nThese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. These methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nWe examined one method, support vector machine\\\\n(SVM), and compared it with a simple and readily available\\\\nmethod based on the PubMed similar articles feature. We\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. We paired both\\\\nwith a focused Boolean search within MEDLINE. We\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for SVM and seed articles for the PubMed-\\\\n\\\\nrelated articles search. We therefore sought to determine\\\\nif a focused Boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\nComparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. In this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (CQ), and PubMed-related articles are tested in a\\\\ncohort of six updated Cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 Agency for Healthcare\\\\nResearch and Quality (AHRQ) evidence reports. The\\\\nCochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. The replication in the\\\\nAHRQ cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. All records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nOther research [17,18] suggests that searches using the\\\\nPubMed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a Boolean-type search of\\\\nMEDLINE. We tested an additional search method, SVM,\\\\nin the Cochrane and AHRQ samples to permit comparison\\\\nwith our PubMed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the PubMed\\\\nsimilar articles method.\\\\n\\\\nThe aim of this article was to test whether the combined\\\\napproach of a focused Boolean search paired with a second\\\\nsearch using the similar articles feature of PubMed or SVM\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. Methods\\\\n\\\\n2.1. Formation of the study cohorts\\\\n\\\\nThis analysis uses a data set created for an updating\\\\nstudy sponsored by AHRQ [16]. Methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\\\nsearch of the ACP Journal Club database (Ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nAHRQ reports were identiﬁed through the PubMed\\\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\\\njrid21544]. Screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. Screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c110\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nFor inclusion, all reviews must have included random-\\\\nized controlled trials (RCTs) and provided meta-analysis\\\\nfor at least one outcome. The MEDLINE search strategies\\\\nhad to be reported in enough detail to permit replication.\\\\nFifteen AHRQ evidence reports thought\\\\nlikely to have\\\\nimportant new evidence were selected and used to validate\\\\nthe updating methods used in the main cohort.\\\\n\\\\nCochrane reviews were selected to meet minimum qual-\\\\nity and relevance standards, as deﬁned by ACP Journal\\\\nClub [19]. We also required that the review had been up-\\\\ndated and that the text of both the original review and an\\\\nupdated version was available. The search for the original\\\\nreview must have included MEDLINE and at least one\\\\nother electronic bibliographic database and one or more\\\\nnondatabase method such as hand searching or checking\\\\nreference lists. Such comprehensive searching was likely\\\\nto identify most relevant\\\\nliterature and form a useful\\\\ntraining set of included examples. At least 10 RCTs or\\\\nquasi-RCTs must have been included in the original review\\\\nto give an adequate training set for SVM.\\\\n\\\\n2.2. Test searches\\\\n\\\\n2.2.1. Clinical query\\\\n\\\\nBoolean searches were developed by a librarian experi-\\\\nenced in systematic review searches using a protocol\\\\n(Appendix A at www.jclinepi.com). Search strategies were\\\\ndeliberately simple, usually consisting of two or three\\\\nMedical Subject Heading (MeSH) terms representing the\\\\npopulation and intervention of the review. This search\\\\nwas limited by a CQ ﬁlter to ‘‘therapy (best balance of\\\\nsensitivity\\\\n‘‘Randomized\\\\ncontrolled trial.pt. or\\\\nrandomized.mp. or placebo.mp.’’\\\\n[20]. Boolean searches were run in OVID MEDLINE.\\\\n\\\\nspeciﬁcity)’’\\\\n\\\\nthat\\\\n\\\\nand\\\\n\\\\nis,\\\\n\\\\n2.2.2. Related articles\\\\n\\\\nSearches using the PubMed similar articles feature were\\\\nperformed using the PubMed unique identiﬁers (PMID) of\\\\nthe three newest and three largest studies included in the\\\\noriginal review as seed articles. There was no replacement\\\\nin the event of overlap between the largest and newest sets,\\\\nor if one of those studies was not indexed in MEDLINE.\\\\n(All studies included in the original reviews were checked\\\\nfor indexing status in MEDLINE.) The resulting set was\\\\nlimited to the publication type RCT and date limited to\\\\nthe period since the search date of the original review.\\\\n\\\\n2.2.3. Support vector machine\\\\n\\\\nThe SVM searches used for this project have not been\\\\npreviously described so the methods will be reported in\\\\nmore detail. SVM is a well established and broadly applied\\\\nclassiﬁcation method from machine learning [21,22]. A\\\\nclassiﬁcation task is closely related to an information\\\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\\\nthe collection. As SVM is a supervised algorithm,\\\\nit\\\\n\\\\nrequires training examples with known labels. It then places\\\\nthese training examples in a high-dimensional vector space\\\\nin such a way that examples of one class are separated from\\\\nexamples of the other class with the greatest distance to the\\\\nseparating boundary. Predicting the class of a new instance\\\\nhappens by placing it in the same space and determining on\\\\nwhich side of the boundary it falls. The distance to the\\\\nboundary is an indication of the prediction conﬁdence and\\\\nis used to rank instances (documents) as is essential in in-\\\\nformation retrieval tasks or to provide a cutoff threshold.\\\\n\\\\nIn our setup, the training set consisted of search results\\\\nfrom the original review formed with included instances\\\\nbeing the studies included in the review (the relevant re-\\\\ntrievals) and excluded instances being the studies found\\\\nby the search used in the original review but excluded from\\\\nthe review (irrelevant retrievals). The new examples to be\\\\nclassiﬁed were articles indexed since the date of the search\\\\nperformed in the original review.\\\\n\\\\nSVM searches were run on MEDLINE data stored\\\\nlocally at\\\\nthe National Research Council of Canada.\\\\nMEDLINE was refreshed before running the searches.\\\\nThe MEDLINE records were represented as bags of fea-\\\\ntures, where features consisted of lowercased words and\\\\nword combinations (up to four words) from the title and ab-\\\\nstract ﬁelds, full MeSH terms, and contents of all other\\\\nﬁelds in the MEDLINE record. Features were uniformly\\\\nweighted. The SVM implementation used was SVM Light\\\\n[23] used with a linear kernel.\\\\n\\\\nThe approach was piloted with one AHRQ evidence\\\\nreport, and various conﬁgurations were tried, observing\\\\nthe placement of several known new relevant studies within\\\\nthe retrieval. The MEDLINE set was ﬁltered with the\\\\nrevised Highly Sensitive Search Strategy (HSSS) [24]. Pre-\\\\nliminary testing indicated that this ﬁlter would retrieve\\\\n99.1% of MEDLINE-indexed relevant new evidence from\\\\nthe larger cohort of journal-published systematic reviews\\\\n(n 5 72) and Cochrane reviews (n 5 27). This HSSS ﬁlter\\\\nwas added primarily to improve processing speed, relative\\\\nto running the classiﬁer against all of MEDLINE. With\\\\nthe ﬁlter, processing for one systematic review took about\\\\n10 minutes. Next, a coarse-grained ﬁlter was applied;\\\\nincluded and excluded instances were combined in a single\\\\nset labeled POS. Adjacent PubMed IDs (the next higher\\\\nPMID to each member of POS, as long as that higher num-\\\\nber was not itself a member of POS) were combined in a set\\\\nto represent WORLD. The model was trained on POS vs.\\\\nWORLD, using words and phrases from title, abstract,\\\\nauthor names, and journal name. This model was run\\\\nagainst the records passing the HSSS ﬁlter.\\\\n\\\\nFour different subsets of\\\\n\\\\nthe SVM retrieval were\\\\nexamineddone set included all records with a relevance\\\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\\\n(SVM200point5). Other sets consisted of all records with\\\\nrelevance scores of 0.95 or more (SVM95), 0.90 or more\\\\n(SVM90), and 0.80 or more (SVM80). PubMed ID and\\\\nrelevance scores were recorded, and PubMed IDs were\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n111\\\\n\\\\nTable 1. Characteristics of the included reviews\\\\n\\\\nCochrane\\\\nreviews; N\\\\n\\\\nAHRQ evidence\\\\n\\\\nreports; N\\\\n\\\\nincorporated into the database of records for the reviewers\\\\nto screen. Only the results from SVM200point5 are re-\\\\nported here and are described as SVM.\\\\n\\\\n2.3. Determining performance of the test searches\\\\n\\\\nFor the updated Cochrane reviews, reference standard ar-\\\\nticles were those studies included in the updated review that\\\\nwere not in the original and which entered MEDLINE after\\\\nthe search date of the original review. For the AHRQ evi-\\\\ndence reports, the full retrieval set was screened. Records\\\\nfound relevant by the consensus of reviewers were consid-\\\\nered reference standard articles. Recall was the proportion\\\\nof reference standard articles identiﬁed by the search:\\\\n\\\\nNumber of reference standard articles found\\\\nTotal number of reference standard articles\\\\n\\\\nBioVenn software was used to analyze the overlap be-\\\\ntween the retrieval of relevant articles from the three\\\\nsearches and to create Venn diagrams [25].\\\\n\\\\nAll included reviews were classiﬁed into clinical area\\\\nbased on factors such as ISI journal classiﬁcation, the Co-\\\\nchrane Collaboration Review Group where the topic might\\\\nbe placed and the high level MeSH term under which the pop-\\\\nulation (condition) would be indexed. Performance of the\\\\nsearches in different clinical areas was displayed graphically\\\\nfor the searches both alone and in combinations, to allow ex-\\\\namination of differences in parallelism, level, and ﬂatness.\\\\n\\\\nPrecision is the proportion of all retrieved records that\\\\n\\\\nare relevant:\\\\n\\\\nNumber of reference standard articles found\\\\n\\\\nTotal number of records retrieved\\\\n\\\\nThe inverse of precision is the number needed to read to\\\\nﬁnd one eligible study, thus precision inﬂuences the work\\\\nof the review. Precision was calculated only for the AHRQ\\\\nevidence reviews. Because not all candidates retrieved by\\\\nthe Cochrane searches were assessed, precision could not\\\\nbe established in that sample.\\\\n\\\\n2.4. Determining stability of results over time\\\\n\\\\nThe CQ and related article searches, originally run in\\\\nMarch 2008, were repeated in February 2015 in the AHRQ\\\\ncohort. Recall of relevant items was compared with the\\\\noriginal retrievals to determine if any changes introduced\\\\nby National Library of Medicine might invalidate ﬁndings.\\\\nAs SVM conﬁguration is under the control of the investiga-\\\\ntors, it was not retested.\\\\n\\\\n3. Results\\\\n\\\\nSix Cochrane reviews and 10 AHRQ evidence reports\\\\nmet all inclusion criteria (Appendix B at www.jclinepi.\\\\ncom). Characteristics of the included reviews are shown\\\\nin Table 1.\\\\n\\\\n9a\\\\n1\\\\n3\\\\n\\\\n3\\\\n\\\\nd\\\\n1\\\\n\\\\nd\\\\n1\\\\n2\\\\n1\\\\n1\\\\n\\\\n1\\\\nd\\\\nd\\\\n\\\\nd\\\\n3\\\\n3\\\\n4\\\\n\\\\nCharacteristic\\\\n\\\\nTherapy evaluated\\\\n\\\\nMedications\\\\nMedical devices\\\\nProcedures\\\\n\\\\nClinical topic area\\\\n\\\\nCardiac and cardiovascular\\\\n\\\\nsystems\\\\n\\\\nCritical care\\\\nEndocrinology and\\\\n\\\\nmetabolism\\\\n\\\\nInfectious disease\\\\nClinical neurology\\\\nObstetrics and gynecology\\\\nOncology\\\\nPeripheral vascular\\\\n\\\\ndiseases\\\\nPsychiatry\\\\nRespiratory systems\\\\nUrology and nephrology\\\\n\\\\nPublication period\\\\n\\\\nMarch 1997eApril 1999\\\\nMay 1999eJune 2001\\\\nJuly 2001eAugust 2003\\\\nSeptember 2003e\\\\nDecember 2005\\\\n\\\\nMedian included trials\\\\n\\\\nMedian included\\\\n\\\\nparticipants\\\\n\\\\nMEDLINE coverage of\\\\n\\\\n5\\\\n1\\\\nd\\\\n\\\\nd\\\\n\\\\n1\\\\nd\\\\n\\\\n3\\\\nd\\\\nd\\\\nd\\\\nd\\\\n\\\\nd\\\\n1\\\\n1\\\\n\\\\n3\\\\n1\\\\n2\\\\nd\\\\n\\\\n17 (IQR, 14e20)\\\\n\\\\n96 (IQR,\\\\n\\\\n31.75e121.5)\\\\n8,679 (IQR,\\\\n22,830 (IQR, 14,\\\\n4,085e50,109)\\\\n172e49,687)\\\\n99/107 (92.5%) 969/980 (98.8%)\\\\n\\\\nincluded articles\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\\\n\\\\nity; IQR, interquartile range.\\\\n\\\\na Some AHRQ reviews included more than one class of therapy.\\\\n\\\\nRecall of new relevant studies is shown in Table 2. The\\\\nupdated Cochrane reviews had 20 new studies identiﬁed by\\\\nthe review authors and added in the updates. Recall of these\\\\n20 ranged from 1.00 for the related articles method to a low\\\\nof 0.80 CQs (Table 2). Our team identiﬁed 277 new studies\\\\nas relevant for inclusion in the 10 evidence reports that\\\\nwere updated. All test searches showed lower recall for this\\\\ncohort than for the Cochrane reviews and here the CQ out-\\\\nperformed both ranking searches (Table 2).\\\\n\\\\n3.1. Recall of CQ combined with a ranking method\\\\n\\\\nOf the 297 new relevant studies identiﬁed, the combina-\\\\ntion of CQ and related articles searches identiﬁed 270\\\\n(overall\\\\nin\\\\nthe Cochrane cohort and 0.90 (250/277) in the AHRQ\\\\ncohort.\\\\n\\\\nrecall of 0.91). Recall was 1.00 (20/20)\\\\n\\\\nThe combination of CQ and SVM identiﬁed 263 studies\\\\n(overall recall of 0.89). Recall was 1.00 (20/20) in the\\\\nCochrane cohort and 0.88 (243/277) in the AHRQ cohort.\\\\nWhen all three methods were used together, recall was\\\\n0.997 with 296 of the 297 studies identiﬁed.\\\\n\\\\n\\\\x0c112\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nTable 2. Recall of eligible studies by the search methods\\\\n\\\\nCochrane reviews\\\\n\\\\nAHRQ evidence\\\\n\\\\nreports\\\\n\\\\nRetrieval method\\\\n\\\\nClinical query\\\\nRelated articles\\\\nSVM\\\\nTotal\\\\n\\\\nN\\\\n\\\\n16\\\\n20\\\\n19\\\\n20\\\\n\\\\nRecall\\\\n\\\\n0.80\\\\n1.00\\\\n0.95\\\\n\\\\nN\\\\n\\\\n188\\\\n176\\\\n129\\\\n277\\\\n\\\\nRecall\\\\n\\\\n0.68\\\\n0.64\\\\n0.47\\\\n\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\\\n\\\\nity; SVM, support vector machine.\\\\n\\\\nAcross the two cohorts, the overlap and unique component from\\\\nthe retrieval of relevant records by the test searches was examined\\\\n(Fig. 1).\\\\n\\\\n3.2. Consistency across clinical areas\\\\n\\\\nThirteen clinical areas were represented in the larger\\\\nstudy that included 72 journal-published reviews, but only\\\\neight of those clinical areas had two or more new relevant\\\\nstudies in the smaller Cochrane and AHRQ cohorts. Fig. 2\\\\nshows recall of new studies by the three types of searches,\\\\nfor these eight clinical areas.\\\\n\\\\nCombined recall of the CQ Boolean search paired with a\\\\nranking search is shown in Fig. 3. The combination showed\\\\ncomplete recall of relevant new studies in three of eight\\\\nareas when CQ was paired with related articles and recall\\\\nof 0.67 or higher in all areas. Four of eight clinical areas\\\\nhad complete recall for the combination of CQ and SVM,\\\\nand recall was 0.80 or higher in all areas. Even for periph-\\\\neral vascular disease, where all three searches performed\\\\n\\\\nFig. 2. Recall of new studies by clinical area for each search method.\\\\nAbbreviations: SVM, support vector machine.\\\\n\\\\nfairly poorly individually, recall was 0.76 for the CQ/\\\\nrelated articles and 0.90 in the CQ/SVM pairing.\\\\n\\\\n3.3. Search precision\\\\n\\\\nPrecision, across the 10 AHRQ reviews, was 0.11 for the\\\\nCQ, 0.22 for related articles, and 0.19 for SVM (Table 3).\\\\n\\\\nFig. 1. Overlapping and unique retrieval of relevant studies by each\\\\nsearch method. The size of the circle is proportional to the size of\\\\nthe retrieval, but the overlap and unique portions are approximations.\\\\nExact ﬁgures for the components are labeled. Abbreviations: SVM,\\\\nsupport vector machine.\\\\n\\\\nFig. 3. Recall of new studies by clinical area for search methods in\\\\ncombination. Abbreviations: CQ, clinical query; SVM, support vector\\\\nmachine.\\\\n\\\\n\\\\x0cTable 3. Overall precision in the AHRQ Sample\\\\n\\\\n4. Discussion\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n113\\\\n\\\\nRetrieval method\\\\n\\\\nClinical query\\\\nRelated articles\\\\nSVM\\\\nClinical query þ related\\\\nClinical query þ SVM\\\\nClinical query þ related\\\\n\\\\narticles\\\\n\\\\nEligible\\\\nstudies\\\\nretrieved\\\\n\\\\n187\\\\n176\\\\n128\\\\n250\\\\n\\\\n243\\\\n276\\\\n\\\\nNo. of candidates\\\\n\\\\nretrieved\\\\n\\\\nPrecision\\\\n\\\\n1,637\\\\n814\\\\n659\\\\n2,318a\\\\n\\\\n2,247a\\\\n3,264a\\\\n\\\\n0.11\\\\n0.22\\\\n0.19\\\\n0.11\\\\n\\\\n0.11\\\\n0.08\\\\n\\\\narticles þ SVM\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and\\\\n\\\\nQuality; SVM, support vector machine.\\\\n\\\\na Number of candidates after removal of duplicate records.\\\\n\\\\nPrecision for the individual reviews ranged from 0.0 to 0.38\\\\nfor CQs, 0.004 to 0.80 for related articles, and 0.01 to 0.43\\\\nfor SVM. Overall precision was 0.11 for CQ and either\\\\nrelated articles or SVM. Overall precision was 0.08 when\\\\nCQ and both related articles and SVM were used. For\\\\nranked searches where a ﬁxed number of records will be\\\\nscreened, that number is always the denominator, so preci-\\\\nsion will tend to increase as the number of relevant records\\\\nincreasesdthe maximum retrieval size for SVM was cap-\\\\nped at 200 records.\\\\n\\\\n3.4. Retrieval consistency over time\\\\n\\\\nSearches were retested for the AHRQ cohort using\\\\nPubMed results obtained February 24, 2015. Recall of the\\\\nrelated articles searches across the 10 AHRQ reports was\\\\n0.64 originally and 0.50 when repeated (Fig. 4). Of the\\\\nthree extreme cases, one rose from 0.00 to 0.80, one re-\\\\nmained at 0.00, and one dropped from 1.00 to 0.60. CQ\\\\nhad overall\\\\nrecall of 0.68 originally and 0.70 when\\\\nrepeated. Changes in extreme cases were minimal. Overall\\\\ncombined recall of CQ and related article was 0.90 origi-\\\\nnally,\\\\nfalling to 0.85 when repeated. Considering the\\\\nextreme values, all review with combined recall of 1.00\\\\noriginally remained at 1.00, whereas the one review with\\\\n0.00 recall in combination showed recall of 0.80 when\\\\nrepeated.\\\\n\\\\n1.0\\\\n0.9\\\\n0.8\\\\n0.7\\\\n0.6\\\\n0.5\\\\n0.4\\\\n0.3\\\\n0.2\\\\n0.1\\\\n0.0\\\\n\\\\nOriginal\\\\n\\\\n2015\\\\n\\\\nClinical Query\\\\n\\\\nRelated Ar(cid:415)cles\\\\n\\\\nCombined\\\\n\\\\nFig. 4. Recall of new studies for search methods, alone and in combi-\\\\nnation originally (left) and when retested in 2015 (right).\\\\n\\\\nWe expected that\\\\n\\\\nthe sophisticated SVM approach\\\\nwould perform well when paired with more stripped-\\\\ndown, focused Boolean searches. Indeed, the two together\\\\nwere able to replace the multidatabase, multimodel\\\\nsearches used by the original review teams in updating\\\\nthe Cochrane searches. The new relevant studies for the Co-\\\\nchrane reviews may have been relatively easy to ﬁnd, but\\\\nthe pairing was also effective in the AHRQ set, and there\\\\nshowed better precision than is usually seen with traditional\\\\nsearches for systematic review of RCTs [9]. The AHRQ set\\\\nwas formed from the new studies identiﬁed for more com-\\\\nplex interventions in a rigorous, well-funded study [16].\\\\n\\\\nWe were surprised that the related articles approach per-\\\\nformed almost as well as SVM when used in combination\\\\nwith the CQ. The related article search has advantages over\\\\nSVMdit requires far less data preparation, and no special\\\\nsoftware is needed for its use. This makes it useful not only\\\\nin updating, but also, if appropriate seed articles can be\\\\nfound, in original reviews.\\\\n\\\\nThat both SVM and related articles sometimes showed\\\\npoor recall when used alone, but consistently good recall\\\\nwhen used with a Boolean method suggests that there is\\\\nreal beneﬁt in using complementary search methods to\\\\nquerying MEDLINE. The related article method is not very\\\\ntime consuming, and easily added to other planned search\\\\nefforts, while the third method used here, SVM, is more\\\\ntechnical. The combined performance of all three methods\\\\nwas surprising, but such a setup might become unpractical\\\\nin day-to-day use.\\\\n\\\\nOther investigators have similar ﬁndings. Examination\\\\nof the supplemental material presented in the Appendix C\\\\nto the article by Waffenschmidt et al. [18] reveals that\\\\nacross the 19 reviews tested, the combination of a simple\\\\nstructured Boolean search (SSBS) and the ﬁrst 50 retrieval\\\\nof PubMed similar articles showed complete retrieval of all\\\\nreference standard articles in 14 of 19 reviews and never\\\\nless than 0.90 recall (Appendix C at www.jclinepi.com pre-\\\\nsents data from the table by Waffenschmidt graphically).\\\\nWaffenschmidt concluded ‘‘the combination of these two\\\\nsearch techniques that are independent of each other seems\\\\nto compensate the respective weaknesses.’’\\\\n\\\\nSSBS search by Waffenschmidt was constructed in\\\\nPubMed from search terms selected for the indication and\\\\nintervention with PubMed’s narrow CQ ﬁlter (category:\\\\ntherapy). Their search using the similar articles feature (Re-\\\\nlCits) did not use a set of seed articles; rather, the RelCits\\\\nfunction was applied for each relevant citation previously\\\\nidentiﬁed in PubMed. Their test articles were the included\\\\nstudies in 19 systematic reviews of drugs.\\\\n\\\\nAgoritsas et al. also described search construction\\\\nmethods for searches based on CQ and PubMed similar ar-\\\\nticles, tested for their ability to retrieve the included studies\\\\nof 30 Cochrane reviews [17]. Although their search con-\\\\nstruction methods differed from those used here, both\\\\n\\\\n\\\\x0c', 'Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nComplementary approaches to searching MEDLINE may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\\\n\\\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\\\n\\\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\\\n\\\\n0R6, Canada\\\\n\\\\nUnited Kingdom\\\\n\\\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\\\n\\\\nAccepted 7 March 2016; Published online 11 March 2016\\\\n\\\\nAbstract\\\\n\\\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\\\n\\\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\\\ncombined. Related articles showed least stability over time.\\\\n\\\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\\\n\\\\n1. Introduction\\\\n\\\\nSystematic review searches need to have high recall.\\\\nMechanisms to achieve this usually include expansive\\\\nBoolean searches of multiple databases. This approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nMEDLINE gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. However,\\\\n\\\\nConﬂict of interest: None.\\\\nFunding: Some of this work was funded by the US Agency for Health-\\\\ncare Research and Quality, Department of Health and Human Services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* Corresponding author. Tel.: 613-737-7600.\\\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through MEDLINE. Recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in MEDLINE, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nSuccessful retrieval through a Boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. The search of\\\\nmultiple databases can therefore be helpful. The target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. A text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. These tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nWhat is new?\\\\n\\\\nKey ﬁndings\\\\n(cid:1) Searches using known relevant studies and the\\\\nsimilar articles feature of PubMed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) For a given question, if the Boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. The two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) The precision of\\\\n\\\\nmethod appears better\\\\nexhaustive Boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nWhat this adds to what was known?\\\\n(cid:1) The paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nIt is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nWhat is the implication and what should change\\\\nnow?\\\\n(cid:1) Using the simple and universally available PubMed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) If the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nMEDLINE.\\\\n\\\\nto large retrievals with low precision [9]. These resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional Boolean searches have\\\\nbeen sought [10].\\\\n\\\\nMost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nThese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. These methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nWe examined one method, support vector machine\\\\n(SVM), and compared it with a simple and readily available\\\\nmethod based on the PubMed similar articles feature. We\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. We paired both\\\\nwith a focused Boolean search within MEDLINE. We\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for SVM and seed articles for the PubMed-\\\\n\\\\nrelated articles search. We therefore sought to determine\\\\nif a focused Boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\nComparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. In this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (CQ), and PubMed-related articles are tested in a\\\\ncohort of six updated Cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 Agency for Healthcare\\\\nResearch and Quality (AHRQ) evidence reports. The\\\\nCochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. The replication in the\\\\nAHRQ cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. All records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nOther research [17,18] suggests that searches using the\\\\nPubMed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a Boolean-type search of\\\\nMEDLINE. We tested an additional search method, SVM,\\\\nin the Cochrane and AHRQ samples to permit comparison\\\\nwith our PubMed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the PubMed\\\\nsimilar articles method.\\\\n\\\\nThe aim of this article was to test whether the combined\\\\napproach of a focused Boolean search paired with a second\\\\nsearch using the similar articles feature of PubMed or SVM\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. Methods\\\\n\\\\n2.1. Formation of the study cohorts\\\\n\\\\nThis analysis uses a data set created for an updating\\\\nstudy sponsored by AHRQ [16]. Methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\\\nsearch of the ACP Journal Club database (Ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nAHRQ reports were identiﬁed through the PubMed\\\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\\\njrid21544]. Screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. Screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c110\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nFor inclusion, all reviews must have included random-\\\\nized controlled trials (RCTs) and provided meta-analysis\\\\nfor at least one outcome. The MEDLINE search strategies\\\\nhad to be reported in enough detail to permit replication.\\\\nFifteen AHRQ evidence reports thought\\\\nlikely to have\\\\nimportant new evidence were selected and used to validate\\\\nthe updating methods used in the main cohort.\\\\n\\\\nCochrane reviews were selected to meet minimum qual-\\\\nity and relevance standards, as deﬁned by ACP Journal\\\\nClub [19]. We also required that the review had been up-\\\\ndated and that the text of both the original review and an\\\\nupdated version was available. The search for the original\\\\nreview must have included MEDLINE and at least one\\\\nother electronic bibliographic database and one or more\\\\nnondatabase method such as hand searching or checking\\\\nreference lists. Such comprehensive searching was likely\\\\nto identify most relevant\\\\nliterature and form a useful\\\\ntraining set of included examples. At least 10 RCTs or\\\\nquasi-RCTs must have been included in the original review\\\\nto give an adequate training set for SVM.\\\\n\\\\n2.2. Test searches\\\\n\\\\n2.2.1. Clinical query\\\\n\\\\nBoolean searches were developed by a librarian experi-\\\\nenced in systematic review searches using a protocol\\\\n(Appendix A at www.jclinepi.com). Search strategies were\\\\ndeliberately simple, usually consisting of two or three\\\\nMedical Subject Heading (MeSH) terms representing the\\\\npopulation and intervention of the review. This search\\\\nwas limited by a CQ ﬁlter to ‘‘therapy (best balance of\\\\nsensitivity\\\\n‘‘Randomized\\\\ncontrolled trial.pt. or\\\\nrandomized.mp. or placebo.mp.’’\\\\n[20]. Boolean searches were run in OVID MEDLINE.\\\\n\\\\nspeciﬁcity)’’\\\\n\\\\nthat\\\\n\\\\nand\\\\n\\\\nis,\\\\n\\\\n2.2.2. Related articles\\\\n\\\\nSearches using the PubMed similar articles feature were\\\\nperformed using the PubMed unique identiﬁers (PMID) of\\\\nthe three newest and three largest studies included in the\\\\noriginal review as seed articles. There was no replacement\\\\nin the event of overlap between the largest and newest sets,\\\\nor if one of those studies was not indexed in MEDLINE.\\\\n(All studies included in the original reviews were checked\\\\nfor indexing status in MEDLINE.) The resulting set was\\\\nlimited to the publication type RCT and date limited to\\\\nthe period since the search date of the original review.\\\\n\\\\n2.2.3. Support vector machine\\\\n\\\\nThe SVM searches used for this project have not been\\\\npreviously described so the methods will be reported in\\\\nmore detail. SVM is a well established and broadly applied\\\\nclassiﬁcation method from machine learning [21,22]. A\\\\nclassiﬁcation task is closely related to an information\\\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\\\nthe collection. As SVM is a supervised algorithm,\\\\nit\\\\n\\\\nrequires training examples with known labels. It then places\\\\nthese training examples in a high-dimensional vector space\\\\nin such a way that examples of one class are separated from\\\\nexamples of the other class with the greatest distance to the\\\\nseparating boundary. Predicting the class of a new instance\\\\nhappens by placing it in the same space and determining on\\\\nwhich side of the boundary it falls. The distance to the\\\\nboundary is an indication of the prediction conﬁdence and\\\\nis used to rank instances (documents) as is essential in in-\\\\nformation retrieval tasks or to provide a cutoff threshold.\\\\n\\\\nIn our setup, the training set consisted of search results\\\\nfrom the original review formed with included instances\\\\nbeing the studies included in the review (the relevant re-\\\\ntrievals) and excluded instances being the studies found\\\\nby the search used in the original review but excluded from\\\\nthe review (irrelevant retrievals). The new examples to be\\\\nclassiﬁed were articles indexed since the date of the search\\\\nperformed in the original review.\\\\n\\\\nSVM searches were run on MEDLINE data stored\\\\nlocally at\\\\nthe National Research Council of Canada.\\\\nMEDLINE was refreshed before running the searches.\\\\nThe MEDLINE records were represented as bags of fea-\\\\ntures, where features consisted of lowercased words and\\\\nword combinations (up to four words) from the title and ab-\\\\nstract ﬁelds, full MeSH terms, and contents of all other\\\\nﬁelds in the MEDLINE record. Features were uniformly\\\\nweighted. The SVM implementation used was SVM Light\\\\n[23] used with a linear kernel.\\\\n\\\\nThe approach was piloted with one AHRQ evidence\\\\nreport, and various conﬁgurations were tried, observing\\\\nthe placement of several known new relevant studies within\\\\nthe retrieval. The MEDLINE set was ﬁltered with the\\\\nrevised Highly Sensitive Search Strategy (HSSS) [24]. Pre-\\\\nliminary testing indicated that this ﬁlter would retrieve\\\\n99.1% of MEDLINE-indexed relevant new evidence from\\\\nthe larger cohort of journal-published systematic reviews\\\\n(n 5 72) and Cochrane reviews (n 5 27). This HSSS ﬁlter\\\\nwas added primarily to improve processing speed, relative\\\\nto running the classiﬁer against all of MEDLINE. With\\\\nthe ﬁlter, processing for one systematic review took about\\\\n10 minutes. Next, a coarse-grained ﬁlter was applied;\\\\nincluded and excluded instances were combined in a single\\\\nset labeled POS. Adjacent PubMed IDs (the next higher\\\\nPMID to each member of POS, as long as that higher num-\\\\nber was not itself a member of POS) were combined in a set\\\\nto represent WORLD. The model was trained on POS vs.\\\\nWORLD, using words and phrases from title, abstract,\\\\nauthor names, and journal name. This model was run\\\\nagainst the records passing the HSSS ﬁlter.\\\\n\\\\nFour different subsets of\\\\n\\\\nthe SVM retrieval were\\\\nexamineddone set included all records with a relevance\\\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\\\n(SVM200point5). Other sets consisted of all records with\\\\nrelevance scores of 0.95 or more (SVM95), 0.90 or more\\\\n(SVM90), and 0.80 or more (SVM80). PubMed ID and\\\\nrelevance scores were recorded, and PubMed IDs were\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n111\\\\n\\\\nTable 1. Characteristics of the included reviews\\\\n\\\\nCochrane\\\\nreviews; N\\\\n\\\\nAHRQ evidence\\\\n\\\\nreports; N\\\\n\\\\nincorporated into the database of records for the reviewers\\\\nto screen. Only the results from SVM200point5 are re-\\\\nported here and are described as SVM.\\\\n\\\\n2.3. Determining performance of the test searches\\\\n\\\\nFor the updated Cochrane reviews, reference standard ar-\\\\nticles were those studies included in the updated review that\\\\nwere not in the original and which entered MEDLINE after\\\\nthe search date of the original review. For the AHRQ evi-\\\\ndence reports, the full retrieval set was screened. Records\\\\nfound relevant by the consensus of reviewers were consid-\\\\nered reference standard articles. Recall was the proportion\\\\nof reference standard articles identiﬁed by the search:\\\\n\\\\nNumber of reference standard articles found\\\\nTotal number of reference standard articles\\\\n\\\\nBioVenn software was used to analyze the overlap be-\\\\ntween the retrieval of relevant articles from the three\\\\nsearches and to create Venn diagrams [25].\\\\n\\\\nAll included reviews were classiﬁed into clinical area\\\\nbased on factors such as ISI journal classiﬁcation, the Co-\\\\nchrane Collaboration Review Group where the topic might\\\\nbe placed and the high level MeSH term under which the pop-\\\\nulation (condition) would be indexed. Performance of the\\\\nsearches in different clinical areas was displayed graphically\\\\nfor the searches both alone and in combinations, to allow ex-\\\\namination of differences in parallelism, level, and ﬂatness.\\\\n\\\\nPrecision is the proportion of all retrieved records that\\\\n\\\\nare relevant:\\\\n\\\\nNumber of reference standard articles found\\\\n\\\\nTotal number of records retrieved\\\\n\\\\nThe inverse of precision is the number needed to read to\\\\nﬁnd one eligible study, thus precision inﬂuences the work\\\\nof the review. Precision was calculated only for the AHRQ\\\\nevidence reviews. Because not all candidates retrieved by\\\\nthe Cochrane searches were assessed, precision could not\\\\nbe established in that sample.\\\\n\\\\n2.4. Determining stability of results over time\\\\n\\\\nThe CQ and related article searches, originally run in\\\\nMarch 2008, were repeated in February 2015 in the AHRQ\\\\ncohort. Recall of relevant items was compared with the\\\\noriginal retrievals to determine if any changes introduced\\\\nby National Library of Medicine might invalidate ﬁndings.\\\\nAs SVM conﬁguration is under the control of the investiga-\\\\ntors, it was not retested.\\\\n\\\\n3. Results\\\\n\\\\nSix Cochrane reviews and 10 AHRQ evidence reports\\\\nmet all inclusion criteria (Appendix B at www.jclinepi.\\\\ncom). Characteristics of the included reviews are shown\\\\nin Table 1.\\\\n\\\\n9a\\\\n1\\\\n3\\\\n\\\\n3\\\\n\\\\nd\\\\n1\\\\n\\\\nd\\\\n1\\\\n2\\\\n1\\\\n1\\\\n\\\\n1\\\\nd\\\\nd\\\\n\\\\nd\\\\n3\\\\n3\\\\n4\\\\n\\\\nCharacteristic\\\\n\\\\nTherapy evaluated\\\\n\\\\nMedications\\\\nMedical devices\\\\nProcedures\\\\n\\\\nClinical topic area\\\\n\\\\nCardiac and cardiovascular\\\\n\\\\nsystems\\\\n\\\\nCritical care\\\\nEndocrinology and\\\\n\\\\nmetabolism\\\\n\\\\nInfectious disease\\\\nClinical neurology\\\\nObstetrics and gynecology\\\\nOncology\\\\nPeripheral vascular\\\\n\\\\ndiseases\\\\nPsychiatry\\\\nRespiratory systems\\\\nUrology and nephrology\\\\n\\\\nPublication period\\\\n\\\\nMarch 1997eApril 1999\\\\nMay 1999eJune 2001\\\\nJuly 2001eAugust 2003\\\\nSeptember 2003e\\\\nDecember 2005\\\\n\\\\nMedian included trials\\\\n\\\\nMedian included\\\\n\\\\nparticipants\\\\n\\\\nMEDLINE coverage of\\\\n\\\\n5\\\\n1\\\\nd\\\\n\\\\nd\\\\n\\\\n1\\\\nd\\\\n\\\\n3\\\\nd\\\\nd\\\\nd\\\\nd\\\\n\\\\nd\\\\n1\\\\n1\\\\n\\\\n3\\\\n1\\\\n2\\\\nd\\\\n\\\\n17 (IQR, 14e20)\\\\n\\\\n96 (IQR,\\\\n\\\\n31.75e121.5)\\\\n8,679 (IQR,\\\\n22,830 (IQR, 14,\\\\n4,085e50,109)\\\\n172e49,687)\\\\n99/107 (92.5%) 969/980 (98.8%)\\\\n\\\\nincluded articles\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\\\n\\\\nity; IQR, interquartile range.\\\\n\\\\na Some AHRQ reviews included more than one class of therapy.\\\\n\\\\nRecall of new relevant studies is shown in Table 2. The\\\\nupdated Cochrane reviews had 20 new studies identiﬁed by\\\\nthe review authors and added in the updates. Recall of these\\\\n20 ranged from 1.00 for the related articles method to a low\\\\nof 0.80 CQs (Table 2). Our team identiﬁed 277 new studies\\\\nas relevant for inclusion in the 10 evidence reports that\\\\nwere updated. All test searches showed lower recall for this\\\\ncohort than for the Cochrane reviews and here the CQ out-\\\\nperformed both ranking searches (Table 2).\\\\n\\\\n3.1. Recall of CQ combined with a ranking method\\\\n\\\\nOf the 297 new relevant studies identiﬁed, the combina-\\\\ntion of CQ and related articles searches identiﬁed 270\\\\n(overall\\\\nin\\\\nthe Cochrane cohort and 0.90 (250/277) in the AHRQ\\\\ncohort.\\\\n\\\\nrecall of 0.91). Recall was 1.00 (20/20)\\\\n\\\\nThe combination of CQ and SVM identiﬁed 263 studies\\\\n(overall recall of 0.89). Recall was 1.00 (20/20) in the\\\\nCochrane cohort and 0.88 (243/277) in the AHRQ cohort.\\\\nWhen all three methods were used together, recall was\\\\n0.997 with 296 of the 297 studies identiﬁed.\\\\n\\\\n\\\\x0c112\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nTable 2. Recall of eligible studies by the search methods\\\\n\\\\nCochrane reviews\\\\n\\\\nAHRQ evidence\\\\n\\\\nreports\\\\n\\\\nRetrieval method\\\\n\\\\nClinical query\\\\nRelated articles\\\\nSVM\\\\nTotal\\\\n\\\\nN\\\\n\\\\n16\\\\n20\\\\n19\\\\n20\\\\n\\\\nRecall\\\\n\\\\n0.80\\\\n1.00\\\\n0.95\\\\n\\\\nN\\\\n\\\\n188\\\\n176\\\\n129\\\\n277\\\\n\\\\nRecall\\\\n\\\\n0.68\\\\n0.64\\\\n0.47\\\\n\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\\\n\\\\nity; SVM, support vector machine.\\\\n\\\\nAcross the two cohorts, the overlap and unique component from\\\\nthe retrieval of relevant records by the test searches was examined\\\\n(Fig. 1).\\\\n\\\\n3.2. Consistency across clinical areas\\\\n\\\\nThirteen clinical areas were represented in the larger\\\\nstudy that included 72 journal-published reviews, but only\\\\neight of those clinical areas had two or more new relevant\\\\nstudies in the smaller Cochrane and AHRQ cohorts. Fig. 2\\\\nshows recall of new studies by the three types of searches,\\\\nfor these eight clinical areas.\\\\n\\\\nCombined recall of the CQ Boolean search paired with a\\\\nranking search is shown in Fig. 3. The combination showed\\\\ncomplete recall of relevant new studies in three of eight\\\\nareas when CQ was paired with related articles and recall\\\\nof 0.67 or higher in all areas. Four of eight clinical areas\\\\nhad complete recall for the combination of CQ and SVM,\\\\nand recall was 0.80 or higher in all areas. Even for periph-\\\\neral vascular disease, where all three searches performed\\\\n\\\\nFig. 2. Recall of new studies by clinical area for each search method.\\\\nAbbreviations: SVM, support vector machine.\\\\n\\\\nfairly poorly individually, recall was 0.76 for the CQ/\\\\nrelated articles and 0.90 in the CQ/SVM pairing.\\\\n\\\\n3.3. Search precision\\\\n\\\\nPrecision, across the 10 AHRQ reviews, was 0.11 for the\\\\nCQ, 0.22 for related articles, and 0.19 for SVM (Table 3).\\\\n\\\\nFig. 1. Overlapping and unique retrieval of relevant studies by each\\\\nsearch method. The size of the circle is proportional to the size of\\\\nthe retrieval, but the overlap and unique portions are approximations.\\\\nExact ﬁgures for the components are labeled. Abbreviations: SVM,\\\\nsupport vector machine.\\\\n\\\\nFig. 3. Recall of new studies by clinical area for search methods in\\\\ncombination. Abbreviations: CQ, clinical query; SVM, support vector\\\\nmachine.\\\\n\\\\n\\\\x0cTable 3. Overall precision in the AHRQ Sample\\\\n\\\\n4. Discussion\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n113\\\\n\\\\nRetrieval method\\\\n\\\\nClinical query\\\\nRelated articles\\\\nSVM\\\\nClinical query þ related\\\\nClinical query þ SVM\\\\nClinical query þ related\\\\n\\\\narticles\\\\n\\\\nEligible\\\\nstudies\\\\nretrieved\\\\n\\\\n187\\\\n176\\\\n128\\\\n250\\\\n\\\\n243\\\\n276\\\\n\\\\nNo. of candidates\\\\n\\\\nretrieved\\\\n\\\\nPrecision\\\\n\\\\n1,637\\\\n814\\\\n659\\\\n2,318a\\\\n\\\\n2,247a\\\\n3,264a\\\\n\\\\n0.11\\\\n0.22\\\\n0.19\\\\n0.11\\\\n\\\\n0.11\\\\n0.08\\\\n\\\\narticles þ SVM\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and\\\\n\\\\nQuality; SVM, support vector machine.\\\\n\\\\na Number of candidates after removal of duplicate records.\\\\n\\\\nPrecision for the individual reviews ranged from 0.0 to 0.38\\\\nfor CQs, 0.004 to 0.80 for related articles, and 0.01 to 0.43\\\\nfor SVM. Overall precision was 0.11 for CQ and either\\\\nrelated articles or SVM. Overall precision was 0.08 when\\\\nCQ and both related articles and SVM were used. For\\\\nranked searches where a ﬁxed number of records will be\\\\nscreened, that number is always the denominator, so preci-\\\\nsion will tend to increase as the number of relevant records\\\\nincreasesdthe maximum retrieval size for SVM was cap-\\\\nped at 200 records.\\\\n\\\\n3.4. Retrieval consistency over time\\\\n\\\\nSearches were retested for the AHRQ cohort using\\\\nPubMed results obtained February 24, 2015. Recall of the\\\\nrelated articles searches across the 10 AHRQ reports was\\\\n0.64 originally and 0.50 when repeated (Fig. 4). Of the\\\\nthree extreme cases, one rose from 0.00 to 0.80, one re-\\\\nmained at 0.00, and one dropped from 1.00 to 0.60. CQ\\\\nhad overall\\\\nrecall of 0.68 originally and 0.70 when\\\\nrepeated. Changes in extreme cases were minimal. Overall\\\\ncombined recall of CQ and related article was 0.90 origi-\\\\nnally,\\\\nfalling to 0.85 when repeated. Considering the\\\\nextreme values, all review with combined recall of 1.00\\\\noriginally remained at 1.00, whereas the one review with\\\\n0.00 recall in combination showed recall of 0.80 when\\\\nrepeated.\\\\n\\\\n1.0\\\\n0.9\\\\n0.8\\\\n0.7\\\\n0.6\\\\n0.5\\\\n0.4\\\\n0.3\\\\n0.2\\\\n0.1\\\\n0.0\\\\n\\\\nOriginal\\\\n\\\\n2015\\\\n\\\\nClinical Query\\\\n\\\\nRelated Ar(cid:415)cles\\\\n\\\\nCombined\\\\n\\\\nFig. 4. Recall of new studies for search methods, alone and in combi-\\\\nnation originally (left) and when retested in 2015 (right).\\\\n\\\\nWe expected that\\\\n\\\\nthe sophisticated SVM approach\\\\nwould perform well when paired with more stripped-\\\\ndown, focused Boolean searches. Indeed, the two together\\\\nwere able to replace the multidatabase, multimodel\\\\nsearches used by the original review teams in updating\\\\nthe Cochrane searches. The new relevant studies for the Co-\\\\nchrane reviews may have been relatively easy to ﬁnd, but\\\\nthe pairing was also effective in the AHRQ set, and there\\\\nshowed better precision than is usually seen with traditional\\\\nsearches for systematic review of RCTs [9]. The AHRQ set\\\\nwas formed from the new studies identiﬁed for more com-\\\\nplex interventions in a rigorous, well-funded study [16].\\\\n\\\\nWe were surprised that the related articles approach per-\\\\nformed almost as well as SVM when used in combination\\\\nwith the CQ. The related article search has advantages over\\\\nSVMdit requires far less data preparation, and no special\\\\nsoftware is needed for its use. This makes it useful not only\\\\nin updating, but also, if appropriate seed articles can be\\\\nfound, in original reviews.\\\\n\\\\nThat both SVM and related articles sometimes showed\\\\npoor recall when used alone, but consistently good recall\\\\nwhen used with a Boolean method suggests that there is\\\\nreal beneﬁt in using complementary search methods to\\\\nquerying MEDLINE. The related article method is not very\\\\ntime consuming, and easily added to other planned search\\\\nefforts, while the third method used here, SVM, is more\\\\ntechnical. The combined performance of all three methods\\\\nwas surprising, but such a setup might become unpractical\\\\nin day-to-day use.\\\\n\\\\nOther investigators have similar ﬁndings. Examination\\\\nof the supplemental material presented in the Appendix C\\\\nto the article by Waffenschmidt et al. [18] reveals that\\\\nacross the 19 reviews tested, the combination of a simple\\\\nstructured Boolean search (SSBS) and the ﬁrst 50 retrieval\\\\nof PubMed similar articles showed complete retrieval of all\\\\nreference standard articles in 14 of 19 reviews and never\\\\nless than 0.90 recall (Appendix C at www.jclinepi.com pre-\\\\nsents data from the table by Waffenschmidt graphically).\\\\nWaffenschmidt concluded ‘‘the combination of these two\\\\nsearch techniques that are independent of each other seems\\\\nto compensate the respective weaknesses.’’\\\\n\\\\nSSBS search by Waffenschmidt was constructed in\\\\nPubMed from search terms selected for the indication and\\\\nintervention with PubMed’s narrow CQ ﬁlter (category:\\\\ntherapy). Their search using the similar articles feature (Re-\\\\nlCits) did not use a set of seed articles; rather, the RelCits\\\\nfunction was applied for each relevant citation previously\\\\nidentiﬁed in PubMed. Their test articles were the included\\\\nstudies in 19 systematic reviews of drugs.\\\\n\\\\nAgoritsas et al. also described search construction\\\\nmethods for searches based on CQ and PubMed similar ar-\\\\nticles, tested for their ability to retrieve the included studies\\\\nof 30 Cochrane reviews [17]. Although their search con-\\\\nstruction methods differed from those used here, both\\\\n\\\\n\\\\x0c114\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nsearches were derived using standard methods. Their\\\\napproach to the structured Boolean search used terms from\\\\nthe population, intervention, and comparison with the CQ,\\\\nlimited to humans and English. Two clinicians selected the\\\\nPubMed similar articles seeds from the initial PubMed\\\\nretrieval. They noted that no one method provided consis-\\\\ntently high retrieval [17].\\\\n\\\\nThus, the robust nature of this pairing of Boolean and\\\\nnon-Boolean searches has been shown in several contexts,\\\\ngiving support\\\\nto the hypothesis that methods such as\\\\nrelated articles and SVM can compensate for the variation\\\\ninherent\\\\nin selecting search terms or assigning subject\\\\nheadings during indexing.\\\\n\\\\nConsidering stability of the searches over time, the\\\\nminimal change seen in the performance of the CQ is\\\\nlikely explained by indexing changes of a few records.\\\\nThe computation appears not to have changed. This sug-\\\\ngests that indexing changes may also impact SVM results\\\\nover time; however, this is likely to result in improved\\\\nperformance, as was seen with the CQ. The computation\\\\nof similar articles is still described by National Library\\\\nof Medicine as being based on the algorithm described\\\\nby Kim et al. [26]. That algorithm would suggest changes\\\\nover time in the nearest neighbor score as the frequency of\\\\ncertain terms in the MEDLINE corpus changes. A full\\\\nexploration of the changes in nearest neighbor scores of\\\\nsimilar articles over time is beyond the scope of this\\\\nstudy. It should be noted that when the similar articles\\\\nfeature was initially studied, it was simple to submit seed\\\\narticles, and then add limits such as date or RCT publica-\\\\ntion type. On retesting, such additional limits were more\\\\ncomplex to apply, and the elink utility seemed to be the\\\\nmost practical approach to identifying the related articles\\\\n[27].\\\\n\\\\nThe beneﬁt of the complementary searches, relative to\\\\nBoolean searching alone, was greatest for AHRQ evidence\\\\nreports and less for the simpler Cochrane reviews, suggest-\\\\ning that this approach may be particularly useful for more\\\\ncomplex evidence. There are examples of its utility in the\\\\nliterature. For example, in a realist review of a multidisci-\\\\nplinary body of\\\\nliterature identifying six domains of\\\\nClinical Practice Guideline implementability, the use of\\\\nPubMed’s similar articles feature as a third stage of search-\\\\ning identiﬁed 131 records of which 104 were relevant [28].\\\\nIn a systematic review of evidence on the links between\\\\npatient experience and clinical safety and effectiveness,\\\\nthe authors used PubMed similar articles to snowball on\\\\narticles\\\\nidentiﬁed through an EMBASE search to\\\\novercome the limitations of predeﬁned searches for com-\\\\nplex evidence [29].\\\\n\\\\nThere is always the possibility that a search, or even a\\\\npair of searches, will fail. One method to detect such\\\\nfailures is to test whether the search strategies ﬁnd known\\\\nrelevant items [30]. In the updating case, this is easily done\\\\nusing the included studies of the original review as a test\\\\n\\\\nset, allowing the review to determine the MEDLINE\\\\ncoverage of their particular topic at the same time.\\\\n\\\\n4.1. Limitations\\\\n\\\\nThere are two limitations to our proposed strategy. Other\\\\ndatabases should be searched in the unusual event that\\\\nnumerous studies, representing more than a small propor-\\\\ntion of the total N, are not included in MEDLINE. Second,\\\\nwhen it is important to ﬁnd articles too new to be indexed\\\\nby MEDLINE, systematic reviewers may wish to conduct a\\\\nsimple PubMed search limited to the nonindexed subsets\\\\n[31,32].\\\\n\\\\n5. Conclusion\\\\n\\\\nThe general approach of a Boolean plus a ranking\\\\nsearch is effective in MEDLINE retrieval for systematic\\\\nreviews. Very high levels of identiﬁcation of relevant\\\\nMEDLINE records, with adequate precision, are possible\\\\nusing a focused Boolean search complemented by a docu-\\\\nment similarity or ranking method. The efﬁcacy of a\\\\nfocused Boolean search paired with a search using the\\\\nPubMed similar articles feature is in agreement with pre-\\\\nvious studies [17,18], and this study shows that this com-\\\\nplementary effect also occurs when SVM is used as the\\\\nadditional method. The beneﬁt of using two complemen-\\\\ntary approaches to achieving high recall in MEDLINE is\\\\na robust effect.\\\\n\\\\nPubMed-related articles is a parsimonious method, as it\\\\nis readily available to all review teams without cost. The\\\\napproach is robust across clinical domains, and the effect\\\\nhas now been demonstrated in several samples. The method\\\\nmay be sufﬁcient for updating systematic reviews of inter-\\\\nventions and may be used for new reviews of interventions\\\\nwhen paired with a trials registry search. It is likely to work\\\\nin any type of search where MEDLINE provides good sub-\\\\nject coverage even if retrieval through traditional search\\\\nmethods has been challenging.\\\\n\\\\nAcknowledgments\\\\n\\\\nMohamed Ansari and Jun Ji undertook screening and\\\\nrelevance assessment of candidate studies retrieved by the\\\\ntest\\\\nsearches, Tamara Rader developed the Boolean\\\\nsearches, and Raymond Daniel processed related articles\\\\nsearches and obtained full-text articles. David Moher cosu-\\\\npervised Margaret Sampson doctoral dissertation and pro-\\\\nvided invaluable advice and guidance.\\\\n\\\\nSupplementary data\\\\n\\\\nSupplementary data related to this article can be found at\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004.\\\\n\\\\n\\\\x0c', 'Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nComplementary approaches to searching MEDLINE may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\\\n\\\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\\\n\\\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\\\n\\\\n0R6, Canada\\\\n\\\\nUnited Kingdom\\\\n\\\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\\\n\\\\nAccepted 7 March 2016; Published online 11 March 2016\\\\n\\\\nAbstract\\\\n\\\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\\\n\\\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\\\ncombined. Related articles showed least stability over time.\\\\n\\\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\\\n\\\\n1. Introduction\\\\n\\\\nSystematic review searches need to have high recall.\\\\nMechanisms to achieve this usually include expansive\\\\nBoolean searches of multiple databases. This approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nMEDLINE gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. However,\\\\n\\\\nConﬂict of interest: None.\\\\nFunding: Some of this work was funded by the US Agency for Health-\\\\ncare Research and Quality, Department of Health and Human Services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* Corresponding author. Tel.: 613-737-7600.\\\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through MEDLINE. Recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in MEDLINE, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nSuccessful retrieval through a Boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. The search of\\\\nmultiple databases can therefore be helpful. The target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. A text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. These tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nWhat is new?\\\\n\\\\nKey ﬁndings\\\\n(cid:1) Searches using known relevant studies and the\\\\nsimilar articles feature of PubMed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) For a given question, if the Boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. The two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) The precision of\\\\n\\\\nmethod appears better\\\\nexhaustive Boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nWhat this adds to what was known?\\\\n(cid:1) The paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nIt is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nWhat is the implication and what should change\\\\nnow?\\\\n(cid:1) Using the simple and universally available PubMed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) If the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nMEDLINE.\\\\n\\\\nto large retrievals with low precision [9]. These resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional Boolean searches have\\\\nbeen sought [10].\\\\n\\\\nMost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nThese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. These methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nWe examined one method, support vector machine\\\\n(SVM), and compared it with a simple and readily available\\\\nmethod based on the PubMed similar articles feature. We\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. We paired both\\\\nwith a focused Boolean search within MEDLINE. We\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for SVM and seed articles for the PubMed-\\\\n\\\\nrelated articles search. We therefore sought to determine\\\\nif a focused Boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\nComparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. In this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (CQ), and PubMed-related articles are tested in a\\\\ncohort of six updated Cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 Agency for Healthcare\\\\nResearch and Quality (AHRQ) evidence reports. The\\\\nCochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. The replication in the\\\\nAHRQ cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. All records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nOther research [17,18] suggests that searches using the\\\\nPubMed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a Boolean-type search of\\\\nMEDLINE. We tested an additional search method, SVM,\\\\nin the Cochrane and AHRQ samples to permit comparison\\\\nwith our PubMed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the PubMed\\\\nsimilar articles method.\\\\n\\\\nThe aim of this article was to test whether the combined\\\\napproach of a focused Boolean search paired with a second\\\\nsearch using the similar articles feature of PubMed or SVM\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. Methods\\\\n\\\\n2.1. Formation of the study cohorts\\\\n\\\\nThis analysis uses a data set created for an updating\\\\nstudy sponsored by AHRQ [16]. Methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\\\nsearch of the ACP Journal Club database (Ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nAHRQ reports were identiﬁed through the PubMed\\\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\\\njrid21544]. Screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. Screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c110\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nFor inclusion, all reviews must have included random-\\\\nized controlled trials (RCTs) and provided meta-analysis\\\\nfor at least one outcome. The MEDLINE search strategies\\\\nhad to be reported in enough detail to permit replication.\\\\nFifteen AHRQ evidence reports thought\\\\nlikely to have\\\\nimportant new evidence were selected and used to validate\\\\nthe updating methods used in the main cohort.\\\\n\\\\nCochrane reviews were selected to meet minimum qual-\\\\nity and relevance standards, as deﬁned by ACP Journal\\\\nClub [19]. We also required that the review had been up-\\\\ndated and that the text of both the original review and an\\\\nupdated version was available. The search for the original\\\\nreview must have included MEDLINE and at least one\\\\nother electronic bibliographic database and one or more\\\\nnondatabase method such as hand searching or checking\\\\nreference lists. Such comprehensive searching was likely\\\\nto identify most relevant\\\\nliterature and form a useful\\\\ntraining set of included examples. At least 10 RCTs or\\\\nquasi-RCTs must have been included in the original review\\\\nto give an adequate training set for SVM.\\\\n\\\\n2.2. Test searches\\\\n\\\\n2.2.1. Clinical query\\\\n\\\\nBoolean searches were developed by a librarian experi-\\\\nenced in systematic review searches using a protocol\\\\n(Appendix A at www.jclinepi.com). Search strategies were\\\\ndeliberately simple, usually consisting of two or three\\\\nMedical Subject Heading (MeSH) terms representing the\\\\npopulation and intervention of the review. This search\\\\nwas limited by a CQ ﬁlter to ‘‘therapy (best balance of\\\\nsensitivity\\\\n‘‘Randomized\\\\ncontrolled trial.pt. or\\\\nrandomized.mp. or placebo.mp.’’\\\\n[20]. Boolean searches were run in OVID MEDLINE.\\\\n\\\\nspeciﬁcity)’’\\\\n\\\\nthat\\\\n\\\\nand\\\\n\\\\nis,\\\\n\\\\n2.2.2. Related articles\\\\n\\\\nSearches using the PubMed similar articles feature were\\\\nperformed using the PubMed unique identiﬁers (PMID) of\\\\nthe three newest and three largest studies included in the\\\\noriginal review as seed articles. There was no replacement\\\\nin the event of overlap between the largest and newest sets,\\\\nor if one of those studies was not indexed in MEDLINE.\\\\n(All studies included in the original reviews were checked\\\\nfor indexing status in MEDLINE.) The resulting set was\\\\nlimited to the publication type RCT and date limited to\\\\nthe period since the search date of the original review.\\\\n\\\\n2.2.3. Support vector machine\\\\n\\\\nThe SVM searches used for this project have not been\\\\npreviously described so the methods will be reported in\\\\nmore detail. SVM is a well established and broadly applied\\\\nclassiﬁcation method from machine learning [21,22]. A\\\\nclassiﬁcation task is closely related to an information\\\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\\\nthe collection. As SVM is a supervised algorithm,\\\\nit\\\\n\\\\nrequires training examples with known labels. It then places\\\\nthese training examples in a high-dimensional vector space\\\\nin such a way that examples of one class are separated from\\\\nexamples of the other class with the greatest distance to the\\\\nseparating boundary. Predicting the class of a new instance\\\\nhappens by placing it in the same space and determining on\\\\nwhich side of the boundary it falls. The distance to the\\\\nboundary is an indication of the prediction conﬁdence and\\\\nis used to rank instances (documents) as is essential in in-\\\\nformation retrieval tasks or to provide a cutoff threshold.\\\\n\\\\nIn our setup, the training set consisted of search results\\\\nfrom the original review formed with included instances\\\\nbeing the studies included in the review (the relevant re-\\\\ntrievals) and excluded instances being the studies found\\\\nby the search used in the original review but excluded from\\\\nthe review (irrelevant retrievals). The new examples to be\\\\nclassiﬁed were articles indexed since the date of the search\\\\nperformed in the original review.\\\\n\\\\nSVM searches were run on MEDLINE data stored\\\\nlocally at\\\\nthe National Research Council of Canada.\\\\nMEDLINE was refreshed before running the searches.\\\\nThe MEDLINE records were represented as bags of fea-\\\\ntures, where features consisted of lowercased words and\\\\nword combinations (up to four words) from the title and ab-\\\\nstract ﬁelds, full MeSH terms, and contents of all other\\\\nﬁelds in the MEDLINE record. Features were uniformly\\\\nweighted. The SVM implementation used was SVM Light\\\\n[23] used with a linear kernel.\\\\n\\\\nThe approach was piloted with one AHRQ evidence\\\\nreport, and various conﬁgurations were tried, observing\\\\nthe placement of several known new relevant studies within\\\\nthe retrieval. The MEDLINE set was ﬁltered with the\\\\nrevised Highly Sensitive Search Strategy (HSSS) [24]. Pre-\\\\nliminary testing indicated that this ﬁlter would retrieve\\\\n99.1% of MEDLINE-indexed relevant new evidence from\\\\nthe larger cohort of journal-published systematic reviews\\\\n(n 5 72) and Cochrane reviews (n 5 27). This HSSS ﬁlter\\\\nwas added primarily to improve processing speed, relative\\\\nto running the classiﬁer against all of MEDLINE. With\\\\nthe ﬁlter, processing for one systematic review took about\\\\n10 minutes. Next, a coarse-grained ﬁlter was applied;\\\\nincluded and excluded instances were combined in a single\\\\nset labeled POS. Adjacent PubMed IDs (the next higher\\\\nPMID to each member of POS, as long as that higher num-\\\\nber was not itself a member of POS) were combined in a set\\\\nto represent WORLD. The model was trained on POS vs.\\\\nWORLD, using words and phrases from title, abstract,\\\\nauthor names, and journal name. This model was run\\\\nagainst the records passing the HSSS ﬁlter.\\\\n\\\\nFour different subsets of\\\\n\\\\nthe SVM retrieval were\\\\nexamineddone set included all records with a relevance\\\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\\\n(SVM200point5). Other sets consisted of all records with\\\\nrelevance scores of 0.95 or more (SVM95), 0.90 or more\\\\n(SVM90), and 0.80 or more (SVM80). PubMed ID and\\\\nrelevance scores were recorded, and PubMed IDs were\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n111\\\\n\\\\nTable 1. Characteristics of the included reviews\\\\n\\\\nCochrane\\\\nreviews; N\\\\n\\\\nAHRQ evidence\\\\n\\\\nreports; N\\\\n\\\\nincorporated into the database of records for the reviewers\\\\nto screen. Only the results from SVM200point5 are re-\\\\nported here and are described as SVM.\\\\n\\\\n2.3. Determining performance of the test searches\\\\n\\\\nFor the updated Cochrane reviews, reference standard ar-\\\\nticles were those studies included in the updated review that\\\\nwere not in the original and which entered MEDLINE after\\\\nthe search date of the original review. For the AHRQ evi-\\\\ndence reports, the full retrieval set was screened. Records\\\\nfound relevant by the consensus of reviewers were consid-\\\\nered reference standard articles. Recall was the proportion\\\\nof reference standard articles identiﬁed by the search:\\\\n\\\\nNumber of reference standard articles found\\\\nTotal number of reference standard articles\\\\n\\\\nBioVenn software was used to analyze the overlap be-\\\\ntween the retrieval of relevant articles from the three\\\\nsearches and to create Venn diagrams [25].\\\\n\\\\nAll included reviews were classiﬁed into clinical area\\\\nbased on factors such as ISI journal classiﬁcation, the Co-\\\\nchrane Collaboration Review Group where the topic might\\\\nbe placed and the high level MeSH term under which the pop-\\\\nulation (condition) would be indexed. Performance of the\\\\nsearches in different clinical areas was displayed graphically\\\\nfor the searches both alone and in combinations, to allow ex-\\\\namination of differences in parallelism, level, and ﬂatness.\\\\n\\\\nPrecision is the proportion of all retrieved records that\\\\n\\\\nare relevant:\\\\n\\\\nNumber of reference standard articles found\\\\n\\\\nTotal number of records retrieved\\\\n\\\\nThe inverse of precision is the number needed to read to\\\\nﬁnd one eligible study, thus precision inﬂuences the work\\\\nof the review. Precision was calculated only for the AHRQ\\\\nevidence reviews. Because not all candidates retrieved by\\\\nthe Cochrane searches were assessed, precision could not\\\\nbe established in that sample.\\\\n\\\\n2.4. Determining stability of results over time\\\\n\\\\nThe CQ and related article searches, originally run in\\\\nMarch 2008, were repeated in February 2015 in the AHRQ\\\\ncohort. Recall of relevant items was compared with the\\\\noriginal retrievals to determine if any changes introduced\\\\nby National Library of Medicine might invalidate ﬁndings.\\\\nAs SVM conﬁguration is under the control of the investiga-\\\\ntors, it was not retested.\\\\n\\\\n3. Results\\\\n\\\\nSix Cochrane reviews and 10 AHRQ evidence reports\\\\nmet all inclusion criteria (Appendix B at www.jclinepi.\\\\ncom). Characteristics of the included reviews are shown\\\\nin Table 1.\\\\n\\\\n9a\\\\n1\\\\n3\\\\n\\\\n3\\\\n\\\\nd\\\\n1\\\\n\\\\nd\\\\n1\\\\n2\\\\n1\\\\n1\\\\n\\\\n1\\\\nd\\\\nd\\\\n\\\\nd\\\\n3\\\\n3\\\\n4\\\\n\\\\nCharacteristic\\\\n\\\\nTherapy evaluated\\\\n\\\\nMedications\\\\nMedical devices\\\\nProcedures\\\\n\\\\nClinical topic area\\\\n\\\\nCardiac and cardiovascular\\\\n\\\\nsystems\\\\n\\\\nCritical care\\\\nEndocrinology and\\\\n\\\\nmetabolism\\\\n\\\\nInfectious disease\\\\nClinical neurology\\\\nObstetrics and gynecology\\\\nOncology\\\\nPeripheral vascular\\\\n\\\\ndiseases\\\\nPsychiatry\\\\nRespiratory systems\\\\nUrology and nephrology\\\\n\\\\nPublication period\\\\n\\\\nMarch 1997eApril 1999\\\\nMay 1999eJune 2001\\\\nJuly 2001eAugust 2003\\\\nSeptember 2003e\\\\nDecember 2005\\\\n\\\\nMedian included trials\\\\n\\\\nMedian included\\\\n\\\\nparticipants\\\\n\\\\nMEDLINE coverage of\\\\n\\\\n5\\\\n1\\\\nd\\\\n\\\\nd\\\\n\\\\n1\\\\nd\\\\n\\\\n3\\\\nd\\\\nd\\\\nd\\\\nd\\\\n\\\\nd\\\\n1\\\\n1\\\\n\\\\n3\\\\n1\\\\n2\\\\nd\\\\n\\\\n17 (IQR, 14e20)\\\\n\\\\n96 (IQR,\\\\n\\\\n31.75e121.5)\\\\n8,679 (IQR,\\\\n22,830 (IQR, 14,\\\\n4,085e50,109)\\\\n172e49,687)\\\\n99/107 (92.5%) 969/980 (98.8%)\\\\n\\\\nincluded articles\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\\\n\\\\nity; IQR, interquartile range.\\\\n\\\\na Some AHRQ reviews included more than one class of therapy.\\\\n\\\\nRecall of new relevant studies is shown in Table 2. The\\\\nupdated Cochrane reviews had 20 new studies identiﬁed by\\\\nthe review authors and added in the updates. Recall of these\\\\n20 ranged from 1.00 for the related articles method to a low\\\\nof 0.80 CQs (Table 2). Our team identiﬁed 277 new studies\\\\nas relevant for inclusion in the 10 evidence reports that\\\\nwere updated. All test searches showed lower recall for this\\\\ncohort than for the Cochrane reviews and here the CQ out-\\\\nperformed both ranking searches (Table 2).\\\\n\\\\n3.1. Recall of CQ combined with a ranking method\\\\n\\\\nOf the 297 new relevant studies identiﬁed, the combina-\\\\ntion of CQ and related articles searches identiﬁed 270\\\\n(overall\\\\nin\\\\nthe Cochrane cohort and 0.90 (250/277) in the AHRQ\\\\ncohort.\\\\n\\\\nrecall of 0.91). Recall was 1.00 (20/20)\\\\n\\\\nThe combination of CQ and SVM identiﬁed 263 studies\\\\n(overall recall of 0.89). Recall was 1.00 (20/20) in the\\\\nCochrane cohort and 0.88 (243/277) in the AHRQ cohort.\\\\nWhen all three methods were used together, recall was\\\\n0.997 with 296 of the 297 studies identiﬁed.\\\\n\\\\n\\\\x0c112\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nTable 2. Recall of eligible studies by the search methods\\\\n\\\\nCochrane reviews\\\\n\\\\nAHRQ evidence\\\\n\\\\nreports\\\\n\\\\nRetrieval method\\\\n\\\\nClinical query\\\\nRelated articles\\\\nSVM\\\\nTotal\\\\n\\\\nN\\\\n\\\\n16\\\\n20\\\\n19\\\\n20\\\\n\\\\nRecall\\\\n\\\\n0.80\\\\n1.00\\\\n0.95\\\\n\\\\nN\\\\n\\\\n188\\\\n176\\\\n129\\\\n277\\\\n\\\\nRecall\\\\n\\\\n0.68\\\\n0.64\\\\n0.47\\\\n\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\\\n\\\\nity; SVM, support vector machine.\\\\n\\\\nAcross the two cohorts, the overlap and unique component from\\\\nthe retrieval of relevant records by the test searches was examined\\\\n(Fig. 1).\\\\n\\\\n3.2. Consistency across clinical areas\\\\n\\\\nThirteen clinical areas were represented in the larger\\\\nstudy that included 72 journal-published reviews, but only\\\\neight of those clinical areas had two or more new relevant\\\\nstudies in the smaller Cochrane and AHRQ cohorts. Fig. 2\\\\nshows recall of new studies by the three types of searches,\\\\nfor these eight clinical areas.\\\\n\\\\nCombined recall of the CQ Boolean search paired with a\\\\nranking search is shown in Fig. 3. The combination showed\\\\ncomplete recall of relevant new studies in three of eight\\\\nareas when CQ was paired with related articles and recall\\\\nof 0.67 or higher in all areas. Four of eight clinical areas\\\\nhad complete recall for the combination of CQ and SVM,\\\\nand recall was 0.80 or higher in all areas. Even for periph-\\\\neral vascular disease, where all three searches performed\\\\n\\\\nFig. 2. Recall of new studies by clinical area for each search method.\\\\nAbbreviations: SVM, support vector machine.\\\\n\\\\nfairly poorly individually, recall was 0.76 for the CQ/\\\\nrelated articles and 0.90 in the CQ/SVM pairing.\\\\n\\\\n3.3. Search precision\\\\n\\\\nPrecision, across the 10 AHRQ reviews, was 0.11 for the\\\\nCQ, 0.22 for related articles, and 0.19 for SVM (Table 3).\\\\n\\\\nFig. 1. Overlapping and unique retrieval of relevant studies by each\\\\nsearch method. The size of the circle is proportional to the size of\\\\nthe retrieval, but the overlap and unique portions are approximations.\\\\nExact ﬁgures for the components are labeled. Abbreviations: SVM,\\\\nsupport vector machine.\\\\n\\\\nFig. 3. Recall of new studies by clinical area for search methods in\\\\ncombination. Abbreviations: CQ, clinical query; SVM, support vector\\\\nmachine.\\\\n\\\\n\\\\x0cTable 3. Overall precision in the AHRQ Sample\\\\n\\\\n4. Discussion\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n113\\\\n\\\\nRetrieval method\\\\n\\\\nClinical query\\\\nRelated articles\\\\nSVM\\\\nClinical query þ related\\\\nClinical query þ SVM\\\\nClinical query þ related\\\\n\\\\narticles\\\\n\\\\nEligible\\\\nstudies\\\\nretrieved\\\\n\\\\n187\\\\n176\\\\n128\\\\n250\\\\n\\\\n243\\\\n276\\\\n\\\\nNo. of candidates\\\\n\\\\nretrieved\\\\n\\\\nPrecision\\\\n\\\\n1,637\\\\n814\\\\n659\\\\n2,318a\\\\n\\\\n2,247a\\\\n3,264a\\\\n\\\\n0.11\\\\n0.22\\\\n0.19\\\\n0.11\\\\n\\\\n0.11\\\\n0.08\\\\n\\\\narticles þ SVM\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and\\\\n\\\\nQuality; SVM, support vector machine.\\\\n\\\\na Number of candidates after removal of duplicate records.\\\\n\\\\nPrecision for the individual reviews ranged from 0.0 to 0.38\\\\nfor CQs, 0.004 to 0.80 for related articles, and 0.01 to 0.43\\\\nfor SVM. Overall precision was 0.11 for CQ and either\\\\nrelated articles or SVM. Overall precision was 0.08 when\\\\nCQ and both related articles and SVM were used. For\\\\nranked searches where a ﬁxed number of records will be\\\\nscreened, that number is always the denominator, so preci-\\\\nsion will tend to increase as the number of relevant records\\\\nincreasesdthe maximum retrieval size for SVM was cap-\\\\nped at 200 records.\\\\n\\\\n3.4. Retrieval consistency over time\\\\n\\\\nSearches were retested for the AHRQ cohort using\\\\nPubMed results obtained February 24, 2015. Recall of the\\\\nrelated articles searches across the 10 AHRQ reports was\\\\n0.64 originally and 0.50 when repeated (Fig. 4). Of the\\\\nthree extreme cases, one rose from 0.00 to 0.80, one re-\\\\nmained at 0.00, and one dropped from 1.00 to 0.60. CQ\\\\nhad overall\\\\nrecall of 0.68 originally and 0.70 when\\\\nrepeated. Changes in extreme cases were minimal. Overall\\\\ncombined recall of CQ and related article was 0.90 origi-\\\\nnally,\\\\nfalling to 0.85 when repeated. Considering the\\\\nextreme values, all review with combined recall of 1.00\\\\noriginally remained at 1.00, whereas the one review with\\\\n0.00 recall in combination showed recall of 0.80 when\\\\nrepeated.\\\\n\\\\n1.0\\\\n0.9\\\\n0.8\\\\n0.7\\\\n0.6\\\\n0.5\\\\n0.4\\\\n0.3\\\\n0.2\\\\n0.1\\\\n0.0\\\\n\\\\nOriginal\\\\n\\\\n2015\\\\n\\\\nClinical Query\\\\n\\\\nRelated Ar(cid:415)cles\\\\n\\\\nCombined\\\\n\\\\nFig. 4. Recall of new studies for search methods, alone and in combi-\\\\nnation originally (left) and when retested in 2015 (right).\\\\n\\\\nWe expected that\\\\n\\\\nthe sophisticated SVM approach\\\\nwould perform well when paired with more stripped-\\\\ndown, focused Boolean searches. Indeed, the two together\\\\nwere able to replace the multidatabase, multimodel\\\\nsearches used by the original review teams in updating\\\\nthe Cochrane searches. The new relevant studies for the Co-\\\\nchrane reviews may have been relatively easy to ﬁnd, but\\\\nthe pairing was also effective in the AHRQ set, and there\\\\nshowed better precision than is usually seen with traditional\\\\nsearches for systematic review of RCTs [9]. The AHRQ set\\\\nwas formed from the new studies identiﬁed for more com-\\\\nplex interventions in a rigorous, well-funded study [16].\\\\n\\\\nWe were surprised that the related articles approach per-\\\\nformed almost as well as SVM when used in combination\\\\nwith the CQ. The related article search has advantages over\\\\nSVMdit requires far less data preparation, and no special\\\\nsoftware is needed for its use. This makes it useful not only\\\\nin updating, but also, if appropriate seed articles can be\\\\nfound, in original reviews.\\\\n\\\\nThat both SVM and related articles sometimes showed\\\\npoor recall when used alone, but consistently good recall\\\\nwhen used with a Boolean method suggests that there is\\\\nreal beneﬁt in using complementary search methods to\\\\nquerying MEDLINE. The related article method is not very\\\\ntime consuming, and easily added to other planned search\\\\nefforts, while the third method used here, SVM, is more\\\\ntechnical. The combined performance of all three methods\\\\nwas surprising, but such a setup might become unpractical\\\\nin day-to-day use.\\\\n\\\\nOther investigators have similar ﬁndings. Examination\\\\nof the supplemental material presented in the Appendix C\\\\nto the article by Waffenschmidt et al. [18] reveals that\\\\nacross the 19 reviews tested, the combination of a simple\\\\nstructured Boolean search (SSBS) and the ﬁrst 50 retrieval\\\\nof PubMed similar articles showed complete retrieval of all\\\\nreference standard articles in 14 of 19 reviews and never\\\\nless than 0.90 recall (Appendix C at www.jclinepi.com pre-\\\\nsents data from the table by Waffenschmidt graphically).\\\\nWaffenschmidt concluded ‘‘the combination of these two\\\\nsearch techniques that are independent of each other seems\\\\nto compensate the respective weaknesses.’’\\\\n\\\\nSSBS search by Waffenschmidt was constructed in\\\\nPubMed from search terms selected for the indication and\\\\nintervention with PubMed’s narrow CQ ﬁlter (category:\\\\ntherapy). Their search using the similar articles feature (Re-\\\\nlCits) did not use a set of seed articles; rather, the RelCits\\\\nfunction was applied for each relevant citation previously\\\\nidentiﬁed in PubMed. Their test articles were the included\\\\nstudies in 19 systematic reviews of drugs.\\\\n\\\\nAgoritsas et al. also described search construction\\\\nmethods for searches based on CQ and PubMed similar ar-\\\\nticles, tested for their ability to retrieve the included studies\\\\nof 30 Cochrane reviews [17]. Although their search con-\\\\nstruction methods differed from those used here, both\\\\n\\\\n\\\\x0c114\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nsearches were derived using standard methods. Their\\\\napproach to the structured Boolean search used terms from\\\\nthe population, intervention, and comparison with the CQ,\\\\nlimited to humans and English. Two clinicians selected the\\\\nPubMed similar articles seeds from the initial PubMed\\\\nretrieval. They noted that no one method provided consis-\\\\ntently high retrieval [17].\\\\n\\\\nThus, the robust nature of this pairing of Boolean and\\\\nnon-Boolean searches has been shown in several contexts,\\\\ngiving support\\\\nto the hypothesis that methods such as\\\\nrelated articles and SVM can compensate for the variation\\\\ninherent\\\\nin selecting search terms or assigning subject\\\\nheadings during indexing.\\\\n\\\\nConsidering stability of the searches over time, the\\\\nminimal change seen in the performance of the CQ is\\\\nlikely explained by indexing changes of a few records.\\\\nThe computation appears not to have changed. This sug-\\\\ngests that indexing changes may also impact SVM results\\\\nover time; however, this is likely to result in improved\\\\nperformance, as was seen with the CQ. The computation\\\\nof similar articles is still described by National Library\\\\nof Medicine as being based on the algorithm described\\\\nby Kim et al. [26]. That algorithm would suggest changes\\\\nover time in the nearest neighbor score as the frequency of\\\\ncertain terms in the MEDLINE corpus changes. A full\\\\nexploration of the changes in nearest neighbor scores of\\\\nsimilar articles over time is beyond the scope of this\\\\nstudy. It should be noted that when the similar articles\\\\nfeature was initially studied, it was simple to submit seed\\\\narticles, and then add limits such as date or RCT publica-\\\\ntion type. On retesting, such additional limits were more\\\\ncomplex to apply, and the elink utility seemed to be the\\\\nmost practical approach to identifying the related articles\\\\n[27].\\\\n\\\\nThe beneﬁt of the complementary searches, relative to\\\\nBoolean searching alone, was greatest for AHRQ evidence\\\\nreports and less for the simpler Cochrane reviews, suggest-\\\\ning that this approach may be particularly useful for more\\\\ncomplex evidence. There are examples of its utility in the\\\\nliterature. For example, in a realist review of a multidisci-\\\\nplinary body of\\\\nliterature identifying six domains of\\\\nClinical Practice Guideline implementability, the use of\\\\nPubMed’s similar articles feature as a third stage of search-\\\\ning identiﬁed 131 records of which 104 were relevant [28].\\\\nIn a systematic review of evidence on the links between\\\\npatient experience and clinical safety and effectiveness,\\\\nthe authors used PubMed similar articles to snowball on\\\\narticles\\\\nidentiﬁed through an EMBASE search to\\\\novercome the limitations of predeﬁned searches for com-\\\\nplex evidence [29].\\\\n\\\\nThere is always the possibility that a search, or even a\\\\npair of searches, will fail. One method to detect such\\\\nfailures is to test whether the search strategies ﬁnd known\\\\nrelevant items [30]. In the updating case, this is easily done\\\\nusing the included studies of the original review as a test\\\\n\\\\nset, allowing the review to determine the MEDLINE\\\\ncoverage of their particular topic at the same time.\\\\n\\\\n4.1. Limitations\\\\n\\\\nThere are two limitations to our proposed strategy. Other\\\\ndatabases should be searched in the unusual event that\\\\nnumerous studies, representing more than a small propor-\\\\ntion of the total N, are not included in MEDLINE. Second,\\\\nwhen it is important to ﬁnd articles too new to be indexed\\\\nby MEDLINE, systematic reviewers may wish to conduct a\\\\nsimple PubMed search limited to the nonindexed subsets\\\\n[31,32].\\\\n\\\\n5. Conclusion\\\\n\\\\nThe general approach of a Boolean plus a ranking\\\\nsearch is effective in MEDLINE retrieval for systematic\\\\nreviews. Very high levels of identiﬁcation of relevant\\\\nMEDLINE records, with adequate precision, are possible\\\\nusing a focused Boolean search complemented by a docu-\\\\nment similarity or ranking method. The efﬁcacy of a\\\\nfocused Boolean search paired with a search using the\\\\nPubMed similar articles feature is in agreement with pre-\\\\nvious studies [17,18], and this study shows that this com-\\\\nplementary effect also occurs when SVM is used as the\\\\nadditional method. The beneﬁt of using two complemen-\\\\ntary approaches to achieving high recall in MEDLINE is\\\\na robust effect.\\\\n\\\\nPubMed-related articles is a parsimonious method, as it\\\\nis readily available to all review teams without cost. The\\\\napproach is robust across clinical domains, and the effect\\\\nhas now been demonstrated in several samples. The method\\\\nmay be sufﬁcient for updating systematic reviews of inter-\\\\nventions and may be used for new reviews of interventions\\\\nwhen paired with a trials registry search. It is likely to work\\\\nin any type of search where MEDLINE provides good sub-\\\\nject coverage even if retrieval through traditional search\\\\nmethods has been challenging.\\\\n\\\\nAcknowledgments\\\\n\\\\nMohamed Ansari and Jun Ji undertook screening and\\\\nrelevance assessment of candidate studies retrieved by the\\\\ntest\\\\nsearches, Tamara Rader developed the Boolean\\\\nsearches, and Raymond Daniel processed related articles\\\\nsearches and obtained full-text articles. David Moher cosu-\\\\npervised Margaret Sampson doctoral dissertation and pro-\\\\nvided invaluable advice and guidance.\\\\n\\\\nSupplementary data\\\\n\\\\nSupplementary data related to this article can be found at\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004.\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n115\\\\n\\\\nReferences\\\\n\\\\n[1] Karimi S, Pohl S, Scholer F, Cavedon L, Zobel J. Boolean versus\\\\nranked querying for biomedical systematic reviews. BMC Med\\\\nInform Decis Mak 2010;10(1):58.\\\\n\\\\n[2] Reveiz L, Ospina E, Zorrilla AFC. Should we consider Embase in\\\\nLatin America? J Clin Epidemiol 2004;57:866; author reply 867e8.\\\\n[3] Rathbone J, Carter M, Hoffmann T, Glasziou P. Better duplicate\\\\ndetection for systematic reviewers: evaluation of Systematic Review\\\\nAssistant-Deduplication Module. Syst Rev 2015;4(1):6.\\\\n\\\\n[4] Dickersin K, Scherer R, Lefebvre C. Identifying relevant studies for\\\\n\\\\nsystematic reviews. BMJ 1994;309:1286e91.\\\\n\\\\n[5] Bramer WM, Giustini D, Kramer BM, Anderson P. The comparative\\\\nrecall of Google Scholar versus PubMed in identical searches for\\\\nbiomedical systematic reviews: a review of searches used in system-\\\\natic reviews. Syst Rev 2013;2(1):115.\\\\n\\\\n[6] Wieland LS, Robinson KA, Dickersin K. Understanding why evi-\\\\ndence from randomised clinical trials may not be retrieved from Med-\\\\nline: comparison of indexed and non-indexed records. BMJ 2012;\\\\n344:d7501.\\\\n\\\\n[7] Suarez-Almazor ME, Belseck E, Homik J, Dorgan M, Ramos-\\\\nRemus C. Identifying clinical trials in the medical literature with\\\\nelectronic databases: MEDLINE alone is not enough. Control Clin\\\\nTrials 2000;21:476e87.\\\\n\\\\n[8] O’Leary N, Tiernan E, Walsh D, Lucey N, Kirkova J, Davis MP. The\\\\npitfalls of a systematic MEDLINE review in palliative medicine: symp-\\\\ntom assessment instruments. Am J Hosp Palliat Care 2007;24:181e4.\\\\n[9] Sampson M, Tetzlaff J, Urquhart C. Precision of healthcare system-\\\\natic review searches in a cross-sectional sample. Res Synth Methods\\\\n2011;2:119e25.\\\\n\\\\n[10] Greenhalgh T, Peacock R. Effectiveness and efﬁciency of search\\\\nmethods in systematic reviews of complex evidence: audit of primary\\\\nsources. BMJ 2005;331:1064e5.\\\\n\\\\n[11] Badgett RG, Dylla DP, Megison SD, Glynn Harmon E. An experimental\\\\nsearch strategy retrieves more precise results than PubMed and Google\\\\nfor questions about medical interventions. PeerJ 2015;3:e913.\\\\n\\\\n[12] O’Mara-Eves A, Thomas J, McNaught J, Miwa M, Ananiadou S.\\\\nUsing text mining for study identiﬁcation in systematic reviews: a\\\\nsystematic review of current approaches. Syst Rev 2015;4(1):5.\\\\n\\\\n[13] Bekhuis T, Demner-Fushman D. Towards automating the initial\\\\nscreening phase of a systematic review. Stud Health Technol Inform\\\\n[Internet] 2010;160(Pt 1):146e50. Available at http://www.ncbi.nlm.\\\\nnih.gov/pubmed/20841667. Accessed January 2014.\\\\n\\\\n[14] Wallace BC, Trikalinos TA, Lau J, Brodley C, Schmid CH. Semi-auto-\\\\nmated screening of biomedical citations for systematic reviews. BMC\\\\nBioinformatics 2010;11(1):55. Available at http://www.citeulike.org/\\\\nuser/daforerog/article/6595002. Accessed November 2010.\\\\n\\\\n[15] Sampson M, Shojania KG, McGowan J, Daniel R, Rader T,\\\\nIansavichene AE, et al. Surveillance search techniques identiﬁed the\\\\nneed to update systematic reviews. J Clin Epidemiol 2008;61:755e62.\\\\n[16] Shojania KG, Sampson M, Ansari MT, Ji J, Garritty C, Doucette S,\\\\net al. Updating systematic reviewsetechnical review 16. Health\\\\n(San Francisco). Rockville, MD: Agency for Health Care Policy\\\\nand Research; 2007.\\\\n\\\\n[17] Agoritsas T, Merglen A, Courvoisier DS, Combescure C, Garin N,\\\\nPerrier A, et al. Sensitivity and predictive value of 15 PubMed search\\\\n\\\\nstrategies to answer clinical questions rated against full systematic re-\\\\nviews. J Med Internet Res 2012;14(3):e85.\\\\n\\\\n[18] Waffenschmidt S, Janzen T, Hausner E, Kaiser T. Simple search tech-\\\\nniques in PubMed are potentially suitable for evaluating the\\\\ncompleteness of systematic reviews. J Clin Epidemiol 2013;66:\\\\n660e5.\\\\n\\\\n[19] Purpose and procedure [Internet]. ACP Journal Club. [cited 2007 Feb 7].\\\\nAvailable at http://annals.org/SS/ACPJC_Purpose_and_Procedure.aspx.\\\\n[20] Hedges Team. Search strategies for MEDLINE in Ovid Syntax and\\\\nthe PubMed translation. [Internet]. Health Information Research Unit\\\\nwebsite; 2010. Available at http://hiru.mcmaster.ca/hiru/HIRU_\\\\nHedges_MEDLINE_Strategies.aspx. Accessed January 2011.\\\\n\\\\n[21] Vapnik V. The nature of statistical\\\\n\\\\nlearning theory. New York:\\\\n\\\\nSpringer; 1995.\\\\n\\\\n[22] Joachims T. Text categorization with support vector machines:\\\\nlearning with many relevant features. Machine Learning: ECML-\\\\n98. Berlin: Springer; 1998:137e42.\\\\n\\\\n[23] Joachims T. Making large-scale SVM learning practical.\\\\n\\\\nIn:\\\\nSch€olkopf B, Burges CJC, Smola AJ, editors. Advances in kernel\\\\nmethods: support vector\\\\nlearning [Internet]. MIT Press; 1999.\\\\nAvailable at http://dl.acm.org/citation.cfm?id5299094. Accessed\\\\nSeptember 2015.\\\\n\\\\n[24] Glanville JM, Lefebvre C, Miles JN, Camosso-Steﬁnovic J. How to\\\\nidentify randomized controlled trials in MEDLINE: ten years on. J\\\\nMed Libr Assoc 2006;94:130e6.\\\\n\\\\n[25] Hulsen T, de Vlieg J, Alkema W. BioVenn - a web application for the\\\\nlists using area-\\\\n\\\\ncomparison and visualization of biological\\\\nproportional Venn diagrams. BMC Genomics 2008 Jan;9(1):488.\\\\n\\\\n[26] Kim W, Aronson AR, Wilbur WJ. Automatic MeSH term assignment\\\\n\\\\nand quality assessment. Proc AMIA Symp 2001;319e23.\\\\n\\\\n[27] Sayers E. The E-utilities in-depth: parameters, syntax and more\\\\n[Internet]. National Center for Biotechnology Information (US);\\\\n2015. Available at http://www.ncbi.nlm.nih.gov/books/NBK25499/.\\\\nAccessed March 2015.\\\\n\\\\n[28] Kastner M, Bhattacharyya O, Hayden L, Makarski J, Estey E,\\\\nDurocher L, et al. Guideline uptake is inﬂuenced by six implement-\\\\nability domains for creating and communicating guidelines: a realist\\\\nreview. J Clin Epidemiol 2015;68:498e509.\\\\n\\\\n[29] Doyle C, Lennox L, Bell D. A systematic review of evidence on the\\\\nlinks between patient experience and clinical safety and effective-\\\\nness. BMJ Open [Internet] 2013;3(1). Available at http://bmjopen.\\\\nbmj.com/content/3/1/e001570.abstract Accessed March 2015.\\\\n\\\\n[30] Sampson M, Mcgowan J. Inquisitio validus Index Medicus: a simple\\\\nmethod of validating MEDLINE systematic review searches. Res\\\\n[internet] 2011;2:103e9. Available at http://\\\\nSynth Methods\\\\nonlinelibrary.wiley.com/doi/10.1002/jrsm.40, Accessed September\\\\n28, 2015.\\\\n\\\\n[31] Duffy S, Misso K, Noake C, Ross J, Stirk L. Supplementary searches\\\\nof PubMed to improve currency of MEDLINE and MEDLINE\\\\nIn-Process searches via OvidSP. UK InterTASC Information Special-\\\\nists’ Sub-Group Workshop, July 9. University of Exeter [Internet];\\\\n2014. Available at http://www.systematic-reviews.com/. Accessed\\\\nAugust 2014.\\\\n\\\\n[32] Thompson JC, Quigley JM, Halfpenny NJA, Scott DA, Hawkins NS.\\\\nImportance and methods of searching for E-publications ahead of\\\\nprint in systematic reviews. Evid Based Med 2016;21:55e9.\\\\n\\\\n\\\\x0c'], ['The current issue and full text archive of this journal is available on Emerald Insight at:\\\\nwww.emeraldinsight.com/1463-7154.htm\\\\n\\\\nProcess mining through artificial\\\\n\\\\nneural networks and support\\\\n\\\\nANN and\\\\nsupport vector\\\\nmachines\\\\n\\\\n1391\\\\n\\\\nReceived 6 February 2015\\\\nRevised 20 April 2015\\\\n24 May 2015\\\\nAccepted 9 June 2015\\\\n\\\\nvector machines\\\\n\\\\nA systematic literature review\\\\n\\\\nAna Rocío Cárdenas Maita and Lucas Corrêa Martins\\\\nSchool of Arts, Sciences and Humanities, University of São Paulo,\\\\n\\\\nSão Paulo, Brazil\\\\n\\\\nCarlos Ramón López Paz\\\\n\\\\nFaculty of Computer Engineering,\\\\n\\\\nHigher Polytechnic Institute “José Antonio Echeverría”, Havana, Cuba, and\\\\n\\\\nSarajane Marques Peres and Marcelo Fantinato\\\\n\\\\nSchool of Arts, Sciences and Humanities, University of São Paulo,\\\\n\\\\nSão Paulo, Brazil\\\\n\\\\nAbstract\\\\nPurpose – Process mining is a research area used to discover, monitor and improve real business\\\\nprocesses by extracting knowledge from event logs available in process-aware information systems.\\\\nThe purpose of this paper is to evaluate the application of artificial neural networks (ANNs)\\\\nand support vector machines (SVMs) in data mining tasks in the process mining context. The goal was\\\\nto understand how these computational\\\\nintelligence techniques are currently being applied in\\\\nprocess mining.\\\\nDesign/methodology/approach – The authors conducted a systematic literature review with three\\\\nresearch questions formulated to evaluate the use of ANNs and SVMs in process mining.\\\\nFindings – The authors identified 11 papers as primary studies according to the criteria established in\\\\nthe review protocol. Most of them deal with process mining enhancement, mainly using ANNs.\\\\nRegarding the data mining task, the authors identified three types of tasks used: categorical prediction\\\\n(or classification); numeric prediction, considering the “regression” type, and clustering analysis.\\\\nOriginality/value – Although there is scientific interest in process mining, little attention has been\\\\nspecifically given to ANNs and SVM. This scenario does not reflect the general context of data mining,\\\\nwhere these two techniques are widely used. This low use may be possibly due to a relative lack of\\\\nknowledge about their potential for this type of problem, which the authors seek to reverse with the\\\\ncompletion of this study.\\\\nKeywords Process mining, Data mining, Artificial neural networks, Computational intelligence,\\\\nSupport-vector machines\\\\nPaper type Literature review\\\\n\\\\n1. Introduction\\\\nBusiness process management (BPM)\\\\nincludes methods, techniques and tools to\\\\nsupport the design, enactment, management and analysis of business processes,\\\\npreviously called business workflows, or simply workflows (van der Aalst et al., 2003).\\\\nThe BPM life cycle includes the following stages: business processes modeling;\\\\nimplementation of business process models; execution and supervision of business\\\\n\\\\nThis work was supported by Fapesp (São Paulo Research Foundation) and Capes (Coordination\\\\nfor the Improvement of Higher Education Personnel), Brazil.\\\\n\\\\nBusiness Process Management\\\\nJournal\\\\nVol. 21 No. 6, 2015\\\\npp. 1391-1415\\\\n© Emerald Group Publishing Limited\\\\n1463-7154\\\\nDOI 10.1108/BPMJ-02-2015-0017\\\\n\\\\n\\\\x0c', 'The current issue and full text archive of this journal is available on Emerald Insight at:\\\\nwww.emeraldinsight.com/1463-7154.htm\\\\n\\\\nProcess mining through artificial\\\\n\\\\nneural networks and support\\\\n\\\\nANN and\\\\nsupport vector\\\\nmachines\\\\n\\\\n1391\\\\n\\\\nReceived 6 February 2015\\\\nRevised 20 April 2015\\\\n24 May 2015\\\\nAccepted 9 June 2015\\\\n\\\\nvector machines\\\\n\\\\nA systematic literature review\\\\n\\\\nAna Rocío Cárdenas Maita and Lucas Corrêa Martins\\\\nSchool of Arts, Sciences and Humanities, University of São Paulo,\\\\n\\\\nSão Paulo, Brazil\\\\n\\\\nCarlos Ramón López Paz\\\\n\\\\nFaculty of Computer Engineering,\\\\n\\\\nHigher Polytechnic Institute “José Antonio Echeverría”, Havana, Cuba, and\\\\n\\\\nSarajane Marques Peres and Marcelo Fantinato\\\\n\\\\nSchool of Arts, Sciences and Humanities, University of São Paulo,\\\\n\\\\nSão Paulo, Brazil\\\\n\\\\nAbstract\\\\nPurpose – Process mining is a research area used to discover, monitor and improve real business\\\\nprocesses by extracting knowledge from event logs available in process-aware information systems.\\\\nThe purpose of this paper is to evaluate the application of artificial neural networks (ANNs)\\\\nand support vector machines (SVMs) in data mining tasks in the process mining context. The goal was\\\\nto understand how these computational\\\\nintelligence techniques are currently being applied in\\\\nprocess mining.\\\\nDesign/methodology/approach – The authors conducted a systematic literature review with three\\\\nresearch questions formulated to evaluate the use of ANNs and SVMs in process mining.\\\\nFindings – The authors identified 11 papers as primary studies according to the criteria established in\\\\nthe review protocol. Most of them deal with process mining enhancement, mainly using ANNs.\\\\nRegarding the data mining task, the authors identified three types of tasks used: categorical prediction\\\\n(or classification); numeric prediction, considering the “regression” type, and clustering analysis.\\\\nOriginality/value – Although there is scientific interest in process mining, little attention has been\\\\nspecifically given to ANNs and SVM. This scenario does not reflect the general context of data mining,\\\\nwhere these two techniques are widely used. This low use may be possibly due to a relative lack of\\\\nknowledge about their potential for this type of problem, which the authors seek to reverse with the\\\\ncompletion of this study.\\\\nKeywords Process mining, Data mining, Artificial neural networks, Computational intelligence,\\\\nSupport-vector machines\\\\nPaper type Literature review\\\\n\\\\n1. Introduction\\\\nBusiness process management (BPM)\\\\nincludes methods, techniques and tools to\\\\nsupport the design, enactment, management and analysis of business processes,\\\\npreviously called business workflows, or simply workflows (van der Aalst et al., 2003).\\\\nThe BPM life cycle includes the following stages: business processes modeling;\\\\nimplementation of business process models; execution and supervision of business\\\\n\\\\nThis work was supported by Fapesp (São Paulo Research Foundation) and Capes (Coordination\\\\nfor the Improvement of Higher Education Personnel), Brazil.\\\\n\\\\nBusiness Process Management\\\\nJournal\\\\nVol. 21 No. 6, 2015\\\\npp. 1391-1415\\\\n© Emerald Group Publishing Limited\\\\n1463-7154\\\\nDOI 10.1108/BPMJ-02-2015-0017\\\\n\\\\n\\\\x0cBPMJ\\\\n21,6\\\\n\\\\n1392\\\\n\\\\nprocess instances; monitoring and auditing of running business process instances; and\\\\nevaluation and improvement of business process models (Weske, 2012). In this last\\\\nstage, the execution and monitoring histories of the instances of a business process can\\\\nbe evaluated aiming its optimization.\\\\n\\\\nAccording to Han and Kamber (2006), data mining refers to extracting or mining\\\\nknowledge from lage amounts of data. Moreover, data mining can be seen as a stage in\\\\na larger process called Knowledge Discovery from Data (Fayyad et al., 1996). In this\\\\nlarger process, data from a specific context is collected, transformed and organized\\\\nbefore undergoing mining. After mining, the result should be organized in a structure\\\\nthat is accessible to direct human interpretation. There are different tasks to be\\\\naddressed in data mining, with different techniques that can be applied to each one.\\\\nAmong these techniques, those from the computational intelligence area are commonly\\\\nused (Wang and Fu, 2005), especially those using inductive reasoning, such as artificial\\\\nneural networks (ANN) and support vector machines (SVM), since they can be applied\\\\nin different data mining tasks (Stahl and Jordanov, 2012; Cortez, 2010). SVM, for\\\\nexample, was included among the top ten algorithms in data mining according to the\\\\nsixth IEEE International Conference on Data Mining held in 2006 (Wu et al., 2007). In\\\\naddition, both techniques are quite capable of conducting non-complex linear\\\\nmappings, common in data mining contexts, and therefore the potential to achieve\\\\nhighly accurate predictions. Finally, these models can extract knowledge, which is\\\\nuseful in decision-making environments (Cortez et al., 2009).\\\\n\\\\nThe combination of both areas – BPM and data mining – establishes a new field of\\\\nstudy, known as business process mining, or simply process mining (van der Aalst,\\\\n2011). It deals with applying data mining tasks on data from the BPM lifecycle. Its goal\\\\nis to extract knowledge about events/data from the work carried out in the different\\\\nphases of a business process, seeking to improve it, discovering associations between\\\\nvariables, behavior or misbehavior patterns (van der Aalst, 2011).\\\\n\\\\nIn this context, the main objective of this paper is to present the results of a study\\\\nthat identified and analyzed the primary studies related to process mining that\\\\nexclusively use ANN or SVM as the data mining technique. Our intention was to reach\\\\nan overview of this type of process mining approach and also to more carefully\\\\nexplore the use of two of the most important computational intelligence techniques.\\\\nWith this purpose, this paper presents the following sections: a theoretical background;\\\\nthe detailed SLR methodology; the results overview; the analysis of the selected\\\\nprimary studies; a discussion on the results; the validity threats analysis; and, finally,\\\\nthe conclusions.\\\\n\\\\n2. \""
      ]
     },
     "execution_count": 579,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "182001"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_intro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = str(data_conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"s: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\\\n\\\\n1. Introduction\\\\n\\\\nSystematic review searches need to have high recall.\\\\nMechanisms to achieve this usually include expansive\\\\nBoolean searches of multiple databases. This approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nMEDLINE gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. However,\\\\n\\\\nConﬂict of interest: None.\\\\nFunding: Some of this work was funded by the US Agency for Health-\\\\ncare Research and Quality, Department of Health and Human Services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* Corresponding author. Tel.: 613-737-7600.\\\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through MEDLINE. Recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in MEDLINE, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nSuccessful retrieval through a Boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. The search of\\\\nmultiple databases can therefore be helpful. The target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. A text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. These tactics on the part of the searcher lead\\\\n\\\\n\\\\x0c', 'Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nComplementary approaches to searching MEDLINE may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\\\n\\\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\\\n\\\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\\\n\\\\n0R6, Canada\\\\n\\\\nUnited Kingdom\\\\n\\\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\\\n\\\\nAccepted 7 March 2016; Published online 11 March 2016\\\\n\\\\nAbstract\\\\n\\\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\\\n\\\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\\\ncombined. Related articles showed least stability over time.\\\\n\\\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\\\n\\\\n1. Introduction\\\\n\\\\nSystematic review searches need to have high recall.\\\\nMechanisms to achieve this usually include expansive\\\\nBoolean searches of multiple databases. This approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nMEDLINE gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. However,\\\\n\\\\nConﬂict of interest: None.\\\\nFunding: Some of this work was funded by the US Agency for Health-\\\\ncare Research and Quality, Department of Health and Human Services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* Corresponding author. Tel.: 613-737-7600.\\\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through MEDLINE. Recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in MEDLINE, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nSuccessful retrieval through a Boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. The search of\\\\nmultiple databases can therefore be helpful. The target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. A text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. These tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nWhat is new?\\\\n\\\\nKey ﬁndings\\\\n(cid:1) Searches using known relevant studies and the\\\\nsimilar articles feature of PubMed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) For a given question, if the Boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. The two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) The precision of\\\\n\\\\nmethod appears better\\\\nexhaustive Boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nWhat this adds to what was known?\\\\n(cid:1) The paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nIt is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nWhat is the implication and what should change\\\\nnow?\\\\n(cid:1) Using the simple and universally available PubMed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) If the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nMEDLINE.\\\\n\\\\nto large retrievals with low precision [9]. These resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional Boolean searches have\\\\nbeen sought [10].\\\\n\\\\nMost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nThese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. These methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nWe examined one method, support vector machine\\\\n(SVM), and compared it with a simple and readily available\\\\nmethod based on the PubMed similar articles feature. We\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. We paired both\\\\nwith a focused Boolean search within MEDLINE. We\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for SVM and seed articles for the PubMed-\\\\n\\\\nrelated articles search. We therefore sought to determine\\\\nif a focused Boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\nComparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. In this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (CQ), and PubMed-related articles are tested in a\\\\ncohort of six updated Cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 Agency for Healthcare\\\\nResearch and Quality (AHRQ) evidence reports. The\\\\nCochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. The replication in the\\\\nAHRQ cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. All records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nOther research [17,18] suggests that searches using the\\\\nPubMed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a Boolean-type search of\\\\nMEDLINE. We tested an additional search method, SVM,\\\\nin the Cochrane and AHRQ samples to permit comparison\\\\nwith our PubMed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the PubMed\\\\nsimilar articles method.\\\\n\\\\nThe aim of this article was to test whether the combined\\\\napproach of a focused Boolean search paired with a second\\\\nsearch using the similar articles feature of PubMed or SVM\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. Methods\\\\n\\\\n2.1. Formation of the study cohorts\\\\n\\\\nThis analysis uses a data set created for an updating\\\\nstudy sponsored by AHRQ [16]. Methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\\\nsearch of the ACP Journal Club database (Ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nAHRQ reports were identiﬁed through the PubMed\\\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\\\njrid21544]. Screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. Screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c', 'Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nComplementary approaches to searching MEDLINE may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\\\n\\\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\\\n\\\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\\\n\\\\n0R6, Canada\\\\n\\\\nUnited Kingdom\\\\n\\\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\\\n\\\\nAccepted 7 March 2016; Published online 11 March 2016\\\\n\\\\nAbstract\\\\n\\\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\\\n\\\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\\\ncombined. Related articles showed least stability over time.\\\\n\\\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\\\n\\\\n1. Introduction\\\\n\\\\nSystematic review searches need to have high recall.\\\\nMechanisms to achieve this usually include expansive\\\\nBoolean searches of multiple databases. This approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nMEDLINE gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. However,\\\\n\\\\nConﬂict of interest: None.\\\\nFunding: Some of this work was funded by the US Agency for Health-\\\\ncare Research and Quality, Department of Health and Human Services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* Corresponding author. Tel.: 613-737-7600.\\\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through MEDLINE. Recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in MEDLINE, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nSuccessful retrieval through a Boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. The search of\\\\nmultiple databases can therefore be helpful. The target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. A text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. These tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nWhat is new?\\\\n\\\\nKey ﬁndings\\\\n(cid:1) Searches using known relevant studies and the\\\\nsimilar articles feature of PubMed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) For a given question, if the Boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. The two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) The precision of\\\\n\\\\nmethod appears better\\\\nexhaustive Boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nWhat this adds to what was known?\\\\n(cid:1) The paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nIt is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nWhat is the implication and what should change\\\\nnow?\\\\n(cid:1) Using the simple and universally available PubMed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) If the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nMEDLINE.\\\\n\\\\nto large retrievals with low precision [9]. These resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional Boolean searches have\\\\nbeen sought [10].\\\\n\\\\nMost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nThese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. These methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nWe examined one method, support vector machine\\\\n(SVM), and compared it with a simple and readily available\\\\nmethod based on the PubMed similar articles feature. We\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. We paired both\\\\nwith a focused Boolean search within MEDLINE. We\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for SVM and seed articles for the PubMed-\\\\n\\\\nrelated articles search. We therefore sought to determine\\\\nif a focused Boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\nComparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. In this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (CQ), and PubMed-related articles are tested in a\\\\ncohort of six updated Cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 Agency for Healthcare\\\\nResearch and Quality (AHRQ) evidence reports. The\\\\nCochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. The replication in the\\\\nAHRQ cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. All records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nOther research [17,18] suggests that searches using the\\\\nPubMed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a Boolean-type search of\\\\nMEDLINE. We tested an additional search method, SVM,\\\\nin the Cochrane and AHRQ samples to permit comparison\\\\nwith our PubMed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the PubMed\\\\nsimilar articles method.\\\\n\\\\nThe aim of this article was to test whether the combined\\\\napproach of a focused Boolean search paired with a second\\\\nsearch using the similar articles feature of PubMed or SVM\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. Methods\\\\n\\\\n2.1. Formation of the study cohorts\\\\n\\\\nThis analysis uses a data set created for an updating\\\\nstudy sponsored by AHRQ [16]. Methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\\\nsearch of the ACP Journal Club database (Ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nAHRQ reports were identiﬁed through the PubMed\\\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\\\njrid21544]. Screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. Screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c110\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nFor inclusion, all reviews must have included random-\\\\nized controlled trials (RCTs) and provided meta-analysis\\\\nfor at least one outcome. The MEDLINE search strategies\\\\nhad to be reported in enough detail to permit replication.\\\\nFifteen AHRQ evidence reports thought\\\\nlikely to have\\\\nimportant new evidence were selected and used to validate\\\\nthe updating methods used in the main cohort.\\\\n\\\\nCochrane reviews were selected to meet minimum qual-\\\\nity and relevance standards, as deﬁned by ACP Journal\\\\nClub [19]. We also required that the review had been up-\\\\ndated and that the text of both the original review and an\\\\nupdated version was available. The search for the original\\\\nreview must have included MEDLINE and at least one\\\\nother electronic bibliographic database and one or more\\\\nnondatabase method such as hand searching or checking\\\\nreference lists. Such comprehensive searching was likely\\\\nto identify most relevant\\\\nliterature and form a useful\\\\ntraining set of included examples. At least 10 RCTs or\\\\nquasi-RCTs must have been included in the original review\\\\nto give an adequate training set for SVM.\\\\n\\\\n2.2. Test searches\\\\n\\\\n2.2.1. Clinical query\\\\n\\\\nBoolean searches were developed by a librarian experi-\\\\nenced in systematic review searches using a protocol\\\\n(Appendix A at www.jclinepi.com). Search strategies were\\\\ndeliberately simple, usually consisting of two or three\\\\nMedical Subject Heading (MeSH) terms representing the\\\\npopulation and intervention of the review. This search\\\\nwas limited by a CQ ﬁlter to ‘‘therapy (best balance of\\\\nsensitivity\\\\n‘‘Randomized\\\\ncontrolled trial.pt. or\\\\nrandomized.mp. or placebo.mp.’’\\\\n[20]. Boolean searches were run in OVID MEDLINE.\\\\n\\\\nspeciﬁcity)’’\\\\n\\\\nthat\\\\n\\\\nand\\\\n\\\\nis,\\\\n\\\\n2.2.2. Related articles\\\\n\\\\nSearches using the PubMed similar articles feature were\\\\nperformed using the PubMed unique identiﬁers (PMID) of\\\\nthe three newest and three largest studies included in the\\\\noriginal review as seed articles. There was no replacement\\\\nin the event of overlap between the largest and newest sets,\\\\nor if one of those studies was not indexed in MEDLINE.\\\\n(All studies included in the original reviews were checked\\\\nfor indexing status in MEDLINE.) The resulting set was\\\\nlimited to the publication type RCT and date limited to\\\\nthe period since the search date of the original review.\\\\n\\\\n2.2.3. Support vector machine\\\\n\\\\nThe SVM searches used for this project have not been\\\\npreviously described so the methods will be reported in\\\\nmore detail. SVM is a well established and broadly applied\\\\nclassiﬁcation method from machine learning [21,22]. A\\\\nclassiﬁcation task is closely related to an information\\\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\\\nthe collection. As SVM is a supervised algorithm,\\\\nit\\\\n\\\\nrequires training examples with known labels. It then places\\\\nthese training examples in a high-dimensional vector space\\\\nin such a way that examples of one class are separated from\\\\nexamples of the other class with the greatest distance to the\\\\nseparating boundary. Predicting the class of a new instance\\\\nhappens by placing it in the same space and determining on\\\\nwhich side of the boundary it falls. The distance to the\\\\nboundary is an indication of the prediction conﬁdence and\\\\nis used to rank instances (documents) as is essential in in-\\\\nformation retrieval tasks or to provide a cutoff threshold.\\\\n\\\\nIn our setup, the training set consisted of search results\\\\nfrom the original review formed with included instances\\\\nbeing the studies included in the review (the relevant re-\\\\ntrievals) and excluded instances being the studies found\\\\nby the search used in the original review but excluded from\\\\nthe review (irrelevant retrievals). The new examples to be\\\\nclassiﬁed were articles indexed since the date of the search\\\\nperformed in the original review.\\\\n\\\\nSVM searches were run on MEDLINE data stored\\\\nlocally at\\\\nthe National Research Council of Canada.\\\\nMEDLINE was refreshed before running the searches.\\\\nThe MEDLINE records were represented as bags of fea-\\\\ntures, where features consisted of lowercased words and\\\\nword combinations (up to four words) from the title and ab-\\\\nstract ﬁelds, full MeSH terms, and contents of all other\\\\nﬁelds in the MEDLINE record. Features were uniformly\\\\nweighted. The SVM implementation used was SVM Light\\\\n[23] used with a linear kernel.\\\\n\\\\nThe approach was piloted with one AHRQ evidence\\\\nreport, and various conﬁgurations were tried, observing\\\\nthe placement of several known new relevant studies within\\\\nthe retrieval. The MEDLINE set was ﬁltered with the\\\\nrevised Highly Sensitive Search Strategy (HSSS) [24]. Pre-\\\\nliminary testing indicated that this ﬁlter would retrieve\\\\n99.1% of MEDLINE-indexed relevant new evidence from\\\\nthe larger cohort of journal-published systematic reviews\\\\n(n 5 72) and Cochrane reviews (n 5 27). This HSSS ﬁlter\\\\nwas added primarily to improve processing speed, relative\\\\nto running the classiﬁer against all of MEDLINE. With\\\\nthe ﬁlter, processing for one systematic review took about\\\\n10 minutes. Next, a coarse-grained ﬁlter was applied;\\\\nincluded and excluded instances were combined in a single\\\\nset labeled POS. Adjacent PubMed IDs (the next higher\\\\nPMID to each member of POS, as long as that higher num-\\\\nber was not itself a member of POS) were combined in a set\\\\nto represent WORLD. The model was trained on POS vs.\\\\nWORLD, using words and phrases from title, abstract,\\\\nauthor names, and journal name. This model was run\\\\nagainst the records passing the HSSS ﬁlter.\\\\n\\\\nFour different subsets of\\\\n\\\\nthe SVM retrieval were\\\\nexamineddone set included all records with a relevance\\\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\\\n(SVM200point5). Other sets consisted of all records with\\\\nrelevance scores of 0.95 or more (SVM95), 0.90 or more\\\\n(SVM90), and 0.80 or more (SVM80). PubMed ID and\\\\nrelevance scores were recorded, and PubMed IDs were\\\\n\\\\n\\\\x0c', 'Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nComplementary approaches to searching MEDLINE may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\\\n\\\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\\\n\\\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\\\n\\\\n0R6, Canada\\\\n\\\\nUnited Kingdom\\\\n\\\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\\\n\\\\nAccepted 7 March 2016; Published online 11 March 2016\\\\n\\\\nAbstract\\\\n\\\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\\\n\\\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\\\ncombined. Related articles showed least stability over time.\\\\n\\\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\\\n\\\\n1. Introduction\\\\n\\\\nSystematic review searches need to have high recall.\\\\nMechanisms to achieve this usually include expansive\\\\nBoolean searches of multiple databases. This approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nMEDLINE gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. However,\\\\n\\\\nConﬂict of interest: None.\\\\nFunding: Some of this work was funded by the US Agency for Health-\\\\ncare Research and Quality, Department of Health and Human Services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* Corresponding author. Tel.: 613-737-7600.\\\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through MEDLINE. Recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in MEDLINE, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nSuccessful retrieval through a Boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. The search of\\\\nmultiple databases can therefore be helpful. The target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. A text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. These tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nWhat is new?\\\\n\\\\nKey ﬁndings\\\\n(cid:1) Searches using known relevant studies and the\\\\nsimilar articles feature of PubMed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) For a given question, if the Boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. The two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) The precision of\\\\n\\\\nmethod appears better\\\\nexhaustive Boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nWhat this adds to what was known?\\\\n(cid:1) The paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nIt is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nWhat is the implication and what should change\\\\nnow?\\\\n(cid:1) Using the simple and universally available PubMed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) If the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nMEDLINE.\\\\n\\\\nto large retrievals with low precision [9]. These resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional Boolean searches have\\\\nbeen sought [10].\\\\n\\\\nMost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nThese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. These methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nWe examined one method, support vector machine\\\\n(SVM), and compared it with a simple and readily available\\\\nmethod based on the PubMed similar articles feature. We\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. We paired both\\\\nwith a focused Boolean search within MEDLINE. We\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for SVM and seed articles for the PubMed-\\\\n\\\\nrelated articles search. We therefore sought to determine\\\\nif a focused Boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\nComparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. In this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (CQ), and PubMed-related articles are tested in a\\\\ncohort of six updated Cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 Agency for Healthcare\\\\nResearch and Quality (AHRQ) evidence reports. The\\\\nCochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. The replication in the\\\\nAHRQ cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. All records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nOther research [17,18] suggests that searches using the\\\\nPubMed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a Boolean-type search of\\\\nMEDLINE. We tested an additional search method, SVM,\\\\nin the Cochrane and AHRQ samples to permit comparison\\\\nwith our PubMed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the PubMed\\\\nsimilar articles method.\\\\n\\\\nThe aim of this article was to test whether the combined\\\\napproach of a focused Boolean search paired with a second\\\\nsearch using the similar articles feature of PubMed or SVM\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. Methods\\\\n\\\\n2.1. Formation of the study cohorts\\\\n\\\\nThis analysis uses a data set created for an updating\\\\nstudy sponsored by AHRQ [16]. Methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\\\nsearch of the ACP Journal Club database (Ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nAHRQ reports were identiﬁed through the PubMed\\\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\\\njrid21544]. Screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. Screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c110\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nFor inclusion, all reviews must have included random-\\\\nized controlled trials (RCTs) and provided meta-analysis\\\\nfor at least one outcome. The MEDLINE search strategies\\\\nhad to be reported in enough detail to permit replication.\\\\nFifteen AHRQ evidence reports thought\\\\nlikely to have\\\\nimportant new evidence were selected and used to validate\\\\nthe updating methods used in the main cohort.\\\\n\\\\nCochrane reviews were selected to meet minimum qual-\\\\nity and relevance standards, as deﬁned by ACP Journal\\\\nClub [19]. We also required that the review had been up-\\\\ndated and that the text of both the original review and an\\\\nupdated version was available. The search for the original\\\\nreview must have included MEDLINE and at least one\\\\nother electronic bibliographic database and one or more\\\\nnondatabase method such as hand searching or checking\\\\nreference lists. Such comprehensive searching was likely\\\\nto identify most relevant\\\\nliterature and form a useful\\\\ntraining set of included examples. At least 10 RCTs or\\\\nquasi-RCTs must have been included in the original review\\\\nto give an adequate training set for SVM.\\\\n\\\\n2.2. Test searches\\\\n\\\\n2.2.1. Clinical query\\\\n\\\\nBoolean searches were developed by a librarian experi-\\\\nenced in systematic review searches using a protocol\\\\n(Appendix A at www.jclinepi.com). Search strategies were\\\\ndeliberately simple, usually consisting of two or three\\\\nMedical Subject Heading (MeSH) terms representing the\\\\npopulation and intervention of the review. This search\\\\nwas limited by a CQ ﬁlter to ‘‘therapy (best balance of\\\\nsensitivity\\\\n‘‘Randomized\\\\ncontrolled trial.pt. or\\\\nrandomized.mp. or placebo.mp.’’\\\\n[20]. Boolean searches were run in OVID MEDLINE.\\\\n\\\\nspeciﬁcity)’’\\\\n\\\\nthat\\\\n\\\\nand\\\\n\\\\nis,\\\\n\\\\n2.2.2. Related articles\\\\n\\\\nSearches using the PubMed similar articles feature were\\\\nperformed using the PubMed unique identiﬁers (PMID) of\\\\nthe three newest and three largest studies included in the\\\\noriginal review as seed articles. There was no replacement\\\\nin the event of overlap between the largest and newest sets,\\\\nor if one of those studies was not indexed in MEDLINE.\\\\n(All studies included in the original reviews were checked\\\\nfor indexing status in MEDLINE.) The resulting set was\\\\nlimited to the publication type RCT and date limited to\\\\nthe period since the search date of the original review.\\\\n\\\\n2.2.3. Support vector machine\\\\n\\\\nThe SVM searches used for this project have not been\\\\npreviously described so the methods will be reported in\\\\nmore detail. SVM is a well established and broadly applied\\\\nclassiﬁcation method from machine learning [21,22]. A\\\\nclassiﬁcation task is closely related to an information\\\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\\\nthe collection. As SVM is a supervised algorithm,\\\\nit\\\\n\\\\nrequires training examples with known labels. It then places\\\\nthese training examples in a high-dimensional vector space\\\\nin such a way that examples of one class are separated from\\\\nexamples of the other class with the greatest distance to the\\\\nseparating boundary. Predicting the class of a new instance\\\\nhappens by placing it in the same space and determining on\\\\nwhich side of the boundary it falls. The distance to the\\\\nboundary is an indication of the prediction conﬁdence and\\\\nis used to rank instances (documents) as is essential in in-\\\\nformation retrieval tasks or to provide a cutoff threshold.\\\\n\\\\nIn our setup, the training set consisted of search results\\\\nfrom the original review formed with included instances\\\\nbeing the studies included in the review (the relevant re-\\\\ntrievals) and excluded instances being the studies found\\\\nby the search used in the original review but excluded from\\\\nthe review (irrelevant retrievals). The new examples to be\\\\nclassiﬁed were articles indexed since the date of the search\\\\nperformed in the original review.\\\\n\\\\nSVM searches were run on MEDLINE data stored\\\\nlocally at\\\\nthe National Research Council of Canada.\\\\nMEDLINE was refreshed before running the searches.\\\\nThe MEDLINE records were represented as bags of fea-\\\\ntures, where features consisted of lowercased words and\\\\nword combinations (up to four words) from the title and ab-\\\\nstract ﬁelds, full MeSH terms, and contents of all other\\\\nﬁelds in the MEDLINE record. Features were uniformly\\\\nweighted. The SVM implementation used was SVM Light\\\\n[23] used with a linear kernel.\\\\n\\\\nThe approach was piloted with one AHRQ evidence\\\\nreport, and various conﬁgurations were tried, observing\\\\nthe placement of several known new relevant studies within\\\\nthe retrieval. The MEDLINE set was ﬁltered with the\\\\nrevised Highly Sensitive Search Strategy (HSSS) [24]. Pre-\\\\nliminary testing indicated that this ﬁlter would retrieve\\\\n99.1% of MEDLINE-indexed relevant new evidence from\\\\nthe larger cohort of journal-published systematic reviews\\\\n(n 5 72) and Cochrane reviews (n 5 27). This HSSS ﬁlter\\\\nwas added primarily to improve processing speed, relative\\\\nto running the classiﬁer against all of MEDLINE. With\\\\nthe ﬁlter, processing for one systematic review took about\\\\n10 minutes. Next, a coarse-grained ﬁlter was applied;\\\\nincluded and excluded instances were combined in a single\\\\nset labeled POS. Adjacent PubMed IDs (the next higher\\\\nPMID to each member of POS, as long as that higher num-\\\\nber was not itself a member of POS) were combined in a set\\\\nto represent WORLD. The model was trained on POS vs.\\\\nWORLD, using words and phrases from title, abstract,\\\\nauthor names, and journal name. This model was run\\\\nagainst the records passing the HSSS ﬁlter.\\\\n\\\\nFour different subsets of\\\\n\\\\nthe SVM retrieval were\\\\nexamineddone set included all records with a relevance\\\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\\\n(SVM200point5). Other sets consisted of all records with\\\\nrelevance scores of 0.95 or more (SVM95), 0.90 or more\\\\n(SVM90), and 0.80 or more (SVM80). PubMed ID and\\\\nrelevance scores were recorded, and PubMed IDs were\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n111\\\\n\\\\nTable 1. Characteristics of the included reviews\\\\n\\\\nCochrane\\\\nreviews; N\\\\n\\\\nAHRQ evidence\\\\n\\\\nreports; N\\\\n\\\\nincorporated into the database of records for the reviewers\\\\nto screen. Only the results from SVM200point5 are re-\\\\nported here and are described as SVM.\\\\n\\\\n2.3. Determining performance of the test searches\\\\n\\\\nFor the updated Cochrane reviews, reference standard ar-\\\\nticles were those studies included in the updated review that\\\\nwere not in the original and which entered MEDLINE after\\\\nthe search date of the original review. For the AHRQ evi-\\\\ndence reports, the full retrieval set was screened. Records\\\\nfound relevant by the consensus of reviewers were consid-\\\\nered reference standard articles. Recall was the proportion\\\\nof reference standard articles identiﬁed by the search:\\\\n\\\\nNumber of reference standard articles found\\\\nTotal number of reference standard articles\\\\n\\\\nBioVenn software was used to analyze the overlap be-\\\\ntween the retrieval of relevant articles from the three\\\\nsearches and to create Venn diagrams [25].\\\\n\\\\nAll included reviews were classiﬁed into clinical area\\\\nbased on factors such as ISI journal classiﬁcation, the Co-\\\\nchrane Collaboration Review Group where the topic might\\\\nbe placed and the high level MeSH term under which the pop-\\\\nulation (condition) would be indexed. Performance of the\\\\nsearches in different clinical areas was displayed graphically\\\\nfor the searches both alone and in combinations, to allow ex-\\\\namination of differences in parallelism, level, and ﬂatness.\\\\n\\\\nPrecision is the proportion of all retrieved records that\\\\n\\\\nare relevant:\\\\n\\\\nNumber of reference standard articles found\\\\n\\\\nTotal number of records retrieved\\\\n\\\\nThe inverse of precision is the number needed to read to\\\\nﬁnd one eligible study, thus precision inﬂuences the work\\\\nof the review. Precision was calculated only for the AHRQ\\\\nevidence reviews. Because not all candidates retrieved by\\\\nthe Cochrane searches were assessed, precision could not\\\\nbe established in that sample.\\\\n\\\\n2.4. Determining stability of results over time\\\\n\\\\nThe CQ and related article searches, originally run in\\\\nMarch 2008, were repeated in February 2015 in the AHRQ\\\\ncohort. Recall of relevant items was compared with the\\\\noriginal retrievals to determine if any changes introduced\\\\nby National Library of Medicine might invalidate ﬁndings.\\\\nAs SVM conﬁguration is under the control of the investiga-\\\\ntors, it was not retested.\\\\n\\\\n3. Results\\\\n\\\\nSix Cochrane reviews and 10 AHRQ evidence reports\\\\nmet all inclusion criteria (Appendix B at www.jclinepi.\\\\ncom). Characteristics of the included reviews are shown\\\\nin Table 1.\\\\n\\\\n9a\\\\n1\\\\n3\\\\n\\\\n3\\\\n\\\\nd\\\\n1\\\\n\\\\nd\\\\n1\\\\n2\\\\n1\\\\n1\\\\n\\\\n1\\\\nd\\\\nd\\\\n\\\\nd\\\\n3\\\\n3\\\\n4\\\\n\\\\nCharacteristic\\\\n\\\\nTherapy evaluated\\\\n\\\\nMedications\\\\nMedical devices\\\\nProcedures\\\\n\\\\nClinical topic area\\\\n\\\\nCardiac and cardiovascular\\\\n\\\\nsystems\\\\n\\\\nCritical care\\\\nEndocrinology and\\\\n\\\\nmetabolism\\\\n\\\\nInfectious disease\\\\nClinical neurology\\\\nObstetrics and gynecology\\\\nOncology\\\\nPeripheral vascular\\\\n\\\\ndiseases\\\\nPsychiatry\\\\nRespiratory systems\\\\nUrology and nephrology\\\\n\\\\nPublication period\\\\n\\\\nMarch 1997eApril 1999\\\\nMay 1999eJune 2001\\\\nJuly 2001eAugust 2003\\\\nSeptember 2003e\\\\nDecember 2005\\\\n\\\\nMedian included trials\\\\n\\\\nMedian included\\\\n\\\\nparticipants\\\\n\\\\nMEDLINE coverage of\\\\n\\\\n5\\\\n1\\\\nd\\\\n\\\\nd\\\\n\\\\n1\\\\nd\\\\n\\\\n3\\\\nd\\\\nd\\\\nd\\\\nd\\\\n\\\\nd\\\\n1\\\\n1\\\\n\\\\n3\\\\n1\\\\n2\\\\nd\\\\n\\\\n17 (IQR, 14e20)\\\\n\\\\n96 (IQR,\\\\n\\\\n31.75e121.5)\\\\n8,679 (IQR,\\\\n22,830 (IQR, 14,\\\\n4,085e50,109)\\\\n172e49,687)\\\\n99/107 (92.5%) 969/980 (98.8%)\\\\n\\\\nincluded articles\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\\\n\\\\nity; IQR, interquartile range.\\\\n\\\\na Some AHRQ reviews included more than one class of therapy.\\\\n\\\\nRecall of new relevant studies is shown in Table 2. The\\\\nupdated Cochrane reviews had 20 new studies identiﬁed by\\\\nthe review authors and added in the updates. Recall of these\\\\n20 ranged from 1.00 for the related articles method to a low\\\\nof 0.80 CQs (Table 2). Our team identiﬁed 277 new studies\\\\nas relevant for inclusion in the 10 evidence reports that\\\\nwere updated. All test searches showed lower recall for this\\\\ncohort than for the Cochrane reviews and here the CQ out-\\\\nperformed both ranking searches (Table 2).\\\\n\\\\n3.1. Recall of CQ combined with a ranking method\\\\n\\\\nOf the 297 new relevant studies identiﬁed, the combina-\\\\ntion of CQ and related articles searches identiﬁed 270\\\\n(overall\\\\nin\\\\nthe Cochrane cohort and 0.90 (250/277) in the AHRQ\\\\ncohort.\\\\n\\\\nrecall of 0.91). Recall was 1.00 (20/20)\\\\n\\\\nThe combination of CQ and SVM identiﬁed 263 studies\\\\n(overall recall of 0.89). Recall was 1.00 (20/20) in the\\\\nCochrane cohort and 0.88 (243/277) in the AHRQ cohort.\\\\nWhen all three methods were used together, recall was\\\\n0.997 with 296 of the 297 studies identiﬁed.\\\\n\\\\n\\\\x0c', 'Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nComplementary approaches to searching MEDLINE may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\\\n\\\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\\\n\\\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\\\n\\\\n0R6, Canada\\\\n\\\\nUnited Kingdom\\\\n\\\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\\\n\\\\nAccepted 7 March 2016; Published online 11 March 2016\\\\n\\\\nAbstract\\\\n\\\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\\\n\\\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\\\ncombined. Related articles showed least stability over time.\\\\n\\\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\\\n\\\\n1. Introduction\\\\n\\\\nSystematic review searches need to have high recall.\\\\nMechanisms to achieve this usually include expansive\\\\nBoolean searches of multiple databases. This approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nMEDLINE gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. However,\\\\n\\\\nConﬂict of interest: None.\\\\nFunding: Some of this work was funded by the US Agency for Health-\\\\ncare Research and Quality, Department of Health and Human Services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* Corresponding author. Tel.: 613-737-7600.\\\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through MEDLINE. Recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in MEDLINE, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nSuccessful retrieval through a Boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. The search of\\\\nmultiple databases can therefore be helpful. The target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. A text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. These tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nWhat is new?\\\\n\\\\nKey ﬁndings\\\\n(cid:1) Searches using known relevant studies and the\\\\nsimilar articles feature of PubMed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) For a given question, if the Boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. The two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) The precision of\\\\n\\\\nmethod appears better\\\\nexhaustive Boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nWhat this adds to what was known?\\\\n(cid:1) The paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nIt is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nWhat is the implication and what should change\\\\nnow?\\\\n(cid:1) Using the simple and universally available PubMed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) If the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nMEDLINE.\\\\n\\\\nto large retrievals with low precision [9]. These resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional Boolean searches have\\\\nbeen sought [10].\\\\n\\\\nMost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nThese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. These methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nWe examined one method, support vector machine\\\\n(SVM), and compared it with a simple and readily available\\\\nmethod based on the PubMed similar articles feature. We\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. We paired both\\\\nwith a focused Boolean search within MEDLINE. We\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for SVM and seed articles for the PubMed-\\\\n\\\\nrelated articles search. We therefore sought to determine\\\\nif a focused Boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\nComparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. In this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (CQ), and PubMed-related articles are tested in a\\\\ncohort of six updated Cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 Agency for Healthcare\\\\nResearch and Quality (AHRQ) evidence reports. The\\\\nCochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. The replication in the\\\\nAHRQ cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. All records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nOther research [17,18] suggests that searches using the\\\\nPubMed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a Boolean-type search of\\\\nMEDLINE. We tested an additional search method, SVM,\\\\nin the Cochrane and AHRQ samples to permit comparison\\\\nwith our PubMed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the PubMed\\\\nsimilar articles method.\\\\n\\\\nThe aim of this article was to test whether the combined\\\\napproach of a focused Boolean search paired with a second\\\\nsearch using the similar articles feature of PubMed or SVM\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. Methods\\\\n\\\\n2.1. Formation of the study cohorts\\\\n\\\\nThis analysis uses a data set created for an updating\\\\nstudy sponsored by AHRQ [16]. Methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\\\nsearch of the ACP Journal Club database (Ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nAHRQ reports were identiﬁed through the PubMed\\\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\\\njrid21544]. Screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. Screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c110\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nFor inclusion, all reviews must have included random-\\\\nized controlled trials (RCTs) and provided meta-analysis\\\\nfor at least one outcome. The MEDLINE search strategies\\\\nhad to be reported in enough detail to permit replication.\\\\nFifteen AHRQ evidence reports thought\\\\nlikely to have\\\\nimportant new evidence were selected and used to validate\\\\nthe updating methods used in the main cohort.\\\\n\\\\nCochrane reviews were selected to meet minimum qual-\\\\nity and relevance standards, as deﬁned by ACP Journal\\\\nClub [19]. We also required that the review had been up-\\\\ndated and that the text of both the original review and an\\\\nupdated version was available. The search for the original\\\\nreview must have included MEDLINE and at least one\\\\nother electronic bibliographic database and one or more\\\\nnondatabase method such as hand searching or checking\\\\nreference lists. Such comprehensive searching was likely\\\\nto identify most relevant\\\\nliterature and form a useful\\\\ntraining set of included examples. At least 10 RCTs or\\\\nquasi-RCTs must have been included in the original review\\\\nto give an adequate training set for SVM.\\\\n\\\\n2.2. Test searches\\\\n\\\\n2.2.1. Clinical query\\\\n\\\\nBoolean searches were developed by a librarian experi-\\\\nenced in systematic review searches using a protocol\\\\n(Appendix A at www.jclinepi.com). Search strategies were\\\\ndeliberately simple, usually consisting of two or three\\\\nMedical Subject Heading (MeSH) terms representing the\\\\npopulation and intervention of the review. This search\\\\nwas limited by a CQ ﬁlter to ‘‘therapy (best balance of\\\\nsensitivity\\\\n‘‘Randomized\\\\ncontrolled trial.pt. or\\\\nrandomized.mp. or placebo.mp.’’\\\\n[20]. Boolean searches were run in OVID MEDLINE.\\\\n\\\\nspeciﬁcity)’’\\\\n\\\\nthat\\\\n\\\\nand\\\\n\\\\nis,\\\\n\\\\n2.2.2. Related articles\\\\n\\\\nSearches using the PubMed similar articles feature were\\\\nperformed using the PubMed unique identiﬁers (PMID) of\\\\nthe three newest and three largest studies included in the\\\\noriginal review as seed articles. There was no replacement\\\\nin the event of overlap between the largest and newest sets,\\\\nor if one of those studies was not indexed in MEDLINE.\\\\n(All studies included in the original reviews were checked\\\\nfor indexing status in MEDLINE.) The resulting set was\\\\nlimited to the publication type RCT and date limited to\\\\nthe period since the search date of the original review.\\\\n\\\\n2.2.3. Support vector machine\\\\n\\\\nThe SVM searches used for this project have not been\\\\npreviously described so the methods will be reported in\\\\nmore detail. SVM is a well established and broadly applied\\\\nclassiﬁcation method from machine learning [21,22]. A\\\\nclassiﬁcation task is closely related to an information\\\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\\\nthe collection. As SVM is a supervised algorithm,\\\\nit\\\\n\\\\nrequires training examples with known labels. It then places\\\\nthese training examples in a high-dimensional vector space\\\\nin such a way that examples of one class are separated from\\\\nexamples of the other class with the greatest distance to the\\\\nseparating boundary. Predicting the class of a new instance\\\\nhappens by placing it in the same space and determining on\\\\nwhich side of the boundary it falls. The distance to the\\\\nboundary is an indication of the prediction conﬁdence and\\\\nis used to rank instances (documents) as is essential in in-\\\\nformation retrieval tasks or to provide a cutoff threshold.\\\\n\\\\nIn our setup, the training set consisted of search results\\\\nfrom the original review formed with included instances\\\\nbeing the studies included in the review (the relevant re-\\\\ntrievals) and excluded instances being the studies found\\\\nby the search used in the original review but excluded from\\\\nthe review (irrelevant retrievals). The new examples to be\\\\nclassiﬁed were articles indexed since the date of the search\\\\nperformed in the original review.\\\\n\\\\nSVM searches were run on MEDLINE data stored\\\\nlocally at\\\\nthe National Research Council of Canada.\\\\nMEDLINE was refreshed before running the searches.\\\\nThe MEDLINE records were represented as bags of fea-\\\\ntures, where features consisted of lowercased words and\\\\nword combinations (up to four words) from the title and ab-\\\\nstract ﬁelds, full MeSH terms, and contents of all other\\\\nﬁelds in the MEDLINE record. Features were uniformly\\\\nweighted. The SVM implementation used was SVM Light\\\\n[23] used with a linear kernel.\\\\n\\\\nThe approach was piloted with one AHRQ evidence\\\\nreport, and various conﬁgurations were tried, observing\\\\nthe placement of several known new relevant studies within\\\\nthe retrieval. The MEDLINE set was ﬁltered with the\\\\nrevised Highly Sensitive Search Strategy (HSSS) [24]. Pre-\\\\nliminary testing indicated that this ﬁlter would retrieve\\\\n99.1% of MEDLINE-indexed relevant new evidence from\\\\nthe larger cohort of journal-published systematic reviews\\\\n(n 5 72) and Cochrane reviews (n 5 27). This HSSS ﬁlter\\\\nwas added primarily to improve processing speed, relative\\\\nto running the classiﬁer against all of MEDLINE. With\\\\nthe ﬁlter, processing for one systematic review took about\\\\n10 minutes. Next, a coarse-grained ﬁlter was applied;\\\\nincluded and excluded instances were combined in a single\\\\nset labeled POS. Adjacent PubMed IDs (the next higher\\\\nPMID to each member of POS, as long as that higher num-\\\\nber was not itself a member of POS) were combined in a set\\\\nto represent WORLD. The model was trained on POS vs.\\\\nWORLD, using words and phrases from title, abstract,\\\\nauthor names, and journal name. This model was run\\\\nagainst the records passing the HSSS ﬁlter.\\\\n\\\\nFour different subsets of\\\\n\\\\nthe SVM retrieval were\\\\nexamineddone set included all records with a relevance\\\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\\\n(SVM200point5). Other sets consisted of all records with\\\\nrelevance scores of 0.95 or more (SVM95), 0.90 or more\\\\n(SVM90), and 0.80 or more (SVM80). PubMed ID and\\\\nrelevance scores were recorded, and PubMed IDs were\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n111\\\\n\\\\nTable 1. Characteristics of the included reviews\\\\n\\\\nCochrane\\\\nreviews; N\\\\n\\\\nAHRQ evidence\\\\n\\\\nreports; N\\\\n\\\\nincorporated into the database of records for the reviewers\\\\nto screen. Only the results from SVM200point5 are re-\\\\nported here and are described as SVM.\\\\n\\\\n2.3. Determining performance of the test searches\\\\n\\\\nFor the updated Cochrane reviews, reference standard ar-\\\\nticles were those studies included in the updated review that\\\\nwere not in the original and which entered MEDLINE after\\\\nthe search date of the original review. For the AHRQ evi-\\\\ndence reports, the full retrieval set was screened. Records\\\\nfound relevant by the consensus of reviewers were consid-\\\\nered reference standard articles. Recall was the proportion\\\\nof reference standard articles identiﬁed by the search:\\\\n\\\\nNumber of reference standard articles found\\\\nTotal number of reference standard articles\\\\n\\\\nBioVenn software was used to analyze the overlap be-\\\\ntween the retrieval of relevant articles from the three\\\\nsearches and to create Venn diagrams [25].\\\\n\\\\nAll included reviews were classiﬁed into clinical area\\\\nbased on factors such as ISI journal classiﬁcation, the Co-\\\\nchrane Collaboration Review Group where the topic might\\\\nbe placed and the high level MeSH term under which the pop-\\\\nulation (condition) would be indexed. Performance of the\\\\nsearches in different clinical areas was displayed graphically\\\\nfor the searches both alone and in combinations, to allow ex-\\\\namination of differences in parallelism, level, and ﬂatness.\\\\n\\\\nPrecision is the proportion of all retrieved records that\\\\n\\\\nare relevant:\\\\n\\\\nNumber of reference standard articles found\\\\n\\\\nTotal number of records retrieved\\\\n\\\\nThe inverse of precision is the number needed to read to\\\\nﬁnd one eligible study, thus precision inﬂuences the work\\\\nof the review. Precision was calculated only for the AHRQ\\\\nevidence reviews. Because not all candidates retrieved by\\\\nthe Cochrane searches were assessed, precision could not\\\\nbe established in that sample.\\\\n\\\\n2.4. Determining stability of results over time\\\\n\\\\nThe CQ and related article searches, originally run in\\\\nMarch 2008, were repeated in February 2015 in the AHRQ\\\\ncohort. Recall of relevant items was compared with the\\\\noriginal retrievals to determine if any changes introduced\\\\nby National Library of Medicine might invalidate ﬁndings.\\\\nAs SVM conﬁguration is under the control of the investiga-\\\\ntors, it was not retested.\\\\n\\\\n3. Results\\\\n\\\\nSix Cochrane reviews and 10 AHRQ evidence reports\\\\nmet all inclusion criteria (Appendix B at www.jclinepi.\\\\ncom). Characteristics of the included reviews are shown\\\\nin Table 1.\\\\n\\\\n9a\\\\n1\\\\n3\\\\n\\\\n3\\\\n\\\\nd\\\\n1\\\\n\\\\nd\\\\n1\\\\n2\\\\n1\\\\n1\\\\n\\\\n1\\\\nd\\\\nd\\\\n\\\\nd\\\\n3\\\\n3\\\\n4\\\\n\\\\nCharacteristic\\\\n\\\\nTherapy evaluated\\\\n\\\\nMedications\\\\nMedical devices\\\\nProcedures\\\\n\\\\nClinical topic area\\\\n\\\\nCardiac and cardiovascular\\\\n\\\\nsystems\\\\n\\\\nCritical care\\\\nEndocrinology and\\\\n\\\\nmetabolism\\\\n\\\\nInfectious disease\\\\nClinical neurology\\\\nObstetrics and gynecology\\\\nOncology\\\\nPeripheral vascular\\\\n\\\\ndiseases\\\\nPsychiatry\\\\nRespiratory systems\\\\nUrology and nephrology\\\\n\\\\nPublication period\\\\n\\\\nMarch 1997eApril 1999\\\\nMay 1999eJune 2001\\\\nJuly 2001eAugust 2003\\\\nSeptember 2003e\\\\nDecember 2005\\\\n\\\\nMedian included trials\\\\n\\\\nMedian included\\\\n\\\\nparticipants\\\\n\\\\nMEDLINE coverage of\\\\n\\\\n5\\\\n1\\\\nd\\\\n\\\\nd\\\\n\\\\n1\\\\nd\\\\n\\\\n3\\\\nd\\\\nd\\\\nd\\\\nd\\\\n\\\\nd\\\\n1\\\\n1\\\\n\\\\n3\\\\n1\\\\n2\\\\nd\\\\n\\\\n17 (IQR, 14e20)\\\\n\\\\n96 (IQR,\\\\n\\\\n31.75e121.5)\\\\n8,679 (IQR,\\\\n22,830 (IQR, 14,\\\\n4,085e50,109)\\\\n172e49,687)\\\\n99/107 (92.5%) 969/980 (98.8%)\\\\n\\\\nincluded articles\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\\\n\\\\nity; IQR, interquartile range.\\\\n\\\\na Some AHRQ reviews included more than one class of therapy.\\\\n\\\\nRecall of new relevant studies is shown in Table 2. The\\\\nupdated Cochrane reviews had 20 new studies identiﬁed by\\\\nthe review authors and added in the updates. Recall of these\\\\n20 ranged from 1.00 for the related articles method to a low\\\\nof 0.80 CQs (Table 2). Our team identiﬁed 277 new studies\\\\nas relevant for inclusion in the 10 evidence reports that\\\\nwere updated. All test searches showed lower recall for this\\\\ncohort than for the Cochrane reviews and here the CQ out-\\\\nperformed both ranking searches (Table 2).\\\\n\\\\n3.1. Recall of CQ combined with a ranking method\\\\n\\\\nOf the 297 new relevant studies identiﬁed, the combina-\\\\ntion of CQ and related articles searches identiﬁed 270\\\\n(overall\\\\nin\\\\nthe Cochrane cohort and 0.90 (250/277) in the AHRQ\\\\ncohort.\\\\n\\\\nrecall of 0.91). Recall was 1.00 (20/20)\\\\n\\\\nThe combination of CQ and SVM identiﬁed 263 studies\\\\n(overall recall of 0.89). Recall was 1.00 (20/20) in the\\\\nCochrane cohort and 0.88 (243/277) in the AHRQ cohort.\\\\nWhen all three methods were used together, recall was\\\\n0.997 with 296 of the 297 studies identiﬁed.\\\\n\\\\n\\\\x0c112\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nTable 2. Recall of eligible studies by the search methods\\\\n\\\\nCochrane reviews\\\\n\\\\nAHRQ evidence\\\\n\\\\nreports\\\\n\\\\nRetrieval method\\\\n\\\\nClinical query\\\\nRelated articles\\\\nSVM\\\\nTotal\\\\n\\\\nN\\\\n\\\\n16\\\\n20\\\\n19\\\\n20\\\\n\\\\nRecall\\\\n\\\\n0.80\\\\n1.00\\\\n0.95\\\\n\\\\nN\\\\n\\\\n188\\\\n176\\\\n129\\\\n277\\\\n\\\\nRecall\\\\n\\\\n0.68\\\\n0.64\\\\n0.47\\\\n\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\\\n\\\\nity; SVM, support vector machine.\\\\n\\\\nAcross the two cohorts, the overlap and unique component from\\\\nthe retrieval of relevant records by the test searches was examined\\\\n(Fig. 1).\\\\n\\\\n3.2. Consistency across clinical areas\\\\n\\\\nThirteen clinical areas were represented in the larger\\\\nstudy that included 72 journal-published reviews, but only\\\\neight of those clinical areas had two or more new relevant\\\\nstudies in the smaller Cochrane and AHRQ cohorts. Fig. 2\\\\nshows recall of new studies by the three types of searches,\\\\nfor these eight clinical areas.\\\\n\\\\nCombined recall of the CQ Boolean search paired with a\\\\nranking search is shown in Fig. 3. The combination showed\\\\ncomplete recall of relevant new studies in three of eight\\\\nareas when CQ was paired with related articles and recall\\\\nof 0.67 or higher in all areas. Four of eight clinical areas\\\\nhad complete recall for the combination of CQ and SVM,\\\\nand recall was 0.80 or higher in all areas. Even for periph-\\\\neral vascular disease, where all three searches performed\\\\n\\\\nFig. 2. Recall of new studies by clinical area for each search method.\\\\nAbbreviations: SVM, support vector machine.\\\\n\\\\nfairly poorly individually, recall was 0.76 for the CQ/\\\\nrelated articles and 0.90 in the CQ/SVM pairing.\\\\n\\\\n3.3. Search precision\\\\n\\\\nPrecision, across the 10 AHRQ reviews, was 0.11 for the\\\\nCQ, 0.22 for related articles, and 0.19 for SVM (Table 3).\\\\n\\\\nFig. 1. Overlapping and unique retrieval of relevant studies by each\\\\nsearch method. The size of the circle is proportional to the size of\\\\nthe retrieval, but the overlap and unique portions are approximations.\\\\nExact ﬁgures for the components are labeled. Abbreviations: SVM,\\\\nsupport vector machine.\\\\n\\\\nFig. 3. Recall of new studies by clinical area for search methods in\\\\ncombination. Abbreviations: CQ, clinical query; SVM, support vector\\\\nmachine.\\\\n\\\\n\\\\x0c', 'Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nComplementary approaches to searching MEDLINE may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\\\n\\\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\\\n\\\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\\\n\\\\n0R6, Canada\\\\n\\\\nUnited Kingdom\\\\n\\\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\\\n\\\\nAccepted 7 March 2016; Published online 11 March 2016\\\\n\\\\nAbstract\\\\n\\\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\\\n\\\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\\\ncombined. Related articles showed least stability over time.\\\\n\\\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\\\n\\\\n1. Introduction\\\\n\\\\nSystematic review searches need to have high recall.\\\\nMechanisms to achieve this usually include expansive\\\\nBoolean searches of multiple databases. This approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nMEDLINE gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. However,\\\\n\\\\nConﬂict of interest: None.\\\\nFunding: Some of this work was funded by the US Agency for Health-\\\\ncare Research and Quality, Department of Health and Human Services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* Corresponding author. Tel.: 613-737-7600.\\\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through MEDLINE. Recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in MEDLINE, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nSuccessful retrieval through a Boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. The search of\\\\nmultiple databases can therefore be helpful. The target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. A text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. These tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nWhat is new?\\\\n\\\\nKey ﬁndings\\\\n(cid:1) Searches using known relevant studies and the\\\\nsimilar articles feature of PubMed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) For a given question, if the Boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. The two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) The precision of\\\\n\\\\nmethod appears better\\\\nexhaustive Boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nWhat this adds to what was known?\\\\n(cid:1) The paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nIt is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nWhat is the implication and what should change\\\\nnow?\\\\n(cid:1) Using the simple and universally available PubMed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) If the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nMEDLINE.\\\\n\\\\nto large retrievals with low precision [9]. These resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional Boolean searches have\\\\nbeen sought [10].\\\\n\\\\nMost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nThese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. These methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nWe examined one method, support vector machine\\\\n(SVM), and compared it with a simple and readily available\\\\nmethod based on the PubMed similar articles feature. We\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. We paired both\\\\nwith a focused Boolean search within MEDLINE. We\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for SVM and seed articles for the PubMed-\\\\n\\\\nrelated articles search. We therefore sought to determine\\\\nif a focused Boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\nComparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. In this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (CQ), and PubMed-related articles are tested in a\\\\ncohort of six updated Cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 Agency for Healthcare\\\\nResearch and Quality (AHRQ) evidence reports. The\\\\nCochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. The replication in the\\\\nAHRQ cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. All records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nOther research [17,18] suggests that searches using the\\\\nPubMed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a Boolean-type search of\\\\nMEDLINE. We tested an additional search method, SVM,\\\\nin the Cochrane and AHRQ samples to permit comparison\\\\nwith our PubMed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the PubMed\\\\nsimilar articles method.\\\\n\\\\nThe aim of this article was to test whether the combined\\\\napproach of a focused Boolean search paired with a second\\\\nsearch using the similar articles feature of PubMed or SVM\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. Methods\\\\n\\\\n2.1. Formation of the study cohorts\\\\n\\\\nThis analysis uses a data set created for an updating\\\\nstudy sponsored by AHRQ [16]. Methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\\\nsearch of the ACP Journal Club database (Ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nAHRQ reports were identiﬁed through the PubMed\\\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\\\njrid21544]. Screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. Screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c110\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nFor inclusion, all reviews must have included random-\\\\nized controlled trials (RCTs) and provided meta-analysis\\\\nfor at least one outcome. The MEDLINE search strategies\\\\nhad to be reported in enough detail to permit replication.\\\\nFifteen AHRQ evidence reports thought\\\\nlikely to have\\\\nimportant new evidence were selected and used to validate\\\\nthe updating methods used in the main cohort.\\\\n\\\\nCochrane reviews were selected to meet minimum qual-\\\\nity and relevance standards, as deﬁned by ACP Journal\\\\nClub [19]. We also required that the review had been up-\\\\ndated and that the text of both the original review and an\\\\nupdated version was available. The search for the original\\\\nreview must have included MEDLINE and at least one\\\\nother electronic bibliographic database and one or more\\\\nnondatabase method such as hand searching or checking\\\\nreference lists. Such comprehensive searching was likely\\\\nto identify most relevant\\\\nliterature and form a useful\\\\ntraining set of included examples. At least 10 RCTs or\\\\nquasi-RCTs must have been included in the original review\\\\nto give an adequate training set for SVM.\\\\n\\\\n2.2. Test searches\\\\n\\\\n2.2.1. Clinical query\\\\n\\\\nBoolean searches were developed by a librarian experi-\\\\nenced in systematic review searches using a protocol\\\\n(Appendix A at www.jclinepi.com). Search strategies were\\\\ndeliberately simple, usually consisting of two or three\\\\nMedical Subject Heading (MeSH) terms representing the\\\\npopulation and intervention of the review. This search\\\\nwas limited by a CQ ﬁlter to ‘‘therapy (best balance of\\\\nsensitivity\\\\n‘‘Randomized\\\\ncontrolled trial.pt. or\\\\nrandomized.mp. or placebo.mp.’’\\\\n[20]. Boolean searches were run in OVID MEDLINE.\\\\n\\\\nspeciﬁcity)’’\\\\n\\\\nthat\\\\n\\\\nand\\\\n\\\\nis,\\\\n\\\\n2.2.2. Related articles\\\\n\\\\nSearches using the PubMed similar articles feature were\\\\nperformed using the PubMed unique identiﬁers (PMID) of\\\\nthe three newest and three largest studies included in the\\\\noriginal review as seed articles. There was no replacement\\\\nin the event of overlap between the largest and newest sets,\\\\nor if one of those studies was not indexed in MEDLINE.\\\\n(All studies included in the original reviews were checked\\\\nfor indexing status in MEDLINE.) The resulting set was\\\\nlimited to the publication type RCT and date limited to\\\\nthe period since the search date of the original review.\\\\n\\\\n2.2.3. Support vector machine\\\\n\\\\nThe SVM searches used for this project have not been\\\\npreviously described so the methods will be reported in\\\\nmore detail. SVM is a well established and broadly applied\\\\nclassiﬁcation method from machine learning [21,22]. A\\\\nclassiﬁcation task is closely related to an information\\\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\\\nthe collection. As SVM is a supervised algorithm,\\\\nit\\\\n\\\\nrequires training examples with known labels. It then places\\\\nthese training examples in a high-dimensional vector space\\\\nin such a way that examples of one class are separated from\\\\nexamples of the other class with the greatest distance to the\\\\nseparating boundary. Predicting the class of a new instance\\\\nhappens by placing it in the same space and determining on\\\\nwhich side of the boundary it falls. The distance to the\\\\nboundary is an indication of the prediction conﬁdence and\\\\nis used to rank instances (documents) as is essential in in-\\\\nformation retrieval tasks or to provide a cutoff threshold.\\\\n\\\\nIn our setup, the training set consisted of search results\\\\nfrom the original review formed with included instances\\\\nbeing the studies included in the review (the relevant re-\\\\ntrievals) and excluded instances being the studies found\\\\nby the search used in the original review but excluded from\\\\nthe review (irrelevant retrievals). The new examples to be\\\\nclassiﬁed were articles indexed since the date of the search\\\\nperformed in the original review.\\\\n\\\\nSVM searches were run on MEDLINE data stored\\\\nlocally at\\\\nthe National Research Council of Canada.\\\\nMEDLINE was refreshed before running the searches.\\\\nThe MEDLINE records were represented as bags of fea-\\\\ntures, where features consisted of lowercased words and\\\\nword combinations (up to four words) from the title and ab-\\\\nstract ﬁelds, full MeSH terms, and contents of all other\\\\nﬁelds in the MEDLINE record. Features were uniformly\\\\nweighted. The SVM implementation used was SVM Light\\\\n[23] used with a linear kernel.\\\\n\\\\nThe approach was piloted with one AHRQ evidence\\\\nreport, and various conﬁgurations were tried, observing\\\\nthe placement of several known new relevant studies within\\\\nthe retrieval. The MEDLINE set was ﬁltered with the\\\\nrevised Highly Sensitive Search Strategy (HSSS) [24]. Pre-\\\\nliminary testing indicated that this ﬁlter would retrieve\\\\n99.1% of MEDLINE-indexed relevant new evidence from\\\\nthe larger cohort of journal-published systematic reviews\\\\n(n 5 72) and Cochrane reviews (n 5 27). This HSSS ﬁlter\\\\nwas added primarily to improve processing speed, relative\\\\nto running the classiﬁer against all of MEDLINE. With\\\\nthe ﬁlter, processing for one systematic review took about\\\\n10 minutes. Next, a coarse-grained ﬁlter was applied;\\\\nincluded and excluded instances were combined in a single\\\\nset labeled POS. Adjacent PubMed IDs (the next higher\\\\nPMID to each member of POS, as long as that higher num-\\\\nber was not itself a member of POS) were combined in a set\\\\nto represent WORLD. The model was trained on POS vs.\\\\nWORLD, using words and phrases from title, abstract,\\\\nauthor names, and journal name. This model was run\\\\nagainst the records passing the HSSS ﬁlter.\\\\n\\\\nFour different subsets of\\\\n\\\\nthe SVM retrieval were\\\\nexamineddone set included all records with a relevance\\\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\\\n(SVM200point5). Other sets consisted of all records with\\\\nrelevance scores of 0.95 or more (SVM95), 0.90 or more\\\\n(SVM90), and 0.80 or more (SVM80). PubMed ID and\\\\nrelevance scores were recorded, and PubMed IDs were\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n111\\\\n\\\\nTable 1. Characteristics of the included reviews\\\\n\\\\nCochrane\\\\nreviews; N\\\\n\\\\nAHRQ evidence\\\\n\\\\nreports; N\\\\n\\\\nincorporated into the database of records for the reviewers\\\\nto screen. Only the results from SVM200point5 are re-\\\\nported here and are described as SVM.\\\\n\\\\n2.3. Determining performance of the test searches\\\\n\\\\nFor the updated Cochrane reviews, reference standard ar-\\\\nticles were those studies included in the updated review that\\\\nwere not in the original and which entered MEDLINE after\\\\nthe search date of the original review. For the AHRQ evi-\\\\ndence reports, the full retrieval set was screened. Records\\\\nfound relevant by the consensus of reviewers were consid-\\\\nered reference standard articles. Recall was the proportion\\\\nof reference standard articles identiﬁed by the search:\\\\n\\\\nNumber of reference standard articles found\\\\nTotal number of reference standard articles\\\\n\\\\nBioVenn software was used to analyze the overlap be-\\\\ntween the retrieval of relevant articles from the three\\\\nsearches and to create Venn diagrams [25].\\\\n\\\\nAll included reviews were classiﬁed into clinical area\\\\nbased on factors such as ISI journal classiﬁcation, the Co-\\\\nchrane Collaboration Review Group where the topic might\\\\nbe placed and the high level MeSH term under which the pop-\\\\nulation (condition) would be indexed. Performance of the\\\\nsearches in different clinical areas was displayed graphically\\\\nfor the searches both alone and in combinations, to allow ex-\\\\namination of differences in parallelism, level, and ﬂatness.\\\\n\\\\nPrecision is the proportion of all retrieved records that\\\\n\\\\nare relevant:\\\\n\\\\nNumber of reference standard articles found\\\\n\\\\nTotal number of records retrieved\\\\n\\\\nThe inverse of precision is the number needed to read to\\\\nﬁnd one eligible study, thus precision inﬂuences the work\\\\nof the review. Precision was calculated only for the AHRQ\\\\nevidence reviews. Because not all candidates retrieved by\\\\nthe Cochrane searches were assessed, precision could not\\\\nbe established in that sample.\\\\n\\\\n2.4. Determining stability of results over time\\\\n\\\\nThe CQ and related article searches, originally run in\\\\nMarch 2008, were repeated in February 2015 in the AHRQ\\\\ncohort. Recall of relevant items was compared with the\\\\noriginal retrievals to determine if any changes introduced\\\\nby National Library of Medicine might invalidate ﬁndings.\\\\nAs SVM conﬁguration is under the control of the investiga-\\\\ntors, it was not retested.\\\\n\\\\n3. Results\\\\n\\\\nSix Cochrane reviews and 10 AHRQ evidence reports\\\\nmet all inclusion criteria (Appendix B at www.jclinepi.\\\\ncom). Characteristics of the included reviews are shown\\\\nin Table 1.\\\\n\\\\n9a\\\\n1\\\\n3\\\\n\\\\n3\\\\n\\\\nd\\\\n1\\\\n\\\\nd\\\\n1\\\\n2\\\\n1\\\\n1\\\\n\\\\n1\\\\nd\\\\nd\\\\n\\\\nd\\\\n3\\\\n3\\\\n4\\\\n\\\\nCharacteristic\\\\n\\\\nTherapy evaluated\\\\n\\\\nMedications\\\\nMedical devices\\\\nProcedures\\\\n\\\\nClinical topic area\\\\n\\\\nCardiac and cardiovascular\\\\n\\\\nsystems\\\\n\\\\nCritical care\\\\nEndocrinology and\\\\n\\\\nmetabolism\\\\n\\\\nInfectious disease\\\\nClinical neurology\\\\nObstetrics and gynecology\\\\nOncology\\\\nPeripheral vascular\\\\n\\\\ndiseases\\\\nPsychiatry\\\\nRespiratory systems\\\\nUrology and nephrology\\\\n\\\\nPublication period\\\\n\\\\nMarch 1997eApril 1999\\\\nMay 1999eJune 2001\\\\nJuly 2001eAugust 2003\\\\nSeptember 2003e\\\\nDecember 2005\\\\n\\\\nMedian included trials\\\\n\\\\nMedian included\\\\n\\\\nparticipants\\\\n\\\\nMEDLINE coverage of\\\\n\\\\n5\\\\n1\\\\nd\\\\n\\\\nd\\\\n\\\\n1\\\\nd\\\\n\\\\n3\\\\nd\\\\nd\\\\nd\\\\nd\\\\n\\\\nd\\\\n1\\\\n1\\\\n\\\\n3\\\\n1\\\\n2\\\\nd\\\\n\\\\n17 (IQR, 14e20)\\\\n\\\\n96 (IQR,\\\\n\\\\n31.75e121.5)\\\\n8,679 (IQR,\\\\n22,830 (IQR, 14,\\\\n4,085e50,109)\\\\n172e49,687)\\\\n99/107 (92.5%) 969/980 (98.8%)\\\\n\\\\nincluded articles\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\\\n\\\\nity; IQR, interquartile range.\\\\n\\\\na Some AHRQ reviews included more than one class of therapy.\\\\n\\\\nRecall of new relevant studies is shown in Table 2. The\\\\nupdated Cochrane reviews had 20 new studies identiﬁed by\\\\nthe review authors and added in the updates. Recall of these\\\\n20 ranged from 1.00 for the related articles method to a low\\\\nof 0.80 CQs (Table 2). Our team identiﬁed 277 new studies\\\\nas relevant for inclusion in the 10 evidence reports that\\\\nwere updated. All test searches showed lower recall for this\\\\ncohort than for the Cochrane reviews and here the CQ out-\\\\nperformed both ranking searches (Table 2).\\\\n\\\\n3.1. Recall of CQ combined with a ranking method\\\\n\\\\nOf the 297 new relevant studies identiﬁed, the combina-\\\\ntion of CQ and related articles searches identiﬁed 270\\\\n(overall\\\\nin\\\\nthe Cochrane cohort and 0.90 (250/277) in the AHRQ\\\\ncohort.\\\\n\\\\nrecall of 0.91). Recall was 1.00 (20/20)\\\\n\\\\nThe combination of CQ and SVM identiﬁed 263 studies\\\\n(overall recall of 0.89). Recall was 1.00 (20/20) in the\\\\nCochrane cohort and 0.88 (243/277) in the AHRQ cohort.\\\\nWhen all three methods were used together, recall was\\\\n0.997 with 296 of the 297 studies identiﬁed.\\\\n\\\\n\\\\x0c112\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nTable 2. Recall of eligible studies by the search methods\\\\n\\\\nCochrane reviews\\\\n\\\\nAHRQ evidence\\\\n\\\\nreports\\\\n\\\\nRetrieval method\\\\n\\\\nClinical query\\\\nRelated articles\\\\nSVM\\\\nTotal\\\\n\\\\nN\\\\n\\\\n16\\\\n20\\\\n19\\\\n20\\\\n\\\\nRecall\\\\n\\\\n0.80\\\\n1.00\\\\n0.95\\\\n\\\\nN\\\\n\\\\n188\\\\n176\\\\n129\\\\n277\\\\n\\\\nRecall\\\\n\\\\n0.68\\\\n0.64\\\\n0.47\\\\n\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\\\n\\\\nity; SVM, support vector machine.\\\\n\\\\nAcross the two cohorts, the overlap and unique component from\\\\nthe retrieval of relevant records by the test searches was examined\\\\n(Fig. 1).\\\\n\\\\n3.2. Consistency across clinical areas\\\\n\\\\nThirteen clinical areas were represented in the larger\\\\nstudy that included 72 journal-published reviews, but only\\\\neight of those clinical areas had two or more new relevant\\\\nstudies in the smaller Cochrane and AHRQ cohorts. Fig. 2\\\\nshows recall of new studies by the three types of searches,\\\\nfor these eight clinical areas.\\\\n\\\\nCombined recall of the CQ Boolean search paired with a\\\\nranking search is shown in Fig. 3. The combination showed\\\\ncomplete recall of relevant new studies in three of eight\\\\nareas when CQ was paired with related articles and recall\\\\nof 0.67 or higher in all areas. Four of eight clinical areas\\\\nhad complete recall for the combination of CQ and SVM,\\\\nand recall was 0.80 or higher in all areas. Even for periph-\\\\neral vascular disease, where all three searches performed\\\\n\\\\nFig. 2. Recall of new studies by clinical area for each search method.\\\\nAbbreviations: SVM, support vector machine.\\\\n\\\\nfairly poorly individually, recall was 0.76 for the CQ/\\\\nrelated articles and 0.90 in the CQ/SVM pairing.\\\\n\\\\n3.3. Search precision\\\\n\\\\nPrecision, across the 10 AHRQ reviews, was 0.11 for the\\\\nCQ, 0.22 for related articles, and 0.19 for SVM (Table 3).\\\\n\\\\nFig. 1. Overlapping and unique retrieval of relevant studies by each\\\\nsearch method. The size of the circle is proportional to the size of\\\\nthe retrieval, but the overlap and unique portions are approximations.\\\\nExact ﬁgures for the components are labeled. Abbreviations: SVM,\\\\nsupport vector machine.\\\\n\\\\nFig. 3. Recall of new studies by clinical area for search methods in\\\\ncombination. Abbreviations: CQ, clinical query; SVM, support vector\\\\nmachine.\\\\n\\\\n\\\\x0cTable 3. Overall precision in the AHRQ Sample\\\\n\\\\n4. Discussion\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n113\\\\n\\\\nRetrieval method\\\\n\\\\nClinical query\\\\nRelated articles\\\\nSVM\\\\nClinical query þ related\\\\nClinical query þ SVM\\\\nClinical query þ related\\\\n\\\\narticles\\\\n\\\\nEligible\\\\nstudies\\\\nretrieved\\\\n\\\\n187\\\\n176\\\\n128\\\\n250\\\\n\\\\n243\\\\n276\\\\n\\\\nNo. of candidates\\\\n\\\\nretrieved\\\\n\\\\nPrecision\\\\n\\\\n1,637\\\\n814\\\\n659\\\\n2,318a\\\\n\\\\n2,247a\\\\n3,264a\\\\n\\\\n0.11\\\\n0.22\\\\n0.19\\\\n0.11\\\\n\\\\n0.11\\\\n0.08\\\\n\\\\narticles þ SVM\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and\\\\n\\\\nQuality; SVM, support vector machine.\\\\n\\\\na Number of candidates after removal of duplicate records.\\\\n\\\\nPrecision for the individual reviews ranged from 0.0 to 0.38\\\\nfor CQs, 0.004 to 0.80 for related articles, and 0.01 to 0.43\\\\nfor SVM. Overall precision was 0.11 for CQ and either\\\\nrelated articles or SVM. Overall precision was 0.08 when\\\\nCQ and both related articles and SVM were used. For\\\\nranked searches where a ﬁxed number of records will be\\\\nscreened, that number is always the denominator, so preci-\\\\nsion will tend to increase as the number of relevant records\\\\nincreasesdthe maximum retrieval size for SVM was cap-\\\\nped at 200 records.\\\\n\\\\n3.4. Retrieval consistency over time\\\\n\\\\nSearches were retested for the AHRQ cohort using\\\\nPubMed results obtained February 24, 2015. Recall of the\\\\nrelated articles searches across the 10 AHRQ reports was\\\\n0.64 originally and 0.50 when repeated (Fig. 4). Of the\\\\nthree extreme cases, one rose from 0.00 to 0.80, one re-\\\\nmained at 0.00, and one dropped from 1.00 to 0.60. CQ\\\\nhad overall\\\\nrecall of 0.68 originally and 0.70 when\\\\nrepeated. Changes in extreme cases were minimal. Overall\\\\ncombined recall of CQ and related article was 0.90 origi-\\\\nnally,\\\\nfalling to 0.85 when repeated. Considering the\\\\nextreme values, all review with combined recall of 1.00\\\\noriginally remained at 1.00, whereas the one review with\\\\n0.00 recall in combination showed recall of 0.80 when\\\\nrepeated.\\\\n\\\\n1.0\\\\n0.9\\\\n0.8\\\\n0.7\\\\n0.6\\\\n0.5\\\\n0.4\\\\n0.3\\\\n0.2\\\\n0.1\\\\n0.0\\\\n\\\\nOriginal\\\\n\\\\n2015\\\\n\\\\nClinical Query\\\\n\\\\nRelated Ar(cid:415)cles\\\\n\\\\nCombined\\\\n\\\\nFig. 4. Recall of new studies for search methods, alone and in combi-\\\\nnation originally (left) and when retested in 2015 (right).\\\\n\\\\nWe expected that\\\\n\\\\nthe sophisticated SVM approach\\\\nwould perform well when paired with more stripped-\\\\ndown, focused Boolean searches. Indeed, the two together\\\\nwere able to replace the multidatabase, multimodel\\\\nsearches used by the original review teams in updating\\\\nthe Cochrane searches. The new relevant studies for the Co-\\\\nchrane reviews may have been relatively easy to ﬁnd, but\\\\nthe pairing was also effective in the AHRQ set, and there\\\\nshowed better precision than is usually seen with traditional\\\\nsearches for systematic review of RCTs [9]. The AHRQ set\\\\nwas formed from the new studies identiﬁed for more com-\\\\nplex interventions in a rigorous, well-funded study [16].\\\\n\\\\nWe were surprised that the related articles approach per-\\\\nformed almost as well as SVM when used in combination\\\\nwith the CQ. The related article search has advantages over\\\\nSVMdit requires far less data preparation, and no special\\\\nsoftware is needed for its use. This makes it useful not only\\\\nin updating, but also, if appropriate seed articles can be\\\\nfound, in original reviews.\\\\n\\\\nThat both SVM and related articles sometimes showed\\\\npoor recall when used alone, but consistently good recall\\\\nwhen used with a Boolean method suggests that there is\\\\nreal beneﬁt in using complementary search methods to\\\\nquerying MEDLINE. The related article method is not very\\\\ntime consuming, and easily added to other planned search\\\\nefforts, while the third method used here, SVM, is more\\\\ntechnical. The combined performance of all three methods\\\\nwas surprising, but such a setup might become unpractical\\\\nin day-to-day use.\\\\n\\\\nOther investigators have similar ﬁndings. Examination\\\\nof the supplemental material presented in the Appendix C\\\\nto the article by Waffenschmidt et al. [18] reveals that\\\\nacross the 19 reviews tested, the combination of a simple\\\\nstructured Boolean search (SSBS) and the ﬁrst 50 retrieval\\\\nof PubMed similar articles showed complete retrieval of all\\\\nreference standard articles in 14 of 19 reviews and never\\\\nless than 0.90 recall (Appendix C at www.jclinepi.com pre-\\\\nsents data from the table by Waffenschmidt graphically).\\\\nWaffenschmidt concluded ‘‘the combination of these two\\\\nsearch techniques that are independent of each other seems\\\\nto compensate the respective weaknesses.’’\\\\n\\\\nSSBS search by Waffenschmidt was constructed in\\\\nPubMed from search terms selected for the indication and\\\\nintervention with PubMed’s narrow CQ ﬁlter (category:\\\\ntherapy). Their search using the similar articles feature (Re-\\\\nlCits) did not use a set of seed articles; rather, the RelCits\\\\nfunction was applied for each relevant citation previously\\\\nidentiﬁed in PubMed. Their test articles were the included\\\\nstudies in 19 systematic reviews of drugs.\\\\n\\\\nAgoritsas et al. also described search construction\\\\nmethods for searches based on CQ and PubMed similar ar-\\\\nticles, tested for their ability to retrieve the included studies\\\\nof 30 Cochrane reviews [17]. Although their search con-\\\\nstruction methods differed from those used here, both\\\\n\\\\n\\\\x0c', 'Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nComplementary approaches to searching MEDLINE may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\\\n\\\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\\\n\\\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\\\n\\\\n0R6, Canada\\\\n\\\\nUnited Kingdom\\\\n\\\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\\\n\\\\nAccepted 7 March 2016; Published online 11 March 2016\\\\n\\\\nAbstract\\\\n\\\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\\\n\\\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\\\ncombined. Related articles showed least stability over time.\\\\n\\\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\\\n\\\\n1. Introduction\\\\n\\\\nSystematic review searches need to have high recall.\\\\nMechanisms to achieve this usually include expansive\\\\nBoolean searches of multiple databases. This approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nMEDLINE gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. However,\\\\n\\\\nConﬂict of interest: None.\\\\nFunding: Some of this work was funded by the US Agency for Health-\\\\ncare Research and Quality, Department of Health and Human Services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* Corresponding author. Tel.: 613-737-7600.\\\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through MEDLINE. Recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in MEDLINE, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nSuccessful retrieval through a Boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. The search of\\\\nmultiple databases can therefore be helpful. The target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. A text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. These tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nWhat is new?\\\\n\\\\nKey ﬁndings\\\\n(cid:1) Searches using known relevant studies and the\\\\nsimilar articles feature of PubMed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) For a given question, if the Boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. The two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) The precision of\\\\n\\\\nmethod appears better\\\\nexhaustive Boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nWhat this adds to what was known?\\\\n(cid:1) The paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nIt is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nWhat is the implication and what should change\\\\nnow?\\\\n(cid:1) Using the simple and universally available PubMed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) If the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nMEDLINE.\\\\n\\\\nto large retrievals with low precision [9]. These resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional Boolean searches have\\\\nbeen sought [10].\\\\n\\\\nMost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nThese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. These methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nWe examined one method, support vector machine\\\\n(SVM), and compared it with a simple and readily available\\\\nmethod based on the PubMed similar articles feature. We\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. We paired both\\\\nwith a focused Boolean search within MEDLINE. We\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for SVM and seed articles for the PubMed-\\\\n\\\\nrelated articles search. We therefore sought to determine\\\\nif a focused Boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\nComparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. In this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (CQ), and PubMed-related articles are tested in a\\\\ncohort of six updated Cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 Agency for Healthcare\\\\nResearch and Quality (AHRQ) evidence reports. The\\\\nCochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. The replication in the\\\\nAHRQ cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. All records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nOther research [17,18] suggests that searches using the\\\\nPubMed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a Boolean-type search of\\\\nMEDLINE. We tested an additional search method, SVM,\\\\nin the Cochrane and AHRQ samples to permit comparison\\\\nwith our PubMed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the PubMed\\\\nsimilar articles method.\\\\n\\\\nThe aim of this article was to test whether the combined\\\\napproach of a focused Boolean search paired with a second\\\\nsearch using the similar articles feature of PubMed or SVM\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. Methods\\\\n\\\\n2.1. Formation of the study cohorts\\\\n\\\\nThis analysis uses a data set created for an updating\\\\nstudy sponsored by AHRQ [16]. Methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\\\nsearch of the ACP Journal Club database (Ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nAHRQ reports were identiﬁed through the PubMed\\\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\\\njrid21544]. Screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. Screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c110\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nFor inclusion, all reviews must have included random-\\\\nized controlled trials (RCTs) and provided meta-analysis\\\\nfor at least one outcome. The MEDLINE search strategies\\\\nhad to be reported in enough detail to permit replication.\\\\nFifteen AHRQ evidence reports thought\\\\nlikely to have\\\\nimportant new evidence were selected and used to validate\\\\nthe updating methods used in the main cohort.\\\\n\\\\nCochrane reviews were selected to meet minimum qual-\\\\nity and relevance standards, as deﬁned by ACP Journal\\\\nClub [19]. We also required that the review had been up-\\\\ndated and that the text of both the original review and an\\\\nupdated version was available. The search for the original\\\\nreview must have included MEDLINE and at least one\\\\nother electronic bibliographic database and one or more\\\\nnondatabase method such as hand searching or checking\\\\nreference lists. Such comprehensive searching was likely\\\\nto identify most relevant\\\\nliterature and form a useful\\\\ntraining set of included examples. At least 10 RCTs or\\\\nquasi-RCTs must have been included in the original review\\\\nto give an adequate training set for SVM.\\\\n\\\\n2.2. Test searches\\\\n\\\\n2.2.1. Clinical query\\\\n\\\\nBoolean searches were developed by a librarian experi-\\\\nenced in systematic review searches using a protocol\\\\n(Appendix A at www.jclinepi.com). Search strategies were\\\\ndeliberately simple, usually consisting of two or three\\\\nMedical Subject Heading (MeSH) terms representing the\\\\npopulation and intervention of the review. This search\\\\nwas limited by a CQ ﬁlter to ‘‘therapy (best balance of\\\\nsensitivity\\\\n‘‘Randomized\\\\ncontrolled trial.pt. or\\\\nrandomized.mp. or placebo.mp.’’\\\\n[20]. Boolean searches were run in OVID MEDLINE.\\\\n\\\\nspeciﬁcity)’’\\\\n\\\\nthat\\\\n\\\\nand\\\\n\\\\nis,\\\\n\\\\n2.2.2. Related articles\\\\n\\\\nSearches using the PubMed similar articles feature were\\\\nperformed using the PubMed unique identiﬁers (PMID) of\\\\nthe three newest and three largest studies included in the\\\\noriginal review as seed articles. There was no replacement\\\\nin the event of overlap between the largest and newest sets,\\\\nor if one of those studies was not indexed in MEDLINE.\\\\n(All studies included in the original reviews were checked\\\\nfor indexing status in MEDLINE.) The resulting set was\\\\nlimited to the publication type RCT and date limited to\\\\nthe period since the search date of the original review.\\\\n\\\\n2.2.3. Support vector machine\\\\n\\\\nThe SVM searches used for this project have not been\\\\npreviously described so the methods will be reported in\\\\nmore detail. SVM is a well established and broadly applied\\\\nclassiﬁcation method from machine learning [21,22]. A\\\\nclassiﬁcation task is closely related to an information\\\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\\\nthe collection. As SVM is a supervised algorithm,\\\\nit\\\\n\\\\nrequires training examples with known labels. It then places\\\\nthese training examples in a high-dimensional vector space\\\\nin such a way that examples of one class are separated from\\\\nexamples of the other class with the greatest distance to the\\\\nseparating boundary. Predicting the class of a new instance\\\\nhappens by placing it in the same space and determining on\\\\nwhich side of the boundary it falls. The distance to the\\\\nboundary is an indication of the prediction conﬁdence and\\\\nis used to rank instances (documents) as is essential in in-\\\\nformation retrieval tasks or to provide a cutoff threshold.\\\\n\\\\nIn our setup, the training set consisted of search results\\\\nfrom the original review formed with included instances\\\\nbeing the studies included in the review (the relevant re-\\\\ntrievals) and excluded instances being the studies found\\\\nby the search used in the original review but excluded from\\\\nthe review (irrelevant retrievals). The new examples to be\\\\nclassiﬁed were articles indexed since the date of the search\\\\nperformed in the original review.\\\\n\\\\nSVM searches were run on MEDLINE data stored\\\\nlocally at\\\\nthe National Research Council of Canada.\\\\nMEDLINE was refreshed before running the searches.\\\\nThe MEDLINE records were represented as bags of fea-\\\\ntures, where features consisted of lowercased words and\\\\nword combinations (up to four words) from the title and ab-\\\\nstract ﬁelds, full MeSH terms, and contents of all other\\\\nﬁelds in the MEDLINE record. Features were uniformly\\\\nweighted. The SVM implementation used was SVM Light\\\\n[23] used with a linear kernel.\\\\n\\\\nThe approach was piloted with one AHRQ evidence\\\\nreport, and various conﬁgurations were tried, observing\\\\nthe placement of several known new relevant studies within\\\\nthe retrieval. The MEDLINE set was ﬁltered with the\\\\nrevised Highly Sensitive Search Strategy (HSSS) [24]. Pre-\\\\nliminary testing indicated that this ﬁlter would retrieve\\\\n99.1% of MEDLINE-indexed relevant new evidence from\\\\nthe larger cohort of journal-published systematic reviews\\\\n(n 5 72) and Cochrane reviews (n 5 27). This HSSS ﬁlter\\\\nwas added primarily to improve processing speed, relative\\\\nto running the classiﬁer against all of MEDLINE. With\\\\nthe ﬁlter, processing for one systematic review took about\\\\n10 minutes. Next, a coarse-grained ﬁlter was applied;\\\\nincluded and excluded instances were combined in a single\\\\nset labeled POS. Adjacent PubMed IDs (the next higher\\\\nPMID to each member of POS, as long as that higher num-\\\\nber was not itself a member of POS) were combined in a set\\\\nto represent WORLD. The model was trained on POS vs.\\\\nWORLD, using words and phrases from title, abstract,\\\\nauthor names, and journal name. This model was run\\\\nagainst the records passing the HSSS ﬁlter.\\\\n\\\\nFour different subsets of\\\\n\\\\nthe SVM retrieval were\\\\nexamineddone set included all records with a relevance\\\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\\\n(SVM200point5). Other sets consisted of all records with\\\\nrelevance scores of 0.95 or more (SVM95), 0.90 or more\\\\n(SVM90), and 0.80 or more (SVM80). PubMed ID and\\\\nrelevance scores were recorded, and PubMed IDs were\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n111\\\\n\\\\nTable 1. Characteristics of the included reviews\\\\n\\\\nCochrane\\\\nreviews; N\\\\n\\\\nAHRQ evidence\\\\n\\\\nreports; N\\\\n\\\\nincorporated into the database of records for the reviewers\\\\nto screen. Only the results from SVM200point5 are re-\\\\nported here and are described as SVM.\\\\n\\\\n2.3. Determining performance of the test searches\\\\n\\\\nFor the updated Cochrane reviews, reference standard ar-\\\\nticles were those studies included in the updated review that\\\\nwere not in the original and which entered MEDLINE after\\\\nthe search date of the original review. For the AHRQ evi-\\\\ndence reports, the full retrieval set was screened. Records\\\\nfound relevant by the consensus of reviewers were consid-\\\\nered reference standard articles. Recall was the proportion\\\\nof reference standard articles identiﬁed by the search:\\\\n\\\\nNumber of reference standard articles found\\\\nTotal number of reference standard articles\\\\n\\\\nBioVenn software was used to analyze the overlap be-\\\\ntween the retrieval of relevant articles from the three\\\\nsearches and to create Venn diagrams [25].\\\\n\\\\nAll included reviews were classiﬁed into clinical area\\\\nbased on factors such as ISI journal classiﬁcation, the Co-\\\\nchrane Collaboration Review Group where the topic might\\\\nbe placed and the high level MeSH term under which the pop-\\\\nulation (condition) would be indexed. Performance of the\\\\nsearches in different clinical areas was displayed graphically\\\\nfor the searches both alone and in combinations, to allow ex-\\\\namination of differences in parallelism, level, and ﬂatness.\\\\n\\\\nPrecision is the proportion of all retrieved records that\\\\n\\\\nare relevant:\\\\n\\\\nNumber of reference standard articles found\\\\n\\\\nTotal number of records retrieved\\\\n\\\\nThe inverse of precision is the number needed to read to\\\\nﬁnd one eligible study, thus precision inﬂuences the work\\\\nof the review. Precision was calculated only for the AHRQ\\\\nevidence reviews. Because not all candidates retrieved by\\\\nthe Cochrane searches were assessed, precision could not\\\\nbe established in that sample.\\\\n\\\\n2.4. Determining stability of results over time\\\\n\\\\nThe CQ and related article searches, originally run in\\\\nMarch 2008, were repeated in February 2015 in the AHRQ\\\\ncohort. Recall of relevant items was compared with the\\\\noriginal retrievals to determine if any changes introduced\\\\nby National Library of Medicine might invalidate ﬁndings.\\\\nAs SVM conﬁguration is under the control of the investiga-\\\\ntors, it was not retested.\\\\n\\\\n3. Results\\\\n\\\\nSix Cochrane reviews and 10 AHRQ evidence reports\\\\nmet all inclusion criteria (Appendix B at www.jclinepi.\\\\ncom). Characteristics of the included reviews are shown\\\\nin Table 1.\\\\n\\\\n9a\\\\n1\\\\n3\\\\n\\\\n3\\\\n\\\\nd\\\\n1\\\\n\\\\nd\\\\n1\\\\n2\\\\n1\\\\n1\\\\n\\\\n1\\\\nd\\\\nd\\\\n\\\\nd\\\\n3\\\\n3\\\\n4\\\\n\\\\nCharacteristic\\\\n\\\\nTherapy evaluated\\\\n\\\\nMedications\\\\nMedical devices\\\\nProcedures\\\\n\\\\nClinical topic area\\\\n\\\\nCardiac and cardiovascular\\\\n\\\\nsystems\\\\n\\\\nCritical care\\\\nEndocrinology and\\\\n\\\\nmetabolism\\\\n\\\\nInfectious disease\\\\nClinical neurology\\\\nObstetrics and gynecology\\\\nOncology\\\\nPeripheral vascular\\\\n\\\\ndiseases\\\\nPsychiatry\\\\nRespiratory systems\\\\nUrology and nephrology\\\\n\\\\nPublication period\\\\n\\\\nMarch 1997eApril 1999\\\\nMay 1999eJune 2001\\\\nJuly 2001eAugust 2003\\\\nSeptember 2003e\\\\nDecember 2005\\\\n\\\\nMedian included trials\\\\n\\\\nMedian included\\\\n\\\\nparticipants\\\\n\\\\nMEDLINE coverage of\\\\n\\\\n5\\\\n1\\\\nd\\\\n\\\\nd\\\\n\\\\n1\\\\nd\\\\n\\\\n3\\\\nd\\\\nd\\\\nd\\\\nd\\\\n\\\\nd\\\\n1\\\\n1\\\\n\\\\n3\\\\n1\\\\n2\\\\nd\\\\n\\\\n17 (IQR, 14e20)\\\\n\\\\n96 (IQR,\\\\n\\\\n31.75e121.5)\\\\n8,679 (IQR,\\\\n22,830 (IQR, 14,\\\\n4,085e50,109)\\\\n172e49,687)\\\\n99/107 (92.5%) 969/980 (98.8%)\\\\n\\\\nincluded articles\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\\\n\\\\nity; IQR, interquartile range.\\\\n\\\\na Some AHRQ reviews included more than one class of therapy.\\\\n\\\\nRecall of new relevant studies is shown in Table 2. The\\\\nupdated Cochrane reviews had 20 new studies identiﬁed by\\\\nthe review authors and added in the updates. Recall of these\\\\n20 ranged from 1.00 for the related articles method to a low\\\\nof 0.80 CQs (Table 2). Our team identiﬁed 277 new studies\\\\nas relevant for inclusion in the 10 evidence reports that\\\\nwere updated. All test searches showed lower recall for this\\\\ncohort than for the Cochrane reviews and here the CQ out-\\\\nperformed both ranking searches (Table 2).\\\\n\\\\n3.1. Recall of CQ combined with a ranking method\\\\n\\\\nOf the 297 new relevant studies identiﬁed, the combina-\\\\ntion of CQ and related articles searches identiﬁed 270\\\\n(overall\\\\nin\\\\nthe Cochrane cohort and 0.90 (250/277) in the AHRQ\\\\ncohort.\\\\n\\\\nrecall of 0.91). Recall was 1.00 (20/20)\\\\n\\\\nThe combination of CQ and SVM identiﬁed 263 studies\\\\n(overall recall of 0.89). Recall was 1.00 (20/20) in the\\\\nCochrane cohort and 0.88 (243/277) in the AHRQ cohort.\\\\nWhen all three methods were used together, recall was\\\\n0.997 with 296 of the 297 studies identiﬁed.\\\\n\\\\n\\\\x0c112\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nTable 2. Recall of eligible studies by the search methods\\\\n\\\\nCochrane reviews\\\\n\\\\nAHRQ evidence\\\\n\\\\nreports\\\\n\\\\nRetrieval method\\\\n\\\\nClinical query\\\\nRelated articles\\\\nSVM\\\\nTotal\\\\n\\\\nN\\\\n\\\\n16\\\\n20\\\\n19\\\\n20\\\\n\\\\nRecall\\\\n\\\\n0.80\\\\n1.00\\\\n0.95\\\\n\\\\nN\\\\n\\\\n188\\\\n176\\\\n129\\\\n277\\\\n\\\\nRecall\\\\n\\\\n0.68\\\\n0.64\\\\n0.47\\\\n\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\\\n\\\\nity; SVM, support vector machine.\\\\n\\\\nAcross the two cohorts, the overlap and unique component from\\\\nthe retrieval of relevant records by the test searches was examined\\\\n(Fig. 1).\\\\n\\\\n3.2. Consistency across clinical areas\\\\n\\\\nThirteen clinical areas were represented in the larger\\\\nstudy that included 72 journal-published reviews, but only\\\\neight of those clinical areas had two or more new relevant\\\\nstudies in the smaller Cochrane and AHRQ cohorts. Fig. 2\\\\nshows recall of new studies by the three types of searches,\\\\nfor these eight clinical areas.\\\\n\\\\nCombined recall of the CQ Boolean search paired with a\\\\nranking search is shown in Fig. 3. The combination showed\\\\ncomplete recall of relevant new studies in three of eight\\\\nareas when CQ was paired with related articles and recall\\\\nof 0.67 or higher in all areas. Four of eight clinical areas\\\\nhad complete recall for the combination of CQ and SVM,\\\\nand recall was 0.80 or higher in all areas. Even for periph-\\\\neral vascular disease, where all three searches performed\\\\n\\\\nFig. 2. Recall of new studies by clinical area for each search method.\\\\nAbbreviations: SVM, support vector machine.\\\\n\\\\nfairly poorly individually, recall was 0.76 for the CQ/\\\\nrelated articles and 0.90 in the CQ/SVM pairing.\\\\n\\\\n3.3. Search precision\\\\n\\\\nPrecision, across the 10 AHRQ reviews, was 0.11 for the\\\\nCQ, 0.22 for related articles, and 0.19 for SVM (Table 3).\\\\n\\\\nFig. 1. Overlapping and unique retrieval of relevant studies by each\\\\nsearch method. The size of the circle is proportional to the size of\\\\nthe retrieval, but the overlap and unique portions are approximations.\\\\nExact ﬁgures for the components are labeled. Abbreviations: SVM,\\\\nsupport vector machine.\\\\n\\\\nFig. 3. Recall of new studies by clinical area for search methods in\\\\ncombination. Abbreviations: CQ, clinical query; SVM, support vector\\\\nmachine.\\\\n\\\\n\\\\x0cTable 3. Overall precision in the AHRQ Sample\\\\n\\\\n4. Discussion\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n113\\\\n\\\\nRetrieval method\\\\n\\\\nClinical query\\\\nRelated articles\\\\nSVM\\\\nClinical query þ related\\\\nClinical query þ SVM\\\\nClinical query þ related\\\\n\\\\narticles\\\\n\\\\nEligible\\\\nstudies\\\\nretrieved\\\\n\\\\n187\\\\n176\\\\n128\\\\n250\\\\n\\\\n243\\\\n276\\\\n\\\\nNo. of candidates\\\\n\\\\nretrieved\\\\n\\\\nPrecision\\\\n\\\\n1,637\\\\n814\\\\n659\\\\n2,318a\\\\n\\\\n2,247a\\\\n3,264a\\\\n\\\\n0.11\\\\n0.22\\\\n0.19\\\\n0.11\\\\n\\\\n0.11\\\\n0.08\\\\n\\\\narticles þ SVM\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and\\\\n\\\\nQuality; SVM, support vector machine.\\\\n\\\\na Number of candidates after removal of duplicate records.\\\\n\\\\nPrecision for the individual reviews ranged from 0.0 to 0.38\\\\nfor CQs, 0.004 to 0.80 for related articles, and 0.01 to 0.43\\\\nfor SVM. Overall precision was 0.11 for CQ and either\\\\nrelated articles or SVM. Overall precision was 0.08 when\\\\nCQ and both related articles and SVM were used. For\\\\nranked searches where a ﬁxed number of records will be\\\\nscreened, that number is always the denominator, so preci-\\\\nsion will tend to increase as the number of relevant records\\\\nincreasesdthe maximum retrieval size for SVM was cap-\\\\nped at 200 records.\\\\n\\\\n3.4. Retrieval consistency over time\\\\n\\\\nSearches were retested for the AHRQ cohort using\\\\nPubMed results obtained February 24, 2015. Recall of the\\\\nrelated articles searches across the 10 AHRQ reports was\\\\n0.64 originally and 0.50 when repeated (Fig. 4). Of the\\\\nthree extreme cases, one rose from 0.00 to 0.80, one re-\\\\nmained at 0.00, and one dropped from 1.00 to 0.60. CQ\\\\nhad overall\\\\nrecall of 0.68 originally and 0.70 when\\\\nrepeated. Changes in extreme cases were minimal. Overall\\\\ncombined recall of CQ and related article was 0.90 origi-\\\\nnally,\\\\nfalling to 0.85 when repeated. Considering the\\\\nextreme values, all review with combined recall of 1.00\\\\noriginally remained at 1.00, whereas the one review with\\\\n0.00 recall in combination showed recall of 0.80 when\\\\nrepeated.\\\\n\\\\n1.0\\\\n0.9\\\\n0.8\\\\n0.7\\\\n0.6\\\\n0.5\\\\n0.4\\\\n0.3\\\\n0.2\\\\n0.1\\\\n0.0\\\\n\\\\nOriginal\\\\n\\\\n2015\\\\n\\\\nClinical Query\\\\n\\\\nRelated Ar(cid:415)cles\\\\n\\\\nCombined\\\\n\\\\nFig. 4. Recall of new studies for search methods, alone and in combi-\\\\nnation originally (left) and when retested in 2015 (right).\\\\n\\\\nWe expected that\\\\n\\\\nthe sophisticated SVM approach\\\\nwould perform well when paired with more stripped-\\\\ndown, focused Boolean searches. Indeed, the two together\\\\nwere able to replace the multidatabase, multimodel\\\\nsearches used by the original review teams in updating\\\\nthe Cochrane searches. The new relevant studies for the Co-\\\\nchrane reviews may have been relatively easy to ﬁnd, but\\\\nthe pairing was also effective in the AHRQ set, and there\\\\nshowed better precision than is usually seen with traditional\\\\nsearches for systematic review of RCTs [9]. The AHRQ set\\\\nwas formed from the new studies identiﬁed for more com-\\\\nplex interventions in a rigorous, well-funded study [16].\\\\n\\\\nWe were surprised that the related articles approach per-\\\\nformed almost as well as SVM when used in combination\\\\nwith the CQ. The related article search has advantages over\\\\nSVMdit requires far less data preparation, and no special\\\\nsoftware is needed for its use. This makes it useful not only\\\\nin updating, but also, if appropriate seed articles can be\\\\nfound, in original reviews.\\\\n\\\\nThat both SVM and related articles sometimes showed\\\\npoor recall when used alone, but consistently good recall\\\\nwhen used with a Boolean method suggests that there is\\\\nreal beneﬁt in using complementary search methods to\\\\nquerying MEDLINE. The related article method is not very\\\\ntime consuming, and easily added to other planned search\\\\nefforts, while the third method used here, SVM, is more\\\\ntechnical. The combined performance of all three methods\\\\nwas surprising, but such a setup might become unpractical\\\\nin day-to-day use.\\\\n\\\\nOther investigators have similar ﬁndings. Examination\\\\nof the supplemental material presented in the Appendix C\\\\nto the article by Waffenschmidt et al. [18] reveals that\\\\nacross the 19 reviews tested, the combination of a simple\\\\nstructured Boolean search (SSBS) and the ﬁrst 50 retrieval\\\\nof PubMed similar articles showed complete retrieval of all\\\\nreference standard articles in 14 of 19 reviews and never\\\\nless than 0.90 recall (Appendix C at www.jclinepi.com pre-\\\\nsents data from the table by Waffenschmidt graphically).\\\\nWaffenschmidt concluded ‘‘the combination of these two\\\\nsearch techniques that are independent of each other seems\\\\nto compensate the respective weaknesses.’’\\\\n\\\\nSSBS search by Waffenschmidt was constructed in\\\\nPubMed from search terms selected for the indication and\\\\nintervention with PubMed’s narrow CQ ﬁlter (category:\\\\ntherapy). Their search using the similar articles feature (Re-\\\\nlCits) did not use a set of seed articles; rather, the RelCits\\\\nfunction was applied for each relevant citation previously\\\\nidentiﬁed in PubMed. Their test articles were the included\\\\nstudies in 19 systematic reviews of drugs.\\\\n\\\\nAgoritsas et al. also described search construction\\\\nmethods for searches based on CQ and PubMed similar ar-\\\\nticles, tested for their ability to retrieve the included studies\\\\nof 30 Cochrane reviews [17]. Although their search con-\\\\nstruction methods differed from those used here, both\\\\n\\\\n\\\\x0c114\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nsearches were derived using standard methods. Their\\\\napproach to the structured Boolean search used terms from\\\\nthe population, intervention, and comparison with the CQ,\\\\nlimited to humans and English. Two clinicians selected the\\\\nPubMed similar articles seeds from the initial PubMed\\\\nretrieval. They noted that no one method provided consis-\\\\ntently high retrieval [17].\\\\n\\\\nThus, the robust nature of this pairing of Boolean and\\\\nnon-Boolean searches has been shown in several contexts,\\\\ngiving support\\\\nto the hypothesis that methods such as\\\\nrelated articles and SVM can compensate for the variation\\\\ninherent\\\\nin selecting search terms or assigning subject\\\\nheadings during indexing.\\\\n\\\\nConsidering stability of the searches over time, the\\\\nminimal change seen in the performance of the CQ is\\\\nlikely explained by indexing changes of a few records.\\\\nThe computation appears not to have changed. This sug-\\\\ngests that indexing changes may also impact SVM results\\\\nover time; however, this is likely to result in improved\\\\nperformance, as was seen with the CQ. The computation\\\\nof similar articles is still described by National Library\\\\nof Medicine as being based on the algorithm described\\\\nby Kim et al. [26]. That algorithm would suggest changes\\\\nover time in the nearest neighbor score as the frequency of\\\\ncertain terms in the MEDLINE corpus changes. A full\\\\nexploration of the changes in nearest neighbor scores of\\\\nsimilar articles over time is beyond the scope of this\\\\nstudy. It should be noted that when the similar articles\\\\nfeature was initially studied, it was simple to submit seed\\\\narticles, and then add limits such as date or RCT publica-\\\\ntion type. On retesting, such additional limits were more\\\\ncomplex to apply, and the elink utility seemed to be the\\\\nmost practical approach to identifying the related articles\\\\n[27].\\\\n\\\\nThe beneﬁt of the complementary searches, relative to\\\\nBoolean searching alone, was greatest for AHRQ evidence\\\\nreports and less for the simpler Cochrane reviews, suggest-\\\\ning that this approach may be particularly useful for more\\\\ncomplex evidence. There are examples of its utility in the\\\\nliterature. For example, in a realist review of a multidisci-\\\\nplinary body of\\\\nliterature identifying six domains of\\\\nClinical Practice Guideline implementability, the use of\\\\nPubMed’s similar articles feature as a third stage of search-\\\\ning identiﬁed 131 records of which 104 were relevant [28].\\\\nIn a systematic review of evidence on the links between\\\\npatient experience and clinical safety and effectiveness,\\\\nthe authors used PubMed similar articles to snowball on\\\\narticles\\\\nidentiﬁed through an EMBASE search to\\\\novercome the limitations of predeﬁned searches for com-\\\\nplex evidence [29].\\\\n\\\\nThere is always the possibility that a search, or even a\\\\npair of searches, will fail. One method to detect such\\\\nfailures is to test whether the search strategies ﬁnd known\\\\nrelevant items [30]. In the updating case, this is easily done\\\\nusing the included studies of the original review as a test\\\\n\\\\nset, allowing the review to determine the MEDLINE\\\\ncoverage of their particular topic at the same time.\\\\n\\\\n4.1. Limitations\\\\n\\\\nThere are two limitations to our proposed strategy. Other\\\\ndatabases should be searched in the unusual event that\\\\nnumerous studies, representing more than a small propor-\\\\ntion of the total N, are not included in MEDLINE. Second,\\\\nwhen it is important to ﬁnd articles too new to be indexed\\\\nby MEDLINE, systematic reviewers may wish to conduct a\\\\nsimple PubMed search limited to the nonindexed subsets\\\\n[31,32].\\\\n\\\\n5. Conclusion\\\\n\\\\nThe general approach of a Boolean plus a ranking\\\\nsearch is effective in MEDLINE retrieval for systematic\\\\nreviews. Very high levels of identiﬁcation of relevant\\\\nMEDLINE records, with adequate precision, are possible\\\\nusing a focused Boolean search complemented by a docu-\\\\nment similarity or ranking method. The efﬁcacy of a\\\\nfocused Boolean search paired with a search using the\\\\nPubMed similar articles feature is in agreement with pre-\\\\nvious studies [17,18], and this study shows that this com-\\\\nplementary effect also occurs when SVM is used as the\\\\nadditional method. The beneﬁt of using two complemen-\\\\ntary approaches to achieving high recall in MEDLINE is\\\\na robust effect.\\\\n\\\\nPubMed-related articles is a parsimonious method, as it\\\\nis readily available to all review teams without cost. The\\\\napproach is robust across clinical domains, and the effect\\\\nhas now been demonstrated in several samples. The method\\\\nmay be sufﬁcient for updating systematic reviews of inter-\\\\nventions and may be used for new reviews of interventions\\\\nwhen paired with a trials registry search. It is likely to work\\\\nin any type of search where MEDLINE provides good sub-\\\\nject coverage even if retrieval through traditional search\\\\nmethods has been challenging.\\\\n\\\\nAcknowledgments\\\\n\\\\nMohamed Ansari and Jun Ji undertook screening and\\\\nrelevance assessment of candidate studies retrieved by the\\\\ntest\\\\nsearches, Tamara Rader developed the Boolean\\\\nsearches, and Raymond Daniel processed related articles\\\\nsearches and obtained full-text articles. David Moher cosu-\\\\npervised Margaret Sampson doctoral dissertation and pro-\\\\nvided invaluable advice and guidance.\\\\n\\\\nSupplementary data\\\\n\\\\nSupplementary data related to this article can be found at\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004.\\\\n\\\\n\\\\x0c', 'Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nComplementary approaches to searching MEDLINE may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nMargaret Sampsona,*, Berry de Bruijnb, Christine Urquhartc, Kaveh Shojaniad\\\\naLibrary and Media Services, Children’s Hospital of Eastern Ontario, 401 Smyth Road, Ottawa, Ontario K1H 8L1, Canada\\\\n\\\\nbNational Research CouncildInformation and Communications Technologies Portfolio (NRC-ICT), 1200 Montreal Rd, bldg M-50, Ottawa, Ontario K1A\\\\n\\\\ncDepartment of Information Studies, Sunnybrook Health Sciences Centre, Aberystwyth University, Llanbadarn Campus, Aberystwyth, Wales SY23 3AL,\\\\n\\\\n0R6, Canada\\\\n\\\\nUnited Kingdom\\\\n\\\\ndCentre for Quality Improvement and Patient Safety, Room H468, 2075 Bayview Avenue Toronto, Ontario M4N 3M5, Canada\\\\n\\\\nAccepted 7 March 2016; Published online 11 March 2016\\\\n\\\\nAbstract\\\\n\\\\nObjectives: To maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming Boolean searches across multiple databases are common. Although MEDLINE provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through Boolean searches alone.\\\\n\\\\nStudy Design and Setting: Recall of one Boolean search method, the clinical query (CQ), combined with a ranking method, support\\\\nvector machine (SVM), or PubMed-related articles, was tested against a gold standard of studies added to 6 updated Cochrane reviews and\\\\n10 Agency for Healthcare Research and Quality (AHRQ) evidence reviews. For the AHRQ sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nResults: Recall of new studies was 0.69 for the CQ, 0.66 for related articles, 0.50 for SVM, 0.91 for the combination of CQ and related\\\\narticles, and 0.89 for the combination of CQ and SVM. Precision was 0.11 for CQ and related articles combined, and 0.11 for CQ and SVM\\\\ncombined. Related articles showed least stability over time.\\\\n\\\\nConclusions: The complementary combination of a Boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in MEDLINE. (cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nKeywords: Information retrieval; Systematic reviews; Support vector machine; Clinical query; PubMed similar articles; Searches; Updating; MEDLINE\\\\n\\\\n1. Introduction\\\\n\\\\nSystematic review searches need to have high recall.\\\\nMechanisms to achieve this usually include expansive\\\\nBoolean searches of multiple databases. This approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nMEDLINE gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. However,\\\\n\\\\nConﬂict of interest: None.\\\\nFunding: Some of this work was funded by the US Agency for Health-\\\\ncare Research and Quality, Department of Health and Human Services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* Corresponding author. Tel.: 613-737-7600.\\\\nE-mail address: msampson@cheo.on.ca (M. Sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 Elsevier Inc. All rights reserved.\\\\n\\\\nin 1994, a landmark article by Dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through MEDLINE. Recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in MEDLINE, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nSuccessful retrieval through a Boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. The search of\\\\nmultiple databases can therefore be helpful. The target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. A text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. These tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nWhat is new?\\\\n\\\\nKey ﬁndings\\\\n(cid:1) Searches using known relevant studies and the\\\\nsimilar articles feature of PubMed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) For a given question, if the Boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. The two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) The precision of\\\\n\\\\nmethod appears better\\\\nexhaustive Boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nWhat this adds to what was known?\\\\n(cid:1) The paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nIt is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nWhat is the implication and what should change\\\\nnow?\\\\n(cid:1) Using the simple and universally available PubMed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) If the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nMEDLINE.\\\\n\\\\nto large retrievals with low precision [9]. These resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional Boolean searches have\\\\nbeen sought [10].\\\\n\\\\nMost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nThese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. These methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nWe examined one method, support vector machine\\\\n(SVM), and compared it with a simple and readily available\\\\nmethod based on the PubMed similar articles feature. We\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. We paired both\\\\nwith a focused Boolean search within MEDLINE. We\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for SVM and seed articles for the PubMed-\\\\n\\\\nrelated articles search. We therefore sought to determine\\\\nif a focused Boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\nComparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. In this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (CQ), and PubMed-related articles are tested in a\\\\ncohort of six updated Cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 Agency for Healthcare\\\\nResearch and Quality (AHRQ) evidence reports. The\\\\nCochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. The replication in the\\\\nAHRQ cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. All records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nOther research [17,18] suggests that searches using the\\\\nPubMed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a Boolean-type search of\\\\nMEDLINE. We tested an additional search method, SVM,\\\\nin the Cochrane and AHRQ samples to permit comparison\\\\nwith our PubMed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the PubMed\\\\nsimilar articles method.\\\\n\\\\nThe aim of this article was to test whether the combined\\\\napproach of a focused Boolean search paired with a second\\\\nsearch using the similar articles feature of PubMed or SVM\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. Methods\\\\n\\\\n2.1. Formation of the study cohorts\\\\n\\\\nThis analysis uses a data set created for an updating\\\\nstudy sponsored by AHRQ [16]. Methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nBrieﬂy, Cochrane reviews were identiﬁed through a\\\\nsearch of the ACP Journal Club database (Ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or MEDLINE(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nAHRQ reports were identiﬁed through the PubMed\\\\nquery ‘‘Evid Rep Technol Assess (Summ)’’[Journal:__\\\\njrid21544]. Screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. Screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c110\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nFor inclusion, all reviews must have included random-\\\\nized controlled trials (RCTs) and provided meta-analysis\\\\nfor at least one outcome. The MEDLINE search strategies\\\\nhad to be reported in enough detail to permit replication.\\\\nFifteen AHRQ evidence reports thought\\\\nlikely to have\\\\nimportant new evidence were selected and used to validate\\\\nthe updating methods used in the main cohort.\\\\n\\\\nCochrane reviews were selected to meet minimum qual-\\\\nity and relevance standards, as deﬁned by ACP Journal\\\\nClub [19]. We also required that the review had been up-\\\\ndated and that the text of both the original review and an\\\\nupdated version was available. The search for the original\\\\nreview must have included MEDLINE and at least one\\\\nother electronic bibliographic database and one or more\\\\nnondatabase method such as hand searching or checking\\\\nreference lists. Such comprehensive searching was likely\\\\nto identify most relevant\\\\nliterature and form a useful\\\\ntraining set of included examples. At least 10 RCTs or\\\\nquasi-RCTs must have been included in the original review\\\\nto give an adequate training set for SVM.\\\\n\\\\n2.2. Test searches\\\\n\\\\n2.2.1. Clinical query\\\\n\\\\nBoolean searches were developed by a librarian experi-\\\\nenced in systematic review searches using a protocol\\\\n(Appendix A at www.jclinepi.com). Search strategies were\\\\ndeliberately simple, usually consisting of two or three\\\\nMedical Subject Heading (MeSH) terms representing the\\\\npopulation and intervention of the review. This search\\\\nwas limited by a CQ ﬁlter to ‘‘therapy (best balance of\\\\nsensitivity\\\\n‘‘Randomized\\\\ncontrolled trial.pt. or\\\\nrandomized.mp. or placebo.mp.’’\\\\n[20]. Boolean searches were run in OVID MEDLINE.\\\\n\\\\nspeciﬁcity)’’\\\\n\\\\nthat\\\\n\\\\nand\\\\n\\\\nis,\\\\n\\\\n2.2.2. Related articles\\\\n\\\\nSearches using the PubMed similar articles feature were\\\\nperformed using the PubMed unique identiﬁers (PMID) of\\\\nthe three newest and three largest studies included in the\\\\noriginal review as seed articles. There was no replacement\\\\nin the event of overlap between the largest and newest sets,\\\\nor if one of those studies was not indexed in MEDLINE.\\\\n(All studies included in the original reviews were checked\\\\nfor indexing status in MEDLINE.) The resulting set was\\\\nlimited to the publication type RCT and date limited to\\\\nthe period since the search date of the original review.\\\\n\\\\n2.2.3. Support vector machine\\\\n\\\\nThe SVM searches used for this project have not been\\\\npreviously described so the methods will be reported in\\\\nmore detail. SVM is a well established and broadly applied\\\\nclassiﬁcation method from machine learning [21,22]. A\\\\nclassiﬁcation task is closely related to an information\\\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\\\nthe collection. As SVM is a supervised algorithm,\\\\nit\\\\n\\\\nrequires training examples with known labels. It then places\\\\nthese training examples in a high-dimensional vector space\\\\nin such a way that examples of one class are separated from\\\\nexamples of the other class with the greatest distance to the\\\\nseparating boundary. Predicting the class of a new instance\\\\nhappens by placing it in the same space and determining on\\\\nwhich side of the boundary it falls. The distance to the\\\\nboundary is an indication of the prediction conﬁdence and\\\\nis used to rank instances (documents) as is essential in in-\\\\nformation retrieval tasks or to provide a cutoff threshold.\\\\n\\\\nIn our setup, the training set consisted of search results\\\\nfrom the original review formed with included instances\\\\nbeing the studies included in the review (the relevant re-\\\\ntrievals) and excluded instances being the studies found\\\\nby the search used in the original review but excluded from\\\\nthe review (irrelevant retrievals). The new examples to be\\\\nclassiﬁed were articles indexed since the date of the search\\\\nperformed in the original review.\\\\n\\\\nSVM searches were run on MEDLINE data stored\\\\nlocally at\\\\nthe National Research Council of Canada.\\\\nMEDLINE was refreshed before running the searches.\\\\nThe MEDLINE records were represented as bags of fea-\\\\ntures, where features consisted of lowercased words and\\\\nword combinations (up to four words) from the title and ab-\\\\nstract ﬁelds, full MeSH terms, and contents of all other\\\\nﬁelds in the MEDLINE record. Features were uniformly\\\\nweighted. The SVM implementation used was SVM Light\\\\n[23] used with a linear kernel.\\\\n\\\\nThe approach was piloted with one AHRQ evidence\\\\nreport, and various conﬁgurations were tried, observing\\\\nthe placement of several known new relevant studies within\\\\nthe retrieval. The MEDLINE set was ﬁltered with the\\\\nrevised Highly Sensitive Search Strategy (HSSS) [24]. Pre-\\\\nliminary testing indicated that this ﬁlter would retrieve\\\\n99.1% of MEDLINE-indexed relevant new evidence from\\\\nthe larger cohort of journal-published systematic reviews\\\\n(n 5 72) and Cochrane reviews (n 5 27). This HSSS ﬁlter\\\\nwas added primarily to improve processing speed, relative\\\\nto running the classiﬁer against all of MEDLINE. With\\\\nthe ﬁlter, processing for one systematic review took about\\\\n10 minutes. Next, a coarse-grained ﬁlter was applied;\\\\nincluded and excluded instances were combined in a single\\\\nset labeled POS. Adjacent PubMed IDs (the next higher\\\\nPMID to each member of POS, as long as that higher num-\\\\nber was not itself a member of POS) were combined in a set\\\\nto represent WORLD. The model was trained on POS vs.\\\\nWORLD, using words and phrases from title, abstract,\\\\nauthor names, and journal name. This model was run\\\\nagainst the records passing the HSSS ﬁlter.\\\\n\\\\nFour different subsets of\\\\n\\\\nthe SVM retrieval were\\\\nexamineddone set included all records with a relevance\\\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\\\n(SVM200point5). Other sets consisted of all records with\\\\nrelevance scores of 0.95 or more (SVM95), 0.90 or more\\\\n(SVM90), and 0.80 or more (SVM80). PubMed ID and\\\\nrelevance scores were recorded, and PubMed IDs were\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n111\\\\n\\\\nTable 1. Characteristics of the included reviews\\\\n\\\\nCochrane\\\\nreviews; N\\\\n\\\\nAHRQ evidence\\\\n\\\\nreports; N\\\\n\\\\nincorporated into the database of records for the reviewers\\\\nto screen. Only the results from SVM200point5 are re-\\\\nported here and are described as SVM.\\\\n\\\\n2.3. Determining performance of the test searches\\\\n\\\\nFor the updated Cochrane reviews, reference standard ar-\\\\nticles were those studies included in the updated review that\\\\nwere not in the original and which entered MEDLINE after\\\\nthe search date of the original review. For the AHRQ evi-\\\\ndence reports, the full retrieval set was screened. Records\\\\nfound relevant by the consensus of reviewers were consid-\\\\nered reference standard articles. Recall was the proportion\\\\nof reference standard articles identiﬁed by the search:\\\\n\\\\nNumber of reference standard articles found\\\\nTotal number of reference standard articles\\\\n\\\\nBioVenn software was used to analyze the overlap be-\\\\ntween the retrieval of relevant articles from the three\\\\nsearches and to create Venn diagrams [25].\\\\n\\\\nAll included reviews were classiﬁed into clinical area\\\\nbased on factors such as ISI journal classiﬁcation, the Co-\\\\nchrane Collaboration Review Group where the topic might\\\\nbe placed and the high level MeSH term under which the pop-\\\\nulation (condition) would be indexed. Performance of the\\\\nsearches in different clinical areas was displayed graphically\\\\nfor the searches both alone and in combinations, to allow ex-\\\\namination of differences in parallelism, level, and ﬂatness.\\\\n\\\\nPrecision is the proportion of all retrieved records that\\\\n\\\\nare relevant:\\\\n\\\\nNumber of reference standard articles found\\\\n\\\\nTotal number of records retrieved\\\\n\\\\nThe inverse of precision is the number needed to read to\\\\nﬁnd one eligible study, thus precision inﬂuences the work\\\\nof the review. Precision was calculated only for the AHRQ\\\\nevidence reviews. Because not all candidates retrieved by\\\\nthe Cochrane searches were assessed, precision could not\\\\nbe established in that sample.\\\\n\\\\n2.4. Determining stability of results over time\\\\n\\\\nThe CQ and related article searches, originally run in\\\\nMarch 2008, were repeated in February 2015 in the AHRQ\\\\ncohort. Recall of relevant items was compared with the\\\\noriginal retrievals to determine if any changes introduced\\\\nby National Library of Medicine might invalidate ﬁndings.\\\\nAs SVM conﬁguration is under the control of the investiga-\\\\ntors, it was not retested.\\\\n\\\\n3. Results\\\\n\\\\nSix Cochrane reviews and 10 AHRQ evidence reports\\\\nmet all inclusion criteria (Appendix B at www.jclinepi.\\\\ncom). Characteristics of the included reviews are shown\\\\nin Table 1.\\\\n\\\\n9a\\\\n1\\\\n3\\\\n\\\\n3\\\\n\\\\nd\\\\n1\\\\n\\\\nd\\\\n1\\\\n2\\\\n1\\\\n1\\\\n\\\\n1\\\\nd\\\\nd\\\\n\\\\nd\\\\n3\\\\n3\\\\n4\\\\n\\\\nCharacteristic\\\\n\\\\nTherapy evaluated\\\\n\\\\nMedications\\\\nMedical devices\\\\nProcedures\\\\n\\\\nClinical topic area\\\\n\\\\nCardiac and cardiovascular\\\\n\\\\nsystems\\\\n\\\\nCritical care\\\\nEndocrinology and\\\\n\\\\nmetabolism\\\\n\\\\nInfectious disease\\\\nClinical neurology\\\\nObstetrics and gynecology\\\\nOncology\\\\nPeripheral vascular\\\\n\\\\ndiseases\\\\nPsychiatry\\\\nRespiratory systems\\\\nUrology and nephrology\\\\n\\\\nPublication period\\\\n\\\\nMarch 1997eApril 1999\\\\nMay 1999eJune 2001\\\\nJuly 2001eAugust 2003\\\\nSeptember 2003e\\\\nDecember 2005\\\\n\\\\nMedian included trials\\\\n\\\\nMedian included\\\\n\\\\nparticipants\\\\n\\\\nMEDLINE coverage of\\\\n\\\\n5\\\\n1\\\\nd\\\\n\\\\nd\\\\n\\\\n1\\\\nd\\\\n\\\\n3\\\\nd\\\\nd\\\\nd\\\\nd\\\\n\\\\nd\\\\n1\\\\n1\\\\n\\\\n3\\\\n1\\\\n2\\\\nd\\\\n\\\\n17 (IQR, 14e20)\\\\n\\\\n96 (IQR,\\\\n\\\\n31.75e121.5)\\\\n8,679 (IQR,\\\\n22,830 (IQR, 14,\\\\n4,085e50,109)\\\\n172e49,687)\\\\n99/107 (92.5%) 969/980 (98.8%)\\\\n\\\\nincluded articles\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\\\n\\\\nity; IQR, interquartile range.\\\\n\\\\na Some AHRQ reviews included more than one class of therapy.\\\\n\\\\nRecall of new relevant studies is shown in Table 2. The\\\\nupdated Cochrane reviews had 20 new studies identiﬁed by\\\\nthe review authors and added in the updates. Recall of these\\\\n20 ranged from 1.00 for the related articles method to a low\\\\nof 0.80 CQs (Table 2). Our team identiﬁed 277 new studies\\\\nas relevant for inclusion in the 10 evidence reports that\\\\nwere updated. All test searches showed lower recall for this\\\\ncohort than for the Cochrane reviews and here the CQ out-\\\\nperformed both ranking searches (Table 2).\\\\n\\\\n3.1. Recall of CQ combined with a ranking method\\\\n\\\\nOf the 297 new relevant studies identiﬁed, the combina-\\\\ntion of CQ and related articles searches identiﬁed 270\\\\n(overall\\\\nin\\\\nthe Cochrane cohort and 0.90 (250/277) in the AHRQ\\\\ncohort.\\\\n\\\\nrecall of 0.91). Recall was 1.00 (20/20)\\\\n\\\\nThe combination of CQ and SVM identiﬁed 263 studies\\\\n(overall recall of 0.89). Recall was 1.00 (20/20) in the\\\\nCochrane cohort and 0.88 (243/277) in the AHRQ cohort.\\\\nWhen all three methods were used together, recall was\\\\n0.997 with 296 of the 297 studies identiﬁed.\\\\n\\\\n\\\\x0c112\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nTable 2. Recall of eligible studies by the search methods\\\\n\\\\nCochrane reviews\\\\n\\\\nAHRQ evidence\\\\n\\\\nreports\\\\n\\\\nRetrieval method\\\\n\\\\nClinical query\\\\nRelated articles\\\\nSVM\\\\nTotal\\\\n\\\\nN\\\\n\\\\n16\\\\n20\\\\n19\\\\n20\\\\n\\\\nRecall\\\\n\\\\n0.80\\\\n1.00\\\\n0.95\\\\n\\\\nN\\\\n\\\\n188\\\\n176\\\\n129\\\\n277\\\\n\\\\nRecall\\\\n\\\\n0.68\\\\n0.64\\\\n0.47\\\\n\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and Qual-\\\\n\\\\nity; SVM, support vector machine.\\\\n\\\\nAcross the two cohorts, the overlap and unique component from\\\\nthe retrieval of relevant records by the test searches was examined\\\\n(Fig. 1).\\\\n\\\\n3.2. Consistency across clinical areas\\\\n\\\\nThirteen clinical areas were represented in the larger\\\\nstudy that included 72 journal-published reviews, but only\\\\neight of those clinical areas had two or more new relevant\\\\nstudies in the smaller Cochrane and AHRQ cohorts. Fig. 2\\\\nshows recall of new studies by the three types of searches,\\\\nfor these eight clinical areas.\\\\n\\\\nCombined recall of the CQ Boolean search paired with a\\\\nranking search is shown in Fig. 3. The combination showed\\\\ncomplete recall of relevant new studies in three of eight\\\\nareas when CQ was paired with related articles and recall\\\\nof 0.67 or higher in all areas. Four of eight clinical areas\\\\nhad complete recall for the combination of CQ and SVM,\\\\nand recall was 0.80 or higher in all areas. Even for periph-\\\\neral vascular disease, where all three searches performed\\\\n\\\\nFig. 2. Recall of new studies by clinical area for each search method.\\\\nAbbreviations: SVM, support vector machine.\\\\n\\\\nfairly poorly individually, recall was 0.76 for the CQ/\\\\nrelated articles and 0.90 in the CQ/SVM pairing.\\\\n\\\\n3.3. Search precision\\\\n\\\\nPrecision, across the 10 AHRQ reviews, was 0.11 for the\\\\nCQ, 0.22 for related articles, and 0.19 for SVM (Table 3).\\\\n\\\\nFig. 1. Overlapping and unique retrieval of relevant studies by each\\\\nsearch method. The size of the circle is proportional to the size of\\\\nthe retrieval, but the overlap and unique portions are approximations.\\\\nExact ﬁgures for the components are labeled. Abbreviations: SVM,\\\\nsupport vector machine.\\\\n\\\\nFig. 3. Recall of new studies by clinical area for search methods in\\\\ncombination. Abbreviations: CQ, clinical query; SVM, support vector\\\\nmachine.\\\\n\\\\n\\\\x0cTable 3. Overall precision in the AHRQ Sample\\\\n\\\\n4. Discussion\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n113\\\\n\\\\nRetrieval method\\\\n\\\\nClinical query\\\\nRelated articles\\\\nSVM\\\\nClinical query þ related\\\\nClinical query þ SVM\\\\nClinical query þ related\\\\n\\\\narticles\\\\n\\\\nEligible\\\\nstudies\\\\nretrieved\\\\n\\\\n187\\\\n176\\\\n128\\\\n250\\\\n\\\\n243\\\\n276\\\\n\\\\nNo. of candidates\\\\n\\\\nretrieved\\\\n\\\\nPrecision\\\\n\\\\n1,637\\\\n814\\\\n659\\\\n2,318a\\\\n\\\\n2,247a\\\\n3,264a\\\\n\\\\n0.11\\\\n0.22\\\\n0.19\\\\n0.11\\\\n\\\\n0.11\\\\n0.08\\\\n\\\\narticles þ SVM\\\\nAbbreviations: AHRQ, Agency for Healthcare Research and\\\\n\\\\nQuality; SVM, support vector machine.\\\\n\\\\na Number of candidates after removal of duplicate records.\\\\n\\\\nPrecision for the individual reviews ranged from 0.0 to 0.38\\\\nfor CQs, 0.004 to 0.80 for related articles, and 0.01 to 0.43\\\\nfor SVM. Overall precision was 0.11 for CQ and either\\\\nrelated articles or SVM. Overall precision was 0.08 when\\\\nCQ and both related articles and SVM were used. For\\\\nranked searches where a ﬁxed number of records will be\\\\nscreened, that number is always the denominator, so preci-\\\\nsion will tend to increase as the number of relevant records\\\\nincreasesdthe maximum retrieval size for SVM was cap-\\\\nped at 200 records.\\\\n\\\\n3.4. Retrieval consistency over time\\\\n\\\\nSearches were retested for the AHRQ cohort using\\\\nPubMed results obtained February 24, 2015. Recall of the\\\\nrelated articles searches across the 10 AHRQ reports was\\\\n0.64 originally and 0.50 when repeated (Fig. 4). Of the\\\\nthree extreme cases, one rose from 0.00 to 0.80, one re-\\\\nmained at 0.00, and one dropped from 1.00 to 0.60. CQ\\\\nhad overall\\\\nrecall of 0.68 originally and 0.70 when\\\\nrepeated. Changes in extreme cases were minimal. Overall\\\\ncombined recall of CQ and related article was 0.90 origi-\\\\nnally,\\\\nfalling to 0.85 when repeated. Considering the\\\\nextreme values, all review with combined recall of 1.00\\\\noriginally remained at 1.00, whereas the one review with\\\\n0.00 recall in combination showed recall of 0.80 when\\\\nrepeated.\\\\n\\\\n1.0\\\\n0.9\\\\n0.8\\\\n0.7\\\\n0.6\\\\n0.5\\\\n0.4\\\\n0.3\\\\n0.2\\\\n0.1\\\\n0.0\\\\n\\\\nOriginal\\\\n\\\\n2015\\\\n\\\\nClinical Query\\\\n\\\\nRelated Ar(cid:415)cles\\\\n\\\\nCombined\\\\n\\\\nFig. 4. Recall of new studies for search methods, alone and in combi-\\\\nnation originally (left) and when retested in 2015 (right).\\\\n\\\\nWe expected that\\\\n\\\\nthe sophisticated SVM approach\\\\nwould perform well when paired with more stripped-\\\\ndown, focused Boolean searches. Indeed, the two together\\\\nwere able to replace the multidatabase, multimodel\\\\nsearches used by the original review teams in updating\\\\nthe Cochrane searches. The new relevant studies for the Co-\\\\nchrane reviews may have been relatively easy to ﬁnd, but\\\\nthe pairing was also effective in the AHRQ set, and there\\\\nshowed better precision than is usually seen with traditional\\\\nsearches for systematic review of RCTs [9]. The AHRQ set\\\\nwas formed from the new studies identiﬁed for more com-\\\\nplex interventions in a rigorous, well-funded study [16].\\\\n\\\\nWe were surprised that the related articles approach per-\\\\nformed almost as well as SVM when used in combination\\\\nwith the CQ. The related article search has advantages over\\\\nSVMdit requires far less data preparation, and no special\\\\nsoftware is needed for its use. This makes it useful not only\\\\nin updating, but also, if appropriate seed articles can be\\\\nfound, in original reviews.\\\\n\\\\nThat both SVM and related articles sometimes showed\\\\npoor recall when used alone, but consistently good recall\\\\nwhen used with a Boolean method suggests that there is\\\\nreal beneﬁt in using complementary search methods to\\\\nquerying MEDLINE. The related article method is not very\\\\ntime consuming, and easily added to other planned search\\\\nefforts, while the third method used here, SVM, is more\\\\ntechnical. The combined performance of all three methods\\\\nwas surprising, but such a setup might become unpractical\\\\nin day-to-day use.\\\\n\\\\nOther investigators have similar ﬁndings. Examination\\\\nof the supplemental material presented in the Appendix C\\\\nto the article by Waffenschmidt et al. [18] reveals that\\\\nacross the 19 reviews tested, the combination of a simple\\\\nstructured Boolean search (SSBS) and the ﬁrst 50 retrieval\\\\nof PubMed similar articles showed complete retrieval of all\\\\nreference standard articles in 14 of 19 reviews and never\\\\nless than 0.90 recall (Appendix C at www.jclinepi.com pre-\\\\nsents data from the table by Waffenschmidt graphically).\\\\nWaffenschmidt concluded ‘‘the combination of these two\\\\nsearch techniques that are independent of each other seems\\\\nto compensate the respective weaknesses.’’\\\\n\\\\nSSBS search by Waffenschmidt was constructed in\\\\nPubMed from search terms selected for the indication and\\\\nintervention with PubMed’s narrow CQ ﬁlter (category:\\\\ntherapy). Their search using the similar articles feature (Re-\\\\nlCits) did not use a set of seed articles; rather, the RelCits\\\\nfunction was applied for each relevant citation previously\\\\nidentiﬁed in PubMed. Their test articles were the included\\\\nstudies in 19 systematic reviews of drugs.\\\\n\\\\nAgoritsas et al. also described search construction\\\\nmethods for searches based on CQ and PubMed similar ar-\\\\nticles, tested for their ability to retrieve the included studies\\\\nof 30 Cochrane reviews [17]. Although their search con-\\\\nstruction methods differed from those used here, both\\\\n\\\\n\\\\x0c114\\\\n\\\\nM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\nsearches were derived using standard methods. Their\\\\napproach to the structured Boolean search used terms from\\\\nthe population, intervention, and comparison with the CQ,\\\\nlimited to humans and English. Two clinicians selected the\\\\nPubMed similar articles seeds from the initial PubMed\\\\nretrieval. They noted that no one method provided consis-\\\\ntently high retrieval [17].\\\\n\\\\nThus, the robust nature of this pairing of Boolean and\\\\nnon-Boolean searches has been shown in several contexts,\\\\ngiving support\\\\nto the hypothesis that methods such as\\\\nrelated articles and SVM can compensate for the variation\\\\ninherent\\\\nin selecting search terms or assigning subject\\\\nheadings during indexing.\\\\n\\\\nConsidering stability of the searches over time, the\\\\nminimal change seen in the performance of the CQ is\\\\nlikely explained by indexing changes of a few records.\\\\nThe computation appears not to have changed. This sug-\\\\ngests that indexing changes may also impact SVM results\\\\nover time; however, this is likely to result in improved\\\\nperformance, as was seen with the CQ. The computation\\\\nof similar articles is still described by National Library\\\\nof Medicine as being based on the algorithm described\\\\nby Kim et al. [26]. That algorithm would suggest changes\\\\nover time in the nearest neighbor score as the frequency of\\\\ncertain terms in the MEDLINE corpus changes. A full\\\\nexploration of the changes in nearest neighbor scores of\\\\nsimilar articles over time is beyond the scope of this\\\\nstudy. It should be noted that when the similar articles\\\\nfeature was initially studied, it was simple to submit seed\\\\narticles, and then add limits such as date or RCT publica-\\\\ntion type. On retesting, such additional limits were more\\\\ncomplex to apply, and the elink utility seemed to be the\\\\nmost practical approach to identifying the related articles\\\\n[27].\\\\n\\\\nThe beneﬁt of the complementary searches, relative to\\\\nBoolean searching alone, was greatest for AHRQ evidence\\\\nreports and less for the simpler Cochrane reviews, suggest-\\\\ning that this approach may be particularly useful for more\\\\ncomplex evidence. There are examples of its utility in the\\\\nliterature. For example, in a realist review of a multidisci-\\\\nplinary body of\\\\nliterature identifying six domains of\\\\nClinical Practice Guideline implementability, the use of\\\\nPubMed’s similar articles feature as a third stage of search-\\\\ning identiﬁed 131 records of which 104 were relevant [28].\\\\nIn a systematic review of evidence on the links between\\\\npatient experience and clinical safety and effectiveness,\\\\nthe authors used PubMed similar articles to snowball on\\\\narticles\\\\nidentiﬁed through an EMBASE search to\\\\novercome the limitations of predeﬁned searches for com-\\\\nplex evidence [29].\\\\n\\\\nThere is always the possibility that a search, or even a\\\\npair of searches, will fail. One method to detect such\\\\nfailures is to test whether the search strategies ﬁnd known\\\\nrelevant items [30]. In the updating case, this is easily done\\\\nusing the included studies of the original review as a test\\\\n\\\\nset, allowing the review to determine the MEDLINE\\\\ncoverage of their particular topic at the same time.\\\\n\\\\n4.1. Limitations\\\\n\\\\nThere are two limitations to our proposed strategy. Other\\\\ndatabases should be searched in the unusual event that\\\\nnumerous studies, representing more than a small propor-\\\\ntion of the total N, are not included in MEDLINE. Second,\\\\nwhen it is important to ﬁnd articles too new to be indexed\\\\nby MEDLINE, systematic reviewers may wish to conduct a\\\\nsimple PubMed search limited to the nonindexed subsets\\\\n[31,32].\\\\n\\\\n5. Conclusion\\\\n\\\\nThe general approach of a Boolean plus a ranking\\\\nsearch is effective in MEDLINE retrieval for systematic\\\\nreviews. Very high levels of identiﬁcation of relevant\\\\nMEDLINE records, with adequate precision, are possible\\\\nusing a focused Boolean search complemented by a docu-\\\\nment similarity or ranking method. The efﬁcacy of a\\\\nfocused Boolean search paired with a search using the\\\\nPubMed similar articles feature is in agreement with pre-\\\\nvious studies [17,18], and this study shows that this com-\\\\nplementary effect also occurs when SVM is used as the\\\\nadditional method. The beneﬁt of using two complemen-\\\\ntary approaches to achieving high recall in MEDLINE is\\\\na robust effect.\\\\n\\\\nPubMed-related articles is a parsimonious method, as it\\\\nis readily available to all review teams without cost. The\\\\napproach is robust across clinical domains, and the effect\\\\nhas now been demonstrated in several samples. The method\\\\nmay be sufﬁcient for updating systematic reviews of inter-\\\\nventions and may be used for new reviews of interventions\\\\nwhen paired with a trials registry search. It is likely to work\\\\nin any type of search where MEDLINE provides good sub-\\\\nject coverage even if retrieval through traditional search\\\\nmethods has been challenging.\\\\n\\\\nAcknowledgments\\\\n\\\\nMohamed Ansari and Jun Ji undertook screening and\\\\nrelevance assessment of candidate studies retrieved by the\\\\ntest\\\\nsearches, Tamara Rader developed the Boolean\\\\nsearches, and Raymond Daniel processed related articles\\\\nsearches and obtained full-text articles. David Moher cosu-\\\\npervised Margaret Sampson doctoral dissertation and pro-\\\\nvided invaluable advice and guidance.\\\\n\\\\nSupplementary data\\\\n\\\\nSupplementary data related to this article can be found at\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004.\\\\n\\\\n\\\\x0cM. Sampson et al. / Journal of Clinical Epidemiology 78 (2016) 108e115\\\\n\\\\n115\\\\n\\\\n\""
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"s: the complementary combination of a boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in medline. (cid:1) 2016 elsevier inc. all rights reserved.\\\\n\\\\nkeywords: information retrieval; systematic reviews; support vector machine; clinical query; pubmed similar articles; searches; updating; medline\\\\n\\\\n1. introduction\\\\n\\\\nsystematic review searches need to have high recall.\\\\nmechanisms to achieve this usually include expansive\\\\nboolean searches of multiple databases. this approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nmedline gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. however,\\\\n\\\\nconﬂict of interest: none.\\\\nfunding: some of this work was funded by the us agency for health-\\\\ncare research and quality, department of health and human services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* corresponding author. tel.: 613-737-7600.\\\\ne-mail address: msampson@cheo.on.ca (m. sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 elsevier inc. all rights reserved.\\\\n\\\\nin 1994, a landmark article by dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through medline. recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in medline, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nsuccessful retrieval through a boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. the search of\\\\nmultiple databases can therefore be helpful. the target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. a text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. these tactics on the part of the searcher lead\\\\n\\\\n\\\\x0c', 'journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\ncomplementary approaches to searching medline may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nmargaret sampsona,*, berry de bruijnb, christine urquhartc, kaveh shojaniad\\\\nalibrary and media services, children’s hospital of eastern ontario, 401 smyth road, ottawa, ontario k1h 8l1, canada\\\\n\\\\nbnational research councildinformation and communications technologies portfolio (nrc-ict), 1200 montreal rd, bldg m-50, ottawa, ontario k1a\\\\n\\\\ncdepartment of information studies, sunnybrook health sciences centre, aberystwyth university, llanbadarn campus, aberystwyth, wales sy23 3al,\\\\n\\\\n0r6, canada\\\\n\\\\nunited kingdom\\\\n\\\\ndcentre for quality improvement and patient safety, room h468, 2075 bayview avenue toronto, ontario m4n 3m5, canada\\\\n\\\\naccepted 7 march 2016; published online 11 march 2016\\\\n\\\\nabstract\\\\n\\\\nobjectives: to maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming boolean searches across multiple databases are common. although medline provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through boolean searches alone.\\\\n\\\\nstudy design and setting: recall of one boolean search method, the clinical query (cq), combined with a ranking method, support\\\\nvector machine (svm), or pubmed-related articles, was tested against a gold standard of studies added to 6 updated cochrane reviews and\\\\n10 agency for healthcare research and quality (ahrq) evidence reviews. for the ahrq sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nresults: recall of new studies was 0.69 for the cq, 0.66 for related articles, 0.50 for svm, 0.91 for the combination of cq and related\\\\narticles, and 0.89 for the combination of cq and svm. precision was 0.11 for cq and related articles combined, and 0.11 for cq and svm\\\\ncombined. related articles showed least stability over time.\\\\n\\\\nconclusions: the complementary combination of a boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in medline. (cid:1) 2016 elsevier inc. all rights reserved.\\\\n\\\\nkeywords: information retrieval; systematic reviews; support vector machine; clinical query; pubmed similar articles; searches; updating; medline\\\\n\\\\n1. introduction\\\\n\\\\nsystematic review searches need to have high recall.\\\\nmechanisms to achieve this usually include expansive\\\\nboolean searches of multiple databases. this approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nmedline gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. however,\\\\n\\\\nconﬂict of interest: none.\\\\nfunding: some of this work was funded by the us agency for health-\\\\ncare research and quality, department of health and human services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* corresponding author. tel.: 613-737-7600.\\\\ne-mail address: msampson@cheo.on.ca (m. sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 elsevier inc. all rights reserved.\\\\n\\\\nin 1994, a landmark article by dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through medline. recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in medline, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nsuccessful retrieval through a boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. the search of\\\\nmultiple databases can therefore be helpful. the target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. a text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. these tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nwhat is new?\\\\n\\\\nkey ﬁndings\\\\n(cid:1) searches using known relevant studies and the\\\\nsimilar articles feature of pubmed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) for a given question, if the boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. the two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) the precision of\\\\n\\\\nmethod appears better\\\\nexhaustive boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nwhat this adds to what was known?\\\\n(cid:1) the paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nit is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nwhat is the implication and what should change\\\\nnow?\\\\n(cid:1) using the simple and universally available pubmed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) if the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nmedline.\\\\n\\\\nto large retrievals with low precision [9]. these resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional boolean searches have\\\\nbeen sought [10].\\\\n\\\\nmost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nthese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. these methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nwe examined one method, support vector machine\\\\n(svm), and compared it with a simple and readily available\\\\nmethod based on the pubmed similar articles feature. we\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. we paired both\\\\nwith a focused boolean search within medline. we\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for svm and seed articles for the pubmed-\\\\n\\\\nrelated articles search. we therefore sought to determine\\\\nif a focused boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\ncomparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. in this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (cq), and pubmed-related articles are tested in a\\\\ncohort of six updated cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 agency for healthcare\\\\nresearch and quality (ahrq) evidence reports. the\\\\ncochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. the replication in the\\\\nahrq cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. all records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nother research [17,18] suggests that searches using the\\\\npubmed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a boolean-type search of\\\\nmedline. we tested an additional search method, svm,\\\\nin the cochrane and ahrq samples to permit comparison\\\\nwith our pubmed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the pubmed\\\\nsimilar articles method.\\\\n\\\\nthe aim of this article was to test whether the combined\\\\napproach of a focused boolean search paired with a second\\\\nsearch using the similar articles feature of pubmed or svm\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. methods\\\\n\\\\n2.1. formation of the study cohorts\\\\n\\\\nthis analysis uses a data set created for an updating\\\\nstudy sponsored by ahrq [16]. methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nbrieﬂy, cochrane reviews were identiﬁed through a\\\\nsearch of the acp journal club database (ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or medline(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nahrq reports were identiﬁed through the pubmed\\\\nquery ‘‘evid rep technol assess (summ)’’[journal:__\\\\njrid21544]. screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c', 'journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\ncomplementary approaches to searching medline may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nmargaret sampsona,*, berry de bruijnb, christine urquhartc, kaveh shojaniad\\\\nalibrary and media services, children’s hospital of eastern ontario, 401 smyth road, ottawa, ontario k1h 8l1, canada\\\\n\\\\nbnational research councildinformation and communications technologies portfolio (nrc-ict), 1200 montreal rd, bldg m-50, ottawa, ontario k1a\\\\n\\\\ncdepartment of information studies, sunnybrook health sciences centre, aberystwyth university, llanbadarn campus, aberystwyth, wales sy23 3al,\\\\n\\\\n0r6, canada\\\\n\\\\nunited kingdom\\\\n\\\\ndcentre for quality improvement and patient safety, room h468, 2075 bayview avenue toronto, ontario m4n 3m5, canada\\\\n\\\\naccepted 7 march 2016; published online 11 march 2016\\\\n\\\\nabstract\\\\n\\\\nobjectives: to maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming boolean searches across multiple databases are common. although medline provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through boolean searches alone.\\\\n\\\\nstudy design and setting: recall of one boolean search method, the clinical query (cq), combined with a ranking method, support\\\\nvector machine (svm), or pubmed-related articles, was tested against a gold standard of studies added to 6 updated cochrane reviews and\\\\n10 agency for healthcare research and quality (ahrq) evidence reviews. for the ahrq sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nresults: recall of new studies was 0.69 for the cq, 0.66 for related articles, 0.50 for svm, 0.91 for the combination of cq and related\\\\narticles, and 0.89 for the combination of cq and svm. precision was 0.11 for cq and related articles combined, and 0.11 for cq and svm\\\\ncombined. related articles showed least stability over time.\\\\n\\\\nconclusions: the complementary combination of a boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in medline. (cid:1) 2016 elsevier inc. all rights reserved.\\\\n\\\\nkeywords: information retrieval; systematic reviews; support vector machine; clinical query; pubmed similar articles; searches; updating; medline\\\\n\\\\n1. introduction\\\\n\\\\nsystematic review searches need to have high recall.\\\\nmechanisms to achieve this usually include expansive\\\\nboolean searches of multiple databases. this approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nmedline gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. however,\\\\n\\\\nconﬂict of interest: none.\\\\nfunding: some of this work was funded by the us agency for health-\\\\ncare research and quality, department of health and human services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* corresponding author. tel.: 613-737-7600.\\\\ne-mail address: msampson@cheo.on.ca (m. sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 elsevier inc. all rights reserved.\\\\n\\\\nin 1994, a landmark article by dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through medline. recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in medline, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nsuccessful retrieval through a boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. the search of\\\\nmultiple databases can therefore be helpful. the target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. a text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. these tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nwhat is new?\\\\n\\\\nkey ﬁndings\\\\n(cid:1) searches using known relevant studies and the\\\\nsimilar articles feature of pubmed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) for a given question, if the boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. the two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) the precision of\\\\n\\\\nmethod appears better\\\\nexhaustive boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nwhat this adds to what was known?\\\\n(cid:1) the paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nit is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nwhat is the implication and what should change\\\\nnow?\\\\n(cid:1) using the simple and universally available pubmed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) if the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nmedline.\\\\n\\\\nto large retrievals with low precision [9]. these resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional boolean searches have\\\\nbeen sought [10].\\\\n\\\\nmost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nthese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. these methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nwe examined one method, support vector machine\\\\n(svm), and compared it with a simple and readily available\\\\nmethod based on the pubmed similar articles feature. we\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. we paired both\\\\nwith a focused boolean search within medline. we\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for svm and seed articles for the pubmed-\\\\n\\\\nrelated articles search. we therefore sought to determine\\\\nif a focused boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\ncomparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. in this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (cq), and pubmed-related articles are tested in a\\\\ncohort of six updated cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 agency for healthcare\\\\nresearch and quality (ahrq) evidence reports. the\\\\ncochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. the replication in the\\\\nahrq cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. all records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nother research [17,18] suggests that searches using the\\\\npubmed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a boolean-type search of\\\\nmedline. we tested an additional search method, svm,\\\\nin the cochrane and ahrq samples to permit comparison\\\\nwith our pubmed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the pubmed\\\\nsimilar articles method.\\\\n\\\\nthe aim of this article was to test whether the combined\\\\napproach of a focused boolean search paired with a second\\\\nsearch using the similar articles feature of pubmed or svm\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. methods\\\\n\\\\n2.1. formation of the study cohorts\\\\n\\\\nthis analysis uses a data set created for an updating\\\\nstudy sponsored by ahrq [16]. methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nbrieﬂy, cochrane reviews were identiﬁed through a\\\\nsearch of the acp journal club database (ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or medline(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nahrq reports were identiﬁed through the pubmed\\\\nquery ‘‘evid rep technol assess (summ)’’[journal:__\\\\njrid21544]. screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c110\\\\n\\\\nm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\nfor inclusion, all reviews must have included random-\\\\nized controlled trials (rcts) and provided meta-analysis\\\\nfor at least one outcome. the medline search strategies\\\\nhad to be reported in enough detail to permit replication.\\\\nfifteen ahrq evidence reports thought\\\\nlikely to have\\\\nimportant new evidence were selected and used to validate\\\\nthe updating methods used in the main cohort.\\\\n\\\\ncochrane reviews were selected to meet minimum qual-\\\\nity and relevance standards, as deﬁned by acp journal\\\\nclub [19]. we also required that the review had been up-\\\\ndated and that the text of both the original review and an\\\\nupdated version was available. the search for the original\\\\nreview must have included medline and at least one\\\\nother electronic bibliographic database and one or more\\\\nnondatabase method such as hand searching or checking\\\\nreference lists. such comprehensive searching was likely\\\\nto identify most relevant\\\\nliterature and form a useful\\\\ntraining set of included examples. at least 10 rcts or\\\\nquasi-rcts must have been included in the original review\\\\nto give an adequate training set for svm.\\\\n\\\\n2.2. test searches\\\\n\\\\n2.2.1. clinical query\\\\n\\\\nboolean searches were developed by a librarian experi-\\\\nenced in systematic review searches using a protocol\\\\n(appendix a at www.jclinepi.com). search strategies were\\\\ndeliberately simple, usually consisting of two or three\\\\nmedical subject heading (mesh) terms representing the\\\\npopulation and intervention of the review. this search\\\\nwas limited by a cq ﬁlter to ‘‘therapy (best balance of\\\\nsensitivity\\\\n‘‘randomized\\\\ncontrolled trial.pt. or\\\\nrandomized.mp. or placebo.mp.’’\\\\n[20]. boolean searches were run in ovid medline.\\\\n\\\\nspeciﬁcity)’’\\\\n\\\\nthat\\\\n\\\\nand\\\\n\\\\nis,\\\\n\\\\n2.2.2. related articles\\\\n\\\\nsearches using the pubmed similar articles feature were\\\\nperformed using the pubmed unique identiﬁers (pmid) of\\\\nthe three newest and three largest studies included in the\\\\noriginal review as seed articles. there was no replacement\\\\nin the event of overlap between the largest and newest sets,\\\\nor if one of those studies was not indexed in medline.\\\\n(all studies included in the original reviews were checked\\\\nfor indexing status in medline.) the resulting set was\\\\nlimited to the publication type rct and date limited to\\\\nthe period since the search date of the original review.\\\\n\\\\n2.2.3. support vector machine\\\\n\\\\nthe svm searches used for this project have not been\\\\npreviously described so the methods will be reported in\\\\nmore detail. svm is a well established and broadly applied\\\\nclassiﬁcation method from machine learning [21,22]. a\\\\nclassiﬁcation task is closely related to an information\\\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\\\nthe collection. as svm is a supervised algorithm,\\\\nit\\\\n\\\\nrequires training examples with known labels. it then places\\\\nthese training examples in a high-dimensional vector space\\\\nin such a way that examples of one class are separated from\\\\nexamples of the other class with the greatest distance to the\\\\nseparating boundary. predicting the class of a new instance\\\\nhappens by placing it in the same space and determining on\\\\nwhich side of the boundary it falls. the distance to the\\\\nboundary is an indication of the prediction conﬁdence and\\\\nis used to rank instances (documents) as is essential in in-\\\\nformation retrieval tasks or to provide a cutoff threshold.\\\\n\\\\nin our setup, the training set consisted of search results\\\\nfrom the original review formed with included instances\\\\nbeing the studies included in the review (the relevant re-\\\\ntrievals) and excluded instances being the studies found\\\\nby the search used in the original review but excluded from\\\\nthe review (irrelevant retrievals). the new examples to be\\\\nclassiﬁed were articles indexed since the date of the search\\\\nperformed in the original review.\\\\n\\\\nsvm searches were run on medline data stored\\\\nlocally at\\\\nthe national research council of canada.\\\\nmedline was refreshed before running the searches.\\\\nthe medline records were represented as bags of fea-\\\\ntures, where features consisted of lowercased words and\\\\nword combinations (up to four words) from the title and ab-\\\\nstract ﬁelds, full mesh terms, and contents of all other\\\\nﬁelds in the medline record. features were uniformly\\\\nweighted. the svm implementation used was svm light\\\\n[23] used with a linear kernel.\\\\n\\\\nthe approach was piloted with one ahrq evidence\\\\nreport, and various conﬁgurations were tried, observing\\\\nthe placement of several known new relevant studies within\\\\nthe retrieval. the medline set was ﬁltered with the\\\\nrevised highly sensitive search strategy (hsss) [24]. pre-\\\\nliminary testing indicated that this ﬁlter would retrieve\\\\n99.1% of medline-indexed relevant new evidence from\\\\nthe larger cohort of journal-published systematic reviews\\\\n(n 5 72) and cochrane reviews (n 5 27). this hsss ﬁlter\\\\nwas added primarily to improve processing speed, relative\\\\nto running the classiﬁer against all of medline. with\\\\nthe ﬁlter, processing for one systematic review took about\\\\n10 minutes. next, a coarse-grained ﬁlter was applied;\\\\nincluded and excluded instances were combined in a single\\\\nset labeled pos. adjacent pubmed ids (the next higher\\\\npmid to each member of pos, as long as that higher num-\\\\nber was not itself a member of pos) were combined in a set\\\\nto represent world. the model was trained on pos vs.\\\\nworld, using words and phrases from title, abstract,\\\\nauthor names, and journal name. this model was run\\\\nagainst the records passing the hsss ﬁlter.\\\\n\\\\nfour different subsets of\\\\n\\\\nthe svm retrieval were\\\\nexamineddone set included all records with a relevance\\\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\\\n(svm200point5). other sets consisted of all records with\\\\nrelevance scores of 0.95 or more (svm95), 0.90 or more\\\\n(svm90), and 0.80 or more (svm80). pubmed id and\\\\nrelevance scores were recorded, and pubmed ids were\\\\n\\\\n\\\\x0c', 'journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\ncomplementary approaches to searching medline may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nmargaret sampsona,*, berry de bruijnb, christine urquhartc, kaveh shojaniad\\\\nalibrary and media services, children’s hospital of eastern ontario, 401 smyth road, ottawa, ontario k1h 8l1, canada\\\\n\\\\nbnational research councildinformation and communications technologies portfolio (nrc-ict), 1200 montreal rd, bldg m-50, ottawa, ontario k1a\\\\n\\\\ncdepartment of information studies, sunnybrook health sciences centre, aberystwyth university, llanbadarn campus, aberystwyth, wales sy23 3al,\\\\n\\\\n0r6, canada\\\\n\\\\nunited kingdom\\\\n\\\\ndcentre for quality improvement and patient safety, room h468, 2075 bayview avenue toronto, ontario m4n 3m5, canada\\\\n\\\\naccepted 7 march 2016; published online 11 march 2016\\\\n\\\\nabstract\\\\n\\\\nobjectives: to maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming boolean searches across multiple databases are common. although medline provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through boolean searches alone.\\\\n\\\\nstudy design and setting: recall of one boolean search method, the clinical query (cq), combined with a ranking method, support\\\\nvector machine (svm), or pubmed-related articles, was tested against a gold standard of studies added to 6 updated cochrane reviews and\\\\n10 agency for healthcare research and quality (ahrq) evidence reviews. for the ahrq sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nresults: recall of new studies was 0.69 for the cq, 0.66 for related articles, 0.50 for svm, 0.91 for the combination of cq and related\\\\narticles, and 0.89 for the combination of cq and svm. precision was 0.11 for cq and related articles combined, and 0.11 for cq and svm\\\\ncombined. related articles showed least stability over time.\\\\n\\\\nconclusions: the complementary combination of a boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in medline. (cid:1) 2016 elsevier inc. all rights reserved.\\\\n\\\\nkeywords: information retrieval; systematic reviews; support vector machine; clinical query; pubmed similar articles; searches; updating; medline\\\\n\\\\n1. introduction\\\\n\\\\nsystematic review searches need to have high recall.\\\\nmechanisms to achieve this usually include expansive\\\\nboolean searches of multiple databases. this approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nmedline gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. however,\\\\n\\\\nconﬂict of interest: none.\\\\nfunding: some of this work was funded by the us agency for health-\\\\ncare research and quality, department of health and human services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* corresponding author. tel.: 613-737-7600.\\\\ne-mail address: msampson@cheo.on.ca (m. sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 elsevier inc. all rights reserved.\\\\n\\\\nin 1994, a landmark article by dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through medline. recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in medline, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nsuccessful retrieval through a boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. the search of\\\\nmultiple databases can therefore be helpful. the target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. a text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. these tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nwhat is new?\\\\n\\\\nkey ﬁndings\\\\n(cid:1) searches using known relevant studies and the\\\\nsimilar articles feature of pubmed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) for a given question, if the boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. the two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) the precision of\\\\n\\\\nmethod appears better\\\\nexhaustive boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nwhat this adds to what was known?\\\\n(cid:1) the paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nit is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nwhat is the implication and what should change\\\\nnow?\\\\n(cid:1) using the simple and universally available pubmed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) if the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nmedline.\\\\n\\\\nto large retrievals with low precision [9]. these resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional boolean searches have\\\\nbeen sought [10].\\\\n\\\\nmost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nthese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. these methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nwe examined one method, support vector machine\\\\n(svm), and compared it with a simple and readily available\\\\nmethod based on the pubmed similar articles feature. we\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. we paired both\\\\nwith a focused boolean search within medline. we\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for svm and seed articles for the pubmed-\\\\n\\\\nrelated articles search. we therefore sought to determine\\\\nif a focused boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\ncomparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. in this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (cq), and pubmed-related articles are tested in a\\\\ncohort of six updated cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 agency for healthcare\\\\nresearch and quality (ahrq) evidence reports. the\\\\ncochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. the replication in the\\\\nahrq cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. all records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nother research [17,18] suggests that searches using the\\\\npubmed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a boolean-type search of\\\\nmedline. we tested an additional search method, svm,\\\\nin the cochrane and ahrq samples to permit comparison\\\\nwith our pubmed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the pubmed\\\\nsimilar articles method.\\\\n\\\\nthe aim of this article was to test whether the combined\\\\napproach of a focused boolean search paired with a second\\\\nsearch using the similar articles feature of pubmed or svm\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. methods\\\\n\\\\n2.1. formation of the study cohorts\\\\n\\\\nthis analysis uses a data set created for an updating\\\\nstudy sponsored by ahrq [16]. methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nbrieﬂy, cochrane reviews were identiﬁed through a\\\\nsearch of the acp journal club database (ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or medline(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nahrq reports were identiﬁed through the pubmed\\\\nquery ‘‘evid rep technol assess (summ)’’[journal:__\\\\njrid21544]. screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c110\\\\n\\\\nm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\nfor inclusion, all reviews must have included random-\\\\nized controlled trials (rcts) and provided meta-analysis\\\\nfor at least one outcome. the medline search strategies\\\\nhad to be reported in enough detail to permit replication.\\\\nfifteen ahrq evidence reports thought\\\\nlikely to have\\\\nimportant new evidence were selected and used to validate\\\\nthe updating methods used in the main cohort.\\\\n\\\\ncochrane reviews were selected to meet minimum qual-\\\\nity and relevance standards, as deﬁned by acp journal\\\\nclub [19]. we also required that the review had been up-\\\\ndated and that the text of both the original review and an\\\\nupdated version was available. the search for the original\\\\nreview must have included medline and at least one\\\\nother electronic bibliographic database and one or more\\\\nnondatabase method such as hand searching or checking\\\\nreference lists. such comprehensive searching was likely\\\\nto identify most relevant\\\\nliterature and form a useful\\\\ntraining set of included examples. at least 10 rcts or\\\\nquasi-rcts must have been included in the original review\\\\nto give an adequate training set for svm.\\\\n\\\\n2.2. test searches\\\\n\\\\n2.2.1. clinical query\\\\n\\\\nboolean searches were developed by a librarian experi-\\\\nenced in systematic review searches using a protocol\\\\n(appendix a at www.jclinepi.com). search strategies were\\\\ndeliberately simple, usually consisting of two or three\\\\nmedical subject heading (mesh) terms representing the\\\\npopulation and intervention of the review. this search\\\\nwas limited by a cq ﬁlter to ‘‘therapy (best balance of\\\\nsensitivity\\\\n‘‘randomized\\\\ncontrolled trial.pt. or\\\\nrandomized.mp. or placebo.mp.’’\\\\n[20]. boolean searches were run in ovid medline.\\\\n\\\\nspeciﬁcity)’’\\\\n\\\\nthat\\\\n\\\\nand\\\\n\\\\nis,\\\\n\\\\n2.2.2. related articles\\\\n\\\\nsearches using the pubmed similar articles feature were\\\\nperformed using the pubmed unique identiﬁers (pmid) of\\\\nthe three newest and three largest studies included in the\\\\noriginal review as seed articles. there was no replacement\\\\nin the event of overlap between the largest and newest sets,\\\\nor if one of those studies was not indexed in medline.\\\\n(all studies included in the original reviews were checked\\\\nfor indexing status in medline.) the resulting set was\\\\nlimited to the publication type rct and date limited to\\\\nthe period since the search date of the original review.\\\\n\\\\n2.2.3. support vector machine\\\\n\\\\nthe svm searches used for this project have not been\\\\npreviously described so the methods will be reported in\\\\nmore detail. svm is a well established and broadly applied\\\\nclassiﬁcation method from machine learning [21,22]. a\\\\nclassiﬁcation task is closely related to an information\\\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\\\nthe collection. as svm is a supervised algorithm,\\\\nit\\\\n\\\\nrequires training examples with known labels. it then places\\\\nthese training examples in a high-dimensional vector space\\\\nin such a way that examples of one class are separated from\\\\nexamples of the other class with the greatest distance to the\\\\nseparating boundary. predicting the class of a new instance\\\\nhappens by placing it in the same space and determining on\\\\nwhich side of the boundary it falls. the distance to the\\\\nboundary is an indication of the prediction conﬁdence and\\\\nis used to rank instances (documents) as is essential in in-\\\\nformation retrieval tasks or to provide a cutoff threshold.\\\\n\\\\nin our setup, the training set consisted of search results\\\\nfrom the original review formed with included instances\\\\nbeing the studies included in the review (the relevant re-\\\\ntrievals) and excluded instances being the studies found\\\\nby the search used in the original review but excluded from\\\\nthe review (irrelevant retrievals). the new examples to be\\\\nclassiﬁed were articles indexed since the date of the search\\\\nperformed in the original review.\\\\n\\\\nsvm searches were run on medline data stored\\\\nlocally at\\\\nthe national research council of canada.\\\\nmedline was refreshed before running the searches.\\\\nthe medline records were represented as bags of fea-\\\\ntures, where features consisted of lowercased words and\\\\nword combinations (up to four words) from the title and ab-\\\\nstract ﬁelds, full mesh terms, and contents of all other\\\\nﬁelds in the medline record. features were uniformly\\\\nweighted. the svm implementation used was svm light\\\\n[23] used with a linear kernel.\\\\n\\\\nthe approach was piloted with one ahrq evidence\\\\nreport, and various conﬁgurations were tried, observing\\\\nthe placement of several known new relevant studies within\\\\nthe retrieval. the medline set was ﬁltered with the\\\\nrevised highly sensitive search strategy (hsss) [24]. pre-\\\\nliminary testing indicated that this ﬁlter would retrieve\\\\n99.1% of medline-indexed relevant new evidence from\\\\nthe larger cohort of journal-published systematic reviews\\\\n(n 5 72) and cochrane reviews (n 5 27). this hsss ﬁlter\\\\nwas added primarily to improve processing speed, relative\\\\nto running the classiﬁer against all of medline. with\\\\nthe ﬁlter, processing for one systematic review took about\\\\n10 minutes. next, a coarse-grained ﬁlter was applied;\\\\nincluded and excluded instances were combined in a single\\\\nset labeled pos. adjacent pubmed ids (the next higher\\\\npmid to each member of pos, as long as that higher num-\\\\nber was not itself a member of pos) were combined in a set\\\\nto represent world. the model was trained on pos vs.\\\\nworld, using words and phrases from title, abstract,\\\\nauthor names, and journal name. this model was run\\\\nagainst the records passing the hsss ﬁlter.\\\\n\\\\nfour different subsets of\\\\n\\\\nthe svm retrieval were\\\\nexamineddone set included all records with a relevance\\\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\\\n(svm200point5). other sets consisted of all records with\\\\nrelevance scores of 0.95 or more (svm95), 0.90 or more\\\\n(svm90), and 0.80 or more (svm80). pubmed id and\\\\nrelevance scores were recorded, and pubmed ids were\\\\n\\\\n\\\\x0cm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\n111\\\\n\\\\ntable 1. characteristics of the included reviews\\\\n\\\\ncochrane\\\\nreviews; n\\\\n\\\\nahrq evidence\\\\n\\\\nreports; n\\\\n\\\\nincorporated into the database of records for the reviewers\\\\nto screen. only the results from svm200point5 are re-\\\\nported here and are described as svm.\\\\n\\\\n2.3. determining performance of the test searches\\\\n\\\\nfor the updated cochrane reviews, reference standard ar-\\\\nticles were those studies included in the updated review that\\\\nwere not in the original and which entered medline after\\\\nthe search date of the original review. for the ahrq evi-\\\\ndence reports, the full retrieval set was screened. records\\\\nfound relevant by the consensus of reviewers were consid-\\\\nered reference standard articles. recall was the proportion\\\\nof reference standard articles identiﬁed by the search:\\\\n\\\\nnumber of reference standard articles found\\\\ntotal number of reference standard articles\\\\n\\\\nbiovenn software was used to analyze the overlap be-\\\\ntween the retrieval of relevant articles from the three\\\\nsearches and to create venn diagrams [25].\\\\n\\\\nall included reviews were classiﬁed into clinical area\\\\nbased on factors such as isi journal classiﬁcation, the co-\\\\nchrane collaboration review group where the topic might\\\\nbe placed and the high level mesh term under which the pop-\\\\nulation (condition) would be indexed. performance of the\\\\nsearches in different clinical areas was displayed graphically\\\\nfor the searches both alone and in combinations, to allow ex-\\\\namination of differences in parallelism, level, and ﬂatness.\\\\n\\\\nprecision is the proportion of all retrieved records that\\\\n\\\\nare relevant:\\\\n\\\\nnumber of reference standard articles found\\\\n\\\\ntotal number of records retrieved\\\\n\\\\nthe inverse of precision is the number needed to read to\\\\nﬁnd one eligible study, thus precision inﬂuences the work\\\\nof the review. precision was calculated only for the ahrq\\\\nevidence reviews. because not all candidates retrieved by\\\\nthe cochrane searches were assessed, precision could not\\\\nbe established in that sample.\\\\n\\\\n2.4. determining stability of results over time\\\\n\\\\nthe cq and related article searches, originally run in\\\\nmarch 2008, were repeated in february 2015 in the ahrq\\\\ncohort. recall of relevant items was compared with the\\\\noriginal retrievals to determine if any changes introduced\\\\nby national library of medicine might invalidate ﬁndings.\\\\nas svm conﬁguration is under the control of the investiga-\\\\ntors, it was not retested.\\\\n\\\\n3. results\\\\n\\\\nsix cochrane reviews and 10 ahrq evidence reports\\\\nmet all inclusion criteria (appendix b at www.jclinepi.\\\\ncom). characteristics of the included reviews are shown\\\\nin table 1.\\\\n\\\\n9a\\\\n1\\\\n3\\\\n\\\\n3\\\\n\\\\nd\\\\n1\\\\n\\\\nd\\\\n1\\\\n2\\\\n1\\\\n1\\\\n\\\\n1\\\\nd\\\\nd\\\\n\\\\nd\\\\n3\\\\n3\\\\n4\\\\n\\\\ncharacteristic\\\\n\\\\ntherapy evaluated\\\\n\\\\nmedications\\\\nmedical devices\\\\nprocedures\\\\n\\\\nclinical topic area\\\\n\\\\ncardiac and cardiovascular\\\\n\\\\nsystems\\\\n\\\\ncritical care\\\\nendocrinology and\\\\n\\\\nmetabolism\\\\n\\\\ninfectious disease\\\\nclinical neurology\\\\nobstetrics and gynecology\\\\noncology\\\\nperipheral vascular\\\\n\\\\ndiseases\\\\npsychiatry\\\\nrespiratory systems\\\\nurology and nephrology\\\\n\\\\npublication period\\\\n\\\\nmarch 1997eapril 1999\\\\nmay 1999ejune 2001\\\\njuly 2001eaugust 2003\\\\nseptember 2003e\\\\ndecember 2005\\\\n\\\\nmedian included trials\\\\n\\\\nmedian included\\\\n\\\\nparticipants\\\\n\\\\nmedline coverage of\\\\n\\\\n5\\\\n1\\\\nd\\\\n\\\\nd\\\\n\\\\n1\\\\nd\\\\n\\\\n3\\\\nd\\\\nd\\\\nd\\\\nd\\\\n\\\\nd\\\\n1\\\\n1\\\\n\\\\n3\\\\n1\\\\n2\\\\nd\\\\n\\\\n17 (iqr, 14e20)\\\\n\\\\n96 (iqr,\\\\n\\\\n31.75e121.5)\\\\n8,679 (iqr,\\\\n22,830 (iqr, 14,\\\\n4,085e50,109)\\\\n172e49,687)\\\\n99/107 (92.5%) 969/980 (98.8%)\\\\n\\\\nincluded articles\\\\nabbreviations: ahrq, agency for healthcare research and qual-\\\\n\\\\nity; iqr, interquartile range.\\\\n\\\\na some ahrq reviews included more than one class of therapy.\\\\n\\\\nrecall of new relevant studies is shown in table 2. the\\\\nupdated cochrane reviews had 20 new studies identiﬁed by\\\\nthe review authors and added in the updates. recall of these\\\\n20 ranged from 1.00 for the related articles method to a low\\\\nof 0.80 cqs (table 2). our team identiﬁed 277 new studies\\\\nas relevant for inclusion in the 10 evidence reports that\\\\nwere updated. all test searches showed lower recall for this\\\\ncohort than for the cochrane reviews and here the cq out-\\\\nperformed both ranking searches (table 2).\\\\n\\\\n3.1. recall of cq combined with a ranking method\\\\n\\\\nof the 297 new relevant studies identiﬁed, the combina-\\\\ntion of cq and related articles searches identiﬁed 270\\\\n(overall\\\\nin\\\\nthe cochrane cohort and 0.90 (250/277) in the ahrq\\\\ncohort.\\\\n\\\\nrecall of 0.91). recall was 1.00 (20/20)\\\\n\\\\nthe combination of cq and svm identiﬁed 263 studies\\\\n(overall recall of 0.89). recall was 1.00 (20/20) in the\\\\ncochrane cohort and 0.88 (243/277) in the ahrq cohort.\\\\nwhen all three methods were used together, recall was\\\\n0.997 with 296 of the 297 studies identiﬁed.\\\\n\\\\n\\\\x0c', 'journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\ncomplementary approaches to searching medline may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nmargaret sampsona,*, berry de bruijnb, christine urquhartc, kaveh shojaniad\\\\nalibrary and media services, children’s hospital of eastern ontario, 401 smyth road, ottawa, ontario k1h 8l1, canada\\\\n\\\\nbnational research councildinformation and communications technologies portfolio (nrc-ict), 1200 montreal rd, bldg m-50, ottawa, ontario k1a\\\\n\\\\ncdepartment of information studies, sunnybrook health sciences centre, aberystwyth university, llanbadarn campus, aberystwyth, wales sy23 3al,\\\\n\\\\n0r6, canada\\\\n\\\\nunited kingdom\\\\n\\\\ndcentre for quality improvement and patient safety, room h468, 2075 bayview avenue toronto, ontario m4n 3m5, canada\\\\n\\\\naccepted 7 march 2016; published online 11 march 2016\\\\n\\\\nabstract\\\\n\\\\nobjectives: to maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming boolean searches across multiple databases are common. although medline provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through boolean searches alone.\\\\n\\\\nstudy design and setting: recall of one boolean search method, the clinical query (cq), combined with a ranking method, support\\\\nvector machine (svm), or pubmed-related articles, was tested against a gold standard of studies added to 6 updated cochrane reviews and\\\\n10 agency for healthcare research and quality (ahrq) evidence reviews. for the ahrq sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nresults: recall of new studies was 0.69 for the cq, 0.66 for related articles, 0.50 for svm, 0.91 for the combination of cq and related\\\\narticles, and 0.89 for the combination of cq and svm. precision was 0.11 for cq and related articles combined, and 0.11 for cq and svm\\\\ncombined. related articles showed least stability over time.\\\\n\\\\nconclusions: the complementary combination of a boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in medline. (cid:1) 2016 elsevier inc. all rights reserved.\\\\n\\\\nkeywords: information retrieval; systematic reviews; support vector machine; clinical query; pubmed similar articles; searches; updating; medline\\\\n\\\\n1. introduction\\\\n\\\\nsystematic review searches need to have high recall.\\\\nmechanisms to achieve this usually include expansive\\\\nboolean searches of multiple databases. this approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nmedline gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. however,\\\\n\\\\nconﬂict of interest: none.\\\\nfunding: some of this work was funded by the us agency for health-\\\\ncare research and quality, department of health and human services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* corresponding author. tel.: 613-737-7600.\\\\ne-mail address: msampson@cheo.on.ca (m. sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 elsevier inc. all rights reserved.\\\\n\\\\nin 1994, a landmark article by dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through medline. recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in medline, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nsuccessful retrieval through a boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. the search of\\\\nmultiple databases can therefore be helpful. the target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. a text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. these tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nwhat is new?\\\\n\\\\nkey ﬁndings\\\\n(cid:1) searches using known relevant studies and the\\\\nsimilar articles feature of pubmed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) for a given question, if the boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. the two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) the precision of\\\\n\\\\nmethod appears better\\\\nexhaustive boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nwhat this adds to what was known?\\\\n(cid:1) the paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nit is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nwhat is the implication and what should change\\\\nnow?\\\\n(cid:1) using the simple and universally available pubmed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) if the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nmedline.\\\\n\\\\nto large retrievals with low precision [9]. these resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional boolean searches have\\\\nbeen sought [10].\\\\n\\\\nmost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nthese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. these methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nwe examined one method, support vector machine\\\\n(svm), and compared it with a simple and readily available\\\\nmethod based on the pubmed similar articles feature. we\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. we paired both\\\\nwith a focused boolean search within medline. we\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for svm and seed articles for the pubmed-\\\\n\\\\nrelated articles search. we therefore sought to determine\\\\nif a focused boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\ncomparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. in this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (cq), and pubmed-related articles are tested in a\\\\ncohort of six updated cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 agency for healthcare\\\\nresearch and quality (ahrq) evidence reports. the\\\\ncochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. the replication in the\\\\nahrq cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. all records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nother research [17,18] suggests that searches using the\\\\npubmed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a boolean-type search of\\\\nmedline. we tested an additional search method, svm,\\\\nin the cochrane and ahrq samples to permit comparison\\\\nwith our pubmed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the pubmed\\\\nsimilar articles method.\\\\n\\\\nthe aim of this article was to test whether the combined\\\\napproach of a focused boolean search paired with a second\\\\nsearch using the similar articles feature of pubmed or svm\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. methods\\\\n\\\\n2.1. formation of the study cohorts\\\\n\\\\nthis analysis uses a data set created for an updating\\\\nstudy sponsored by ahrq [16]. methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nbrieﬂy, cochrane reviews were identiﬁed through a\\\\nsearch of the acp journal club database (ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or medline(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nahrq reports were identiﬁed through the pubmed\\\\nquery ‘‘evid rep technol assess (summ)’’[journal:__\\\\njrid21544]. screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c110\\\\n\\\\nm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\nfor inclusion, all reviews must have included random-\\\\nized controlled trials (rcts) and provided meta-analysis\\\\nfor at least one outcome. the medline search strategies\\\\nhad to be reported in enough detail to permit replication.\\\\nfifteen ahrq evidence reports thought\\\\nlikely to have\\\\nimportant new evidence were selected and used to validate\\\\nthe updating methods used in the main cohort.\\\\n\\\\ncochrane reviews were selected to meet minimum qual-\\\\nity and relevance standards, as deﬁned by acp journal\\\\nclub [19]. we also required that the review had been up-\\\\ndated and that the text of both the original review and an\\\\nupdated version was available. the search for the original\\\\nreview must have included medline and at least one\\\\nother electronic bibliographic database and one or more\\\\nnondatabase method such as hand searching or checking\\\\nreference lists. such comprehensive searching was likely\\\\nto identify most relevant\\\\nliterature and form a useful\\\\ntraining set of included examples. at least 10 rcts or\\\\nquasi-rcts must have been included in the original review\\\\nto give an adequate training set for svm.\\\\n\\\\n2.2. test searches\\\\n\\\\n2.2.1. clinical query\\\\n\\\\nboolean searches were developed by a librarian experi-\\\\nenced in systematic review searches using a protocol\\\\n(appendix a at www.jclinepi.com). search strategies were\\\\ndeliberately simple, usually consisting of two or three\\\\nmedical subject heading (mesh) terms representing the\\\\npopulation and intervention of the review. this search\\\\nwas limited by a cq ﬁlter to ‘‘therapy (best balance of\\\\nsensitivity\\\\n‘‘randomized\\\\ncontrolled trial.pt. or\\\\nrandomized.mp. or placebo.mp.’’\\\\n[20]. boolean searches were run in ovid medline.\\\\n\\\\nspeciﬁcity)’’\\\\n\\\\nthat\\\\n\\\\nand\\\\n\\\\nis,\\\\n\\\\n2.2.2. related articles\\\\n\\\\nsearches using the pubmed similar articles feature were\\\\nperformed using the pubmed unique identiﬁers (pmid) of\\\\nthe three newest and three largest studies included in the\\\\noriginal review as seed articles. there was no replacement\\\\nin the event of overlap between the largest and newest sets,\\\\nor if one of those studies was not indexed in medline.\\\\n(all studies included in the original reviews were checked\\\\nfor indexing status in medline.) the resulting set was\\\\nlimited to the publication type rct and date limited to\\\\nthe period since the search date of the original review.\\\\n\\\\n2.2.3. support vector machine\\\\n\\\\nthe svm searches used for this project have not been\\\\npreviously described so the methods will be reported in\\\\nmore detail. svm is a well established and broadly applied\\\\nclassiﬁcation method from machine learning [21,22]. a\\\\nclassiﬁcation task is closely related to an information\\\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\\\nthe collection. as svm is a supervised algorithm,\\\\nit\\\\n\\\\nrequires training examples with known labels. it then places\\\\nthese training examples in a high-dimensional vector space\\\\nin such a way that examples of one class are separated from\\\\nexamples of the other class with the greatest distance to the\\\\nseparating boundary. predicting the class of a new instance\\\\nhappens by placing it in the same space and determining on\\\\nwhich side of the boundary it falls. the distance to the\\\\nboundary is an indication of the prediction conﬁdence and\\\\nis used to rank instances (documents) as is essential in in-\\\\nformation retrieval tasks or to provide a cutoff threshold.\\\\n\\\\nin our setup, the training set consisted of search results\\\\nfrom the original review formed with included instances\\\\nbeing the studies included in the review (the relevant re-\\\\ntrievals) and excluded instances being the studies found\\\\nby the search used in the original review but excluded from\\\\nthe review (irrelevant retrievals). the new examples to be\\\\nclassiﬁed were articles indexed since the date of the search\\\\nperformed in the original review.\\\\n\\\\nsvm searches were run on medline data stored\\\\nlocally at\\\\nthe national research council of canada.\\\\nmedline was refreshed before running the searches.\\\\nthe medline records were represented as bags of fea-\\\\ntures, where features consisted of lowercased words and\\\\nword combinations (up to four words) from the title and ab-\\\\nstract ﬁelds, full mesh terms, and contents of all other\\\\nﬁelds in the medline record. features were uniformly\\\\nweighted. the svm implementation used was svm light\\\\n[23] used with a linear kernel.\\\\n\\\\nthe approach was piloted with one ahrq evidence\\\\nreport, and various conﬁgurations were tried, observing\\\\nthe placement of several known new relevant studies within\\\\nthe retrieval. the medline set was ﬁltered with the\\\\nrevised highly sensitive search strategy (hsss) [24]. pre-\\\\nliminary testing indicated that this ﬁlter would retrieve\\\\n99.1% of medline-indexed relevant new evidence from\\\\nthe larger cohort of journal-published systematic reviews\\\\n(n 5 72) and cochrane reviews (n 5 27). this hsss ﬁlter\\\\nwas added primarily to improve processing speed, relative\\\\nto running the classiﬁer against all of medline. with\\\\nthe ﬁlter, processing for one systematic review took about\\\\n10 minutes. next, a coarse-grained ﬁlter was applied;\\\\nincluded and excluded instances were combined in a single\\\\nset labeled pos. adjacent pubmed ids (the next higher\\\\npmid to each member of pos, as long as that higher num-\\\\nber was not itself a member of pos) were combined in a set\\\\nto represent world. the model was trained on pos vs.\\\\nworld, using words and phrases from title, abstract,\\\\nauthor names, and journal name. this model was run\\\\nagainst the records passing the hsss ﬁlter.\\\\n\\\\nfour different subsets of\\\\n\\\\nthe svm retrieval were\\\\nexamineddone set included all records with a relevance\\\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\\\n(svm200point5). other sets consisted of all records with\\\\nrelevance scores of 0.95 or more (svm95), 0.90 or more\\\\n(svm90), and 0.80 or more (svm80). pubmed id and\\\\nrelevance scores were recorded, and pubmed ids were\\\\n\\\\n\\\\x0cm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\n111\\\\n\\\\ntable 1. characteristics of the included reviews\\\\n\\\\ncochrane\\\\nreviews; n\\\\n\\\\nahrq evidence\\\\n\\\\nreports; n\\\\n\\\\nincorporated into the database of records for the reviewers\\\\nto screen. only the results from svm200point5 are re-\\\\nported here and are described as svm.\\\\n\\\\n2.3. determining performance of the test searches\\\\n\\\\nfor the updated cochrane reviews, reference standard ar-\\\\nticles were those studies included in the updated review that\\\\nwere not in the original and which entered medline after\\\\nthe search date of the original review. for the ahrq evi-\\\\ndence reports, the full retrieval set was screened. records\\\\nfound relevant by the consensus of reviewers were consid-\\\\nered reference standard articles. recall was the proportion\\\\nof reference standard articles identiﬁed by the search:\\\\n\\\\nnumber of reference standard articles found\\\\ntotal number of reference standard articles\\\\n\\\\nbiovenn software was used to analyze the overlap be-\\\\ntween the retrieval of relevant articles from the three\\\\nsearches and to create venn diagrams [25].\\\\n\\\\nall included reviews were classiﬁed into clinical area\\\\nbased on factors such as isi journal classiﬁcation, the co-\\\\nchrane collaboration review group where the topic might\\\\nbe placed and the high level mesh term under which the pop-\\\\nulation (condition) would be indexed. performance of the\\\\nsearches in different clinical areas was displayed graphically\\\\nfor the searches both alone and in combinations, to allow ex-\\\\namination of differences in parallelism, level, and ﬂatness.\\\\n\\\\nprecision is the proportion of all retrieved records that\\\\n\\\\nare relevant:\\\\n\\\\nnumber of reference standard articles found\\\\n\\\\ntotal number of records retrieved\\\\n\\\\nthe inverse of precision is the number needed to read to\\\\nﬁnd one eligible study, thus precision inﬂuences the work\\\\nof the review. precision was calculated only for the ahrq\\\\nevidence reviews. because not all candidates retrieved by\\\\nthe cochrane searches were assessed, precision could not\\\\nbe established in that sample.\\\\n\\\\n2.4. determining stability of results over time\\\\n\\\\nthe cq and related article searches, originally run in\\\\nmarch 2008, were repeated in february 2015 in the ahrq\\\\ncohort. recall of relevant items was compared with the\\\\noriginal retrievals to determine if any changes introduced\\\\nby national library of medicine might invalidate ﬁndings.\\\\nas svm conﬁguration is under the control of the investiga-\\\\ntors, it was not retested.\\\\n\\\\n3. results\\\\n\\\\nsix cochrane reviews and 10 ahrq evidence reports\\\\nmet all inclusion criteria (appendix b at www.jclinepi.\\\\ncom). characteristics of the included reviews are shown\\\\nin table 1.\\\\n\\\\n9a\\\\n1\\\\n3\\\\n\\\\n3\\\\n\\\\nd\\\\n1\\\\n\\\\nd\\\\n1\\\\n2\\\\n1\\\\n1\\\\n\\\\n1\\\\nd\\\\nd\\\\n\\\\nd\\\\n3\\\\n3\\\\n4\\\\n\\\\ncharacteristic\\\\n\\\\ntherapy evaluated\\\\n\\\\nmedications\\\\nmedical devices\\\\nprocedures\\\\n\\\\nclinical topic area\\\\n\\\\ncardiac and cardiovascular\\\\n\\\\nsystems\\\\n\\\\ncritical care\\\\nendocrinology and\\\\n\\\\nmetabolism\\\\n\\\\ninfectious disease\\\\nclinical neurology\\\\nobstetrics and gynecology\\\\noncology\\\\nperipheral vascular\\\\n\\\\ndiseases\\\\npsychiatry\\\\nrespiratory systems\\\\nurology and nephrology\\\\n\\\\npublication period\\\\n\\\\nmarch 1997eapril 1999\\\\nmay 1999ejune 2001\\\\njuly 2001eaugust 2003\\\\nseptember 2003e\\\\ndecember 2005\\\\n\\\\nmedian included trials\\\\n\\\\nmedian included\\\\n\\\\nparticipants\\\\n\\\\nmedline coverage of\\\\n\\\\n5\\\\n1\\\\nd\\\\n\\\\nd\\\\n\\\\n1\\\\nd\\\\n\\\\n3\\\\nd\\\\nd\\\\nd\\\\nd\\\\n\\\\nd\\\\n1\\\\n1\\\\n\\\\n3\\\\n1\\\\n2\\\\nd\\\\n\\\\n17 (iqr, 14e20)\\\\n\\\\n96 (iqr,\\\\n\\\\n31.75e121.5)\\\\n8,679 (iqr,\\\\n22,830 (iqr, 14,\\\\n4,085e50,109)\\\\n172e49,687)\\\\n99/107 (92.5%) 969/980 (98.8%)\\\\n\\\\nincluded articles\\\\nabbreviations: ahrq, agency for healthcare research and qual-\\\\n\\\\nity; iqr, interquartile range.\\\\n\\\\na some ahrq reviews included more than one class of therapy.\\\\n\\\\nrecall of new relevant studies is shown in table 2. the\\\\nupdated cochrane reviews had 20 new studies identiﬁed by\\\\nthe review authors and added in the updates. recall of these\\\\n20 ranged from 1.00 for the related articles method to a low\\\\nof 0.80 cqs (table 2). our team identiﬁed 277 new studies\\\\nas relevant for inclusion in the 10 evidence reports that\\\\nwere updated. all test searches showed lower recall for this\\\\ncohort than for the cochrane reviews and here the cq out-\\\\nperformed both ranking searches (table 2).\\\\n\\\\n3.1. recall of cq combined with a ranking method\\\\n\\\\nof the 297 new relevant studies identiﬁed, the combina-\\\\ntion of cq and related articles searches identiﬁed 270\\\\n(overall\\\\nin\\\\nthe cochrane cohort and 0.90 (250/277) in the ahrq\\\\ncohort.\\\\n\\\\nrecall of 0.91). recall was 1.00 (20/20)\\\\n\\\\nthe combination of cq and svm identiﬁed 263 studies\\\\n(overall recall of 0.89). recall was 1.00 (20/20) in the\\\\ncochrane cohort and 0.88 (243/277) in the ahrq cohort.\\\\nwhen all three methods were used together, recall was\\\\n0.997 with 296 of the 297 studies identiﬁed.\\\\n\\\\n\\\\x0c112\\\\n\\\\nm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\ntable 2. recall of eligible studies by the search methods\\\\n\\\\ncochrane reviews\\\\n\\\\nahrq evidence\\\\n\\\\nreports\\\\n\\\\nretrieval method\\\\n\\\\nclinical query\\\\nrelated articles\\\\nsvm\\\\ntotal\\\\n\\\\nn\\\\n\\\\n16\\\\n20\\\\n19\\\\n20\\\\n\\\\nrecall\\\\n\\\\n0.80\\\\n1.00\\\\n0.95\\\\n\\\\nn\\\\n\\\\n188\\\\n176\\\\n129\\\\n277\\\\n\\\\nrecall\\\\n\\\\n0.68\\\\n0.64\\\\n0.47\\\\n\\\\nabbreviations: ahrq, agency for healthcare research and qual-\\\\n\\\\nity; svm, support vector machine.\\\\n\\\\nacross the two cohorts, the overlap and unique component from\\\\nthe retrieval of relevant records by the test searches was examined\\\\n(fig. 1).\\\\n\\\\n3.2. consistency across clinical areas\\\\n\\\\nthirteen clinical areas were represented in the larger\\\\nstudy that included 72 journal-published reviews, but only\\\\neight of those clinical areas had two or more new relevant\\\\nstudies in the smaller cochrane and ahrq cohorts. fig. 2\\\\nshows recall of new studies by the three types of searches,\\\\nfor these eight clinical areas.\\\\n\\\\ncombined recall of the cq boolean search paired with a\\\\nranking search is shown in fig. 3. the combination showed\\\\ncomplete recall of relevant new studies in three of eight\\\\nareas when cq was paired with related articles and recall\\\\nof 0.67 or higher in all areas. four of eight clinical areas\\\\nhad complete recall for the combination of cq and svm,\\\\nand recall was 0.80 or higher in all areas. even for periph-\\\\neral vascular disease, where all three searches performed\\\\n\\\\nfig. 2. recall of new studies by clinical area for each search method.\\\\nabbreviations: svm, support vector machine.\\\\n\\\\nfairly poorly individually, recall was 0.76 for the cq/\\\\nrelated articles and 0.90 in the cq/svm pairing.\\\\n\\\\n3.3. search precision\\\\n\\\\nprecision, across the 10 ahrq reviews, was 0.11 for the\\\\ncq, 0.22 for related articles, and 0.19 for svm (table 3).\\\\n\\\\nfig. 1. overlapping and unique retrieval of relevant studies by each\\\\nsearch method. the size of the circle is proportional to the size of\\\\nthe retrieval, but the overlap and unique portions are approximations.\\\\nexact ﬁgures for the components are labeled. abbreviations: svm,\\\\nsupport vector machine.\\\\n\\\\nfig. 3. recall of new studies by clinical area for search methods in\\\\ncombination. abbreviations: cq, clinical query; svm, support vector\\\\nmachine.\\\\n\\\\n\\\\x0c', 'journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\ncomplementary approaches to searching medline may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nmargaret sampsona,*, berry de bruijnb, christine urquhartc, kaveh shojaniad\\\\nalibrary and media services, children’s hospital of eastern ontario, 401 smyth road, ottawa, ontario k1h 8l1, canada\\\\n\\\\nbnational research councildinformation and communications technologies portfolio (nrc-ict), 1200 montreal rd, bldg m-50, ottawa, ontario k1a\\\\n\\\\ncdepartment of information studies, sunnybrook health sciences centre, aberystwyth university, llanbadarn campus, aberystwyth, wales sy23 3al,\\\\n\\\\n0r6, canada\\\\n\\\\nunited kingdom\\\\n\\\\ndcentre for quality improvement and patient safety, room h468, 2075 bayview avenue toronto, ontario m4n 3m5, canada\\\\n\\\\naccepted 7 march 2016; published online 11 march 2016\\\\n\\\\nabstract\\\\n\\\\nobjectives: to maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming boolean searches across multiple databases are common. although medline provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through boolean searches alone.\\\\n\\\\nstudy design and setting: recall of one boolean search method, the clinical query (cq), combined with a ranking method, support\\\\nvector machine (svm), or pubmed-related articles, was tested against a gold standard of studies added to 6 updated cochrane reviews and\\\\n10 agency for healthcare research and quality (ahrq) evidence reviews. for the ahrq sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nresults: recall of new studies was 0.69 for the cq, 0.66 for related articles, 0.50 for svm, 0.91 for the combination of cq and related\\\\narticles, and 0.89 for the combination of cq and svm. precision was 0.11 for cq and related articles combined, and 0.11 for cq and svm\\\\ncombined. related articles showed least stability over time.\\\\n\\\\nconclusions: the complementary combination of a boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in medline. (cid:1) 2016 elsevier inc. all rights reserved.\\\\n\\\\nkeywords: information retrieval; systematic reviews; support vector machine; clinical query; pubmed similar articles; searches; updating; medline\\\\n\\\\n1. introduction\\\\n\\\\nsystematic review searches need to have high recall.\\\\nmechanisms to achieve this usually include expansive\\\\nboolean searches of multiple databases. this approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nmedline gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. however,\\\\n\\\\nconﬂict of interest: none.\\\\nfunding: some of this work was funded by the us agency for health-\\\\ncare research and quality, department of health and human services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* corresponding author. tel.: 613-737-7600.\\\\ne-mail address: msampson@cheo.on.ca (m. sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 elsevier inc. all rights reserved.\\\\n\\\\nin 1994, a landmark article by dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through medline. recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in medline, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nsuccessful retrieval through a boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. the search of\\\\nmultiple databases can therefore be helpful. the target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. a text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. these tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nwhat is new?\\\\n\\\\nkey ﬁndings\\\\n(cid:1) searches using known relevant studies and the\\\\nsimilar articles feature of pubmed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) for a given question, if the boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. the two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) the precision of\\\\n\\\\nmethod appears better\\\\nexhaustive boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nwhat this adds to what was known?\\\\n(cid:1) the paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nit is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nwhat is the implication and what should change\\\\nnow?\\\\n(cid:1) using the simple and universally available pubmed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) if the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nmedline.\\\\n\\\\nto large retrievals with low precision [9]. these resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional boolean searches have\\\\nbeen sought [10].\\\\n\\\\nmost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nthese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. these methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nwe examined one method, support vector machine\\\\n(svm), and compared it with a simple and readily available\\\\nmethod based on the pubmed similar articles feature. we\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. we paired both\\\\nwith a focused boolean search within medline. we\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for svm and seed articles for the pubmed-\\\\n\\\\nrelated articles search. we therefore sought to determine\\\\nif a focused boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\ncomparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. in this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (cq), and pubmed-related articles are tested in a\\\\ncohort of six updated cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 agency for healthcare\\\\nresearch and quality (ahrq) evidence reports. the\\\\ncochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. the replication in the\\\\nahrq cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. all records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nother research [17,18] suggests that searches using the\\\\npubmed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a boolean-type search of\\\\nmedline. we tested an additional search method, svm,\\\\nin the cochrane and ahrq samples to permit comparison\\\\nwith our pubmed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the pubmed\\\\nsimilar articles method.\\\\n\\\\nthe aim of this article was to test whether the combined\\\\napproach of a focused boolean search paired with a second\\\\nsearch using the similar articles feature of pubmed or svm\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. methods\\\\n\\\\n2.1. formation of the study cohorts\\\\n\\\\nthis analysis uses a data set created for an updating\\\\nstudy sponsored by ahrq [16]. methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nbrieﬂy, cochrane reviews were identiﬁed through a\\\\nsearch of the acp journal club database (ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or medline(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nahrq reports were identiﬁed through the pubmed\\\\nquery ‘‘evid rep technol assess (summ)’’[journal:__\\\\njrid21544]. screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c110\\\\n\\\\nm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\nfor inclusion, all reviews must have included random-\\\\nized controlled trials (rcts) and provided meta-analysis\\\\nfor at least one outcome. the medline search strategies\\\\nhad to be reported in enough detail to permit replication.\\\\nfifteen ahrq evidence reports thought\\\\nlikely to have\\\\nimportant new evidence were selected and used to validate\\\\nthe updating methods used in the main cohort.\\\\n\\\\ncochrane reviews were selected to meet minimum qual-\\\\nity and relevance standards, as deﬁned by acp journal\\\\nclub [19]. we also required that the review had been up-\\\\ndated and that the text of both the original review and an\\\\nupdated version was available. the search for the original\\\\nreview must have included medline and at least one\\\\nother electronic bibliographic database and one or more\\\\nnondatabase method such as hand searching or checking\\\\nreference lists. such comprehensive searching was likely\\\\nto identify most relevant\\\\nliterature and form a useful\\\\ntraining set of included examples. at least 10 rcts or\\\\nquasi-rcts must have been included in the original review\\\\nto give an adequate training set for svm.\\\\n\\\\n2.2. test searches\\\\n\\\\n2.2.1. clinical query\\\\n\\\\nboolean searches were developed by a librarian experi-\\\\nenced in systematic review searches using a protocol\\\\n(appendix a at www.jclinepi.com). search strategies were\\\\ndeliberately simple, usually consisting of two or three\\\\nmedical subject heading (mesh) terms representing the\\\\npopulation and intervention of the review. this search\\\\nwas limited by a cq ﬁlter to ‘‘therapy (best balance of\\\\nsensitivity\\\\n‘‘randomized\\\\ncontrolled trial.pt. or\\\\nrandomized.mp. or placebo.mp.’’\\\\n[20]. boolean searches were run in ovid medline.\\\\n\\\\nspeciﬁcity)’’\\\\n\\\\nthat\\\\n\\\\nand\\\\n\\\\nis,\\\\n\\\\n2.2.2. related articles\\\\n\\\\nsearches using the pubmed similar articles feature were\\\\nperformed using the pubmed unique identiﬁers (pmid) of\\\\nthe three newest and three largest studies included in the\\\\noriginal review as seed articles. there was no replacement\\\\nin the event of overlap between the largest and newest sets,\\\\nor if one of those studies was not indexed in medline.\\\\n(all studies included in the original reviews were checked\\\\nfor indexing status in medline.) the resulting set was\\\\nlimited to the publication type rct and date limited to\\\\nthe period since the search date of the original review.\\\\n\\\\n2.2.3. support vector machine\\\\n\\\\nthe svm searches used for this project have not been\\\\npreviously described so the methods will be reported in\\\\nmore detail. svm is a well established and broadly applied\\\\nclassiﬁcation method from machine learning [21,22]. a\\\\nclassiﬁcation task is closely related to an information\\\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\\\nthe collection. as svm is a supervised algorithm,\\\\nit\\\\n\\\\nrequires training examples with known labels. it then places\\\\nthese training examples in a high-dimensional vector space\\\\nin such a way that examples of one class are separated from\\\\nexamples of the other class with the greatest distance to the\\\\nseparating boundary. predicting the class of a new instance\\\\nhappens by placing it in the same space and determining on\\\\nwhich side of the boundary it falls. the distance to the\\\\nboundary is an indication of the prediction conﬁdence and\\\\nis used to rank instances (documents) as is essential in in-\\\\nformation retrieval tasks or to provide a cutoff threshold.\\\\n\\\\nin our setup, the training set consisted of search results\\\\nfrom the original review formed with included instances\\\\nbeing the studies included in the review (the relevant re-\\\\ntrievals) and excluded instances being the studies found\\\\nby the search used in the original review but excluded from\\\\nthe review (irrelevant retrievals). the new examples to be\\\\nclassiﬁed were articles indexed since the date of the search\\\\nperformed in the original review.\\\\n\\\\nsvm searches were run on medline data stored\\\\nlocally at\\\\nthe national research council of canada.\\\\nmedline was refreshed before running the searches.\\\\nthe medline records were represented as bags of fea-\\\\ntures, where features consisted of lowercased words and\\\\nword combinations (up to four words) from the title and ab-\\\\nstract ﬁelds, full mesh terms, and contents of all other\\\\nﬁelds in the medline record. features were uniformly\\\\nweighted. the svm implementation used was svm light\\\\n[23] used with a linear kernel.\\\\n\\\\nthe approach was piloted with one ahrq evidence\\\\nreport, and various conﬁgurations were tried, observing\\\\nthe placement of several known new relevant studies within\\\\nthe retrieval. the medline set was ﬁltered with the\\\\nrevised highly sensitive search strategy (hsss) [24]. pre-\\\\nliminary testing indicated that this ﬁlter would retrieve\\\\n99.1% of medline-indexed relevant new evidence from\\\\nthe larger cohort of journal-published systematic reviews\\\\n(n 5 72) and cochrane reviews (n 5 27). this hsss ﬁlter\\\\nwas added primarily to improve processing speed, relative\\\\nto running the classiﬁer against all of medline. with\\\\nthe ﬁlter, processing for one systematic review took about\\\\n10 minutes. next, a coarse-grained ﬁlter was applied;\\\\nincluded and excluded instances were combined in a single\\\\nset labeled pos. adjacent pubmed ids (the next higher\\\\npmid to each member of pos, as long as that higher num-\\\\nber was not itself a member of pos) were combined in a set\\\\nto represent world. the model was trained on pos vs.\\\\nworld, using words and phrases from title, abstract,\\\\nauthor names, and journal name. this model was run\\\\nagainst the records passing the hsss ﬁlter.\\\\n\\\\nfour different subsets of\\\\n\\\\nthe svm retrieval were\\\\nexamineddone set included all records with a relevance\\\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\\\n(svm200point5). other sets consisted of all records with\\\\nrelevance scores of 0.95 or more (svm95), 0.90 or more\\\\n(svm90), and 0.80 or more (svm80). pubmed id and\\\\nrelevance scores were recorded, and pubmed ids were\\\\n\\\\n\\\\x0cm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\n111\\\\n\\\\ntable 1. characteristics of the included reviews\\\\n\\\\ncochrane\\\\nreviews; n\\\\n\\\\nahrq evidence\\\\n\\\\nreports; n\\\\n\\\\nincorporated into the database of records for the reviewers\\\\nto screen. only the results from svm200point5 are re-\\\\nported here and are described as svm.\\\\n\\\\n2.3. determining performance of the test searches\\\\n\\\\nfor the updated cochrane reviews, reference standard ar-\\\\nticles were those studies included in the updated review that\\\\nwere not in the original and which entered medline after\\\\nthe search date of the original review. for the ahrq evi-\\\\ndence reports, the full retrieval set was screened. records\\\\nfound relevant by the consensus of reviewers were consid-\\\\nered reference standard articles. recall was the proportion\\\\nof reference standard articles identiﬁed by the search:\\\\n\\\\nnumber of reference standard articles found\\\\ntotal number of reference standard articles\\\\n\\\\nbiovenn software was used to analyze the overlap be-\\\\ntween the retrieval of relevant articles from the three\\\\nsearches and to create venn diagrams [25].\\\\n\\\\nall included reviews were classiﬁed into clinical area\\\\nbased on factors such as isi journal classiﬁcation, the co-\\\\nchrane collaboration review group where the topic might\\\\nbe placed and the high level mesh term under which the pop-\\\\nulation (condition) would be indexed. performance of the\\\\nsearches in different clinical areas was displayed graphically\\\\nfor the searches both alone and in combinations, to allow ex-\\\\namination of differences in parallelism, level, and ﬂatness.\\\\n\\\\nprecision is the proportion of all retrieved records that\\\\n\\\\nare relevant:\\\\n\\\\nnumber of reference standard articles found\\\\n\\\\ntotal number of records retrieved\\\\n\\\\nthe inverse of precision is the number needed to read to\\\\nﬁnd one eligible study, thus precision inﬂuences the work\\\\nof the review. precision was calculated only for the ahrq\\\\nevidence reviews. because not all candidates retrieved by\\\\nthe cochrane searches were assessed, precision could not\\\\nbe established in that sample.\\\\n\\\\n2.4. determining stability of results over time\\\\n\\\\nthe cq and related article searches, originally run in\\\\nmarch 2008, were repeated in february 2015 in the ahrq\\\\ncohort. recall of relevant items was compared with the\\\\noriginal retrievals to determine if any changes introduced\\\\nby national library of medicine might invalidate ﬁndings.\\\\nas svm conﬁguration is under the control of the investiga-\\\\ntors, it was not retested.\\\\n\\\\n3. results\\\\n\\\\nsix cochrane reviews and 10 ahrq evidence reports\\\\nmet all inclusion criteria (appendix b at www.jclinepi.\\\\ncom). characteristics of the included reviews are shown\\\\nin table 1.\\\\n\\\\n9a\\\\n1\\\\n3\\\\n\\\\n3\\\\n\\\\nd\\\\n1\\\\n\\\\nd\\\\n1\\\\n2\\\\n1\\\\n1\\\\n\\\\n1\\\\nd\\\\nd\\\\n\\\\nd\\\\n3\\\\n3\\\\n4\\\\n\\\\ncharacteristic\\\\n\\\\ntherapy evaluated\\\\n\\\\nmedications\\\\nmedical devices\\\\nprocedures\\\\n\\\\nclinical topic area\\\\n\\\\ncardiac and cardiovascular\\\\n\\\\nsystems\\\\n\\\\ncritical care\\\\nendocrinology and\\\\n\\\\nmetabolism\\\\n\\\\ninfectious disease\\\\nclinical neurology\\\\nobstetrics and gynecology\\\\noncology\\\\nperipheral vascular\\\\n\\\\ndiseases\\\\npsychiatry\\\\nrespiratory systems\\\\nurology and nephrology\\\\n\\\\npublication period\\\\n\\\\nmarch 1997eapril 1999\\\\nmay 1999ejune 2001\\\\njuly 2001eaugust 2003\\\\nseptember 2003e\\\\ndecember 2005\\\\n\\\\nmedian included trials\\\\n\\\\nmedian included\\\\n\\\\nparticipants\\\\n\\\\nmedline coverage of\\\\n\\\\n5\\\\n1\\\\nd\\\\n\\\\nd\\\\n\\\\n1\\\\nd\\\\n\\\\n3\\\\nd\\\\nd\\\\nd\\\\nd\\\\n\\\\nd\\\\n1\\\\n1\\\\n\\\\n3\\\\n1\\\\n2\\\\nd\\\\n\\\\n17 (iqr, 14e20)\\\\n\\\\n96 (iqr,\\\\n\\\\n31.75e121.5)\\\\n8,679 (iqr,\\\\n22,830 (iqr, 14,\\\\n4,085e50,109)\\\\n172e49,687)\\\\n99/107 (92.5%) 969/980 (98.8%)\\\\n\\\\nincluded articles\\\\nabbreviations: ahrq, agency for healthcare research and qual-\\\\n\\\\nity; iqr, interquartile range.\\\\n\\\\na some ahrq reviews included more than one class of therapy.\\\\n\\\\nrecall of new relevant studies is shown in table 2. the\\\\nupdated cochrane reviews had 20 new studies identiﬁed by\\\\nthe review authors and added in the updates. recall of these\\\\n20 ranged from 1.00 for the related articles method to a low\\\\nof 0.80 cqs (table 2). our team identiﬁed 277 new studies\\\\nas relevant for inclusion in the 10 evidence reports that\\\\nwere updated. all test searches showed lower recall for this\\\\ncohort than for the cochrane reviews and here the cq out-\\\\nperformed both ranking searches (table 2).\\\\n\\\\n3.1. recall of cq combined with a ranking method\\\\n\\\\nof the 297 new relevant studies identiﬁed, the combina-\\\\ntion of cq and related articles searches identiﬁed 270\\\\n(overall\\\\nin\\\\nthe cochrane cohort and 0.90 (250/277) in the ahrq\\\\ncohort.\\\\n\\\\nrecall of 0.91). recall was 1.00 (20/20)\\\\n\\\\nthe combination of cq and svm identiﬁed 263 studies\\\\n(overall recall of 0.89). recall was 1.00 (20/20) in the\\\\ncochrane cohort and 0.88 (243/277) in the ahrq cohort.\\\\nwhen all three methods were used together, recall was\\\\n0.997 with 296 of the 297 studies identiﬁed.\\\\n\\\\n\\\\x0c112\\\\n\\\\nm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\ntable 2. recall of eligible studies by the search methods\\\\n\\\\ncochrane reviews\\\\n\\\\nahrq evidence\\\\n\\\\nreports\\\\n\\\\nretrieval method\\\\n\\\\nclinical query\\\\nrelated articles\\\\nsvm\\\\ntotal\\\\n\\\\nn\\\\n\\\\n16\\\\n20\\\\n19\\\\n20\\\\n\\\\nrecall\\\\n\\\\n0.80\\\\n1.00\\\\n0.95\\\\n\\\\nn\\\\n\\\\n188\\\\n176\\\\n129\\\\n277\\\\n\\\\nrecall\\\\n\\\\n0.68\\\\n0.64\\\\n0.47\\\\n\\\\nabbreviations: ahrq, agency for healthcare research and qual-\\\\n\\\\nity; svm, support vector machine.\\\\n\\\\nacross the two cohorts, the overlap and unique component from\\\\nthe retrieval of relevant records by the test searches was examined\\\\n(fig. 1).\\\\n\\\\n3.2. consistency across clinical areas\\\\n\\\\nthirteen clinical areas were represented in the larger\\\\nstudy that included 72 journal-published reviews, but only\\\\neight of those clinical areas had two or more new relevant\\\\nstudies in the smaller cochrane and ahrq cohorts. fig. 2\\\\nshows recall of new studies by the three types of searches,\\\\nfor these eight clinical areas.\\\\n\\\\ncombined recall of the cq boolean search paired with a\\\\nranking search is shown in fig. 3. the combination showed\\\\ncomplete recall of relevant new studies in three of eight\\\\nareas when cq was paired with related articles and recall\\\\nof 0.67 or higher in all areas. four of eight clinical areas\\\\nhad complete recall for the combination of cq and svm,\\\\nand recall was 0.80 or higher in all areas. even for periph-\\\\neral vascular disease, where all three searches performed\\\\n\\\\nfig. 2. recall of new studies by clinical area for each search method.\\\\nabbreviations: svm, support vector machine.\\\\n\\\\nfairly poorly individually, recall was 0.76 for the cq/\\\\nrelated articles and 0.90 in the cq/svm pairing.\\\\n\\\\n3.3. search precision\\\\n\\\\nprecision, across the 10 ahrq reviews, was 0.11 for the\\\\ncq, 0.22 for related articles, and 0.19 for svm (table 3).\\\\n\\\\nfig. 1. overlapping and unique retrieval of relevant studies by each\\\\nsearch method. the size of the circle is proportional to the size of\\\\nthe retrieval, but the overlap and unique portions are approximations.\\\\nexact ﬁgures for the components are labeled. abbreviations: svm,\\\\nsupport vector machine.\\\\n\\\\nfig. 3. recall of new studies by clinical area for search methods in\\\\ncombination. abbreviations: cq, clinical query; svm, support vector\\\\nmachine.\\\\n\\\\n\\\\x0ctable 3. overall precision in the ahrq sample\\\\n\\\\n4. discussion\\\\n\\\\nm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\n113\\\\n\\\\nretrieval method\\\\n\\\\nclinical query\\\\nrelated articles\\\\nsvm\\\\nclinical query þ related\\\\nclinical query þ svm\\\\nclinical query þ related\\\\n\\\\narticles\\\\n\\\\neligible\\\\nstudies\\\\nretrieved\\\\n\\\\n187\\\\n176\\\\n128\\\\n250\\\\n\\\\n243\\\\n276\\\\n\\\\nno. of candidates\\\\n\\\\nretrieved\\\\n\\\\nprecision\\\\n\\\\n1,637\\\\n814\\\\n659\\\\n2,318a\\\\n\\\\n2,247a\\\\n3,264a\\\\n\\\\n0.11\\\\n0.22\\\\n0.19\\\\n0.11\\\\n\\\\n0.11\\\\n0.08\\\\n\\\\narticles þ svm\\\\nabbreviations: ahrq, agency for healthcare research and\\\\n\\\\nquality; svm, support vector machine.\\\\n\\\\na number of candidates after removal of duplicate records.\\\\n\\\\nprecision for the individual reviews ranged from 0.0 to 0.38\\\\nfor cqs, 0.004 to 0.80 for related articles, and 0.01 to 0.43\\\\nfor svm. overall precision was 0.11 for cq and either\\\\nrelated articles or svm. overall precision was 0.08 when\\\\ncq and both related articles and svm were used. for\\\\nranked searches where a ﬁxed number of records will be\\\\nscreened, that number is always the denominator, so preci-\\\\nsion will tend to increase as the number of relevant records\\\\nincreasesdthe maximum retrieval size for svm was cap-\\\\nped at 200 records.\\\\n\\\\n3.4. retrieval consistency over time\\\\n\\\\nsearches were retested for the ahrq cohort using\\\\npubmed results obtained february 24, 2015. recall of the\\\\nrelated articles searches across the 10 ahrq reports was\\\\n0.64 originally and 0.50 when repeated (fig. 4). of the\\\\nthree extreme cases, one rose from 0.00 to 0.80, one re-\\\\nmained at 0.00, and one dropped from 1.00 to 0.60. cq\\\\nhad overall\\\\nrecall of 0.68 originally and 0.70 when\\\\nrepeated. changes in extreme cases were minimal. overall\\\\ncombined recall of cq and related article was 0.90 origi-\\\\nnally,\\\\nfalling to 0.85 when repeated. considering the\\\\nextreme values, all review with combined recall of 1.00\\\\noriginally remained at 1.00, whereas the one review with\\\\n0.00 recall in combination showed recall of 0.80 when\\\\nrepeated.\\\\n\\\\n1.0\\\\n0.9\\\\n0.8\\\\n0.7\\\\n0.6\\\\n0.5\\\\n0.4\\\\n0.3\\\\n0.2\\\\n0.1\\\\n0.0\\\\n\\\\noriginal\\\\n\\\\n2015\\\\n\\\\nclinical query\\\\n\\\\nrelated ar(cid:415)cles\\\\n\\\\ncombined\\\\n\\\\nfig. 4. recall of new studies for search methods, alone and in combi-\\\\nnation originally (left) and when retested in 2015 (right).\\\\n\\\\nwe expected that\\\\n\\\\nthe sophisticated svm approach\\\\nwould perform well when paired with more stripped-\\\\ndown, focused boolean searches. indeed, the two together\\\\nwere able to replace the multidatabase, multimodel\\\\nsearches used by the original review teams in updating\\\\nthe cochrane searches. the new relevant studies for the co-\\\\nchrane reviews may have been relatively easy to ﬁnd, but\\\\nthe pairing was also effective in the ahrq set, and there\\\\nshowed better precision than is usually seen with traditional\\\\nsearches for systematic review of rcts [9]. the ahrq set\\\\nwas formed from the new studies identiﬁed for more com-\\\\nplex interventions in a rigorous, well-funded study [16].\\\\n\\\\nwe were surprised that the related articles approach per-\\\\nformed almost as well as svm when used in combination\\\\nwith the cq. the related article search has advantages over\\\\nsvmdit requires far less data preparation, and no special\\\\nsoftware is needed for its use. this makes it useful not only\\\\nin updating, but also, if appropriate seed articles can be\\\\nfound, in original reviews.\\\\n\\\\nthat both svm and related articles sometimes showed\\\\npoor recall when used alone, but consistently good recall\\\\nwhen used with a boolean method suggests that there is\\\\nreal beneﬁt in using complementary search methods to\\\\nquerying medline. the related article method is not very\\\\ntime consuming, and easily added to other planned search\\\\nefforts, while the third method used here, svm, is more\\\\ntechnical. the combined performance of all three methods\\\\nwas surprising, but such a setup might become unpractical\\\\nin day-to-day use.\\\\n\\\\nother investigators have similar ﬁndings. examination\\\\nof the supplemental material presented in the appendix c\\\\nto the article by waffenschmidt et al. [18] reveals that\\\\nacross the 19 reviews tested, the combination of a simple\\\\nstructured boolean search (ssbs) and the ﬁrst 50 retrieval\\\\nof pubmed similar articles showed complete retrieval of all\\\\nreference standard articles in 14 of 19 reviews and never\\\\nless than 0.90 recall (appendix c at www.jclinepi.com pre-\\\\nsents data from the table by waffenschmidt graphically).\\\\nwaffenschmidt concluded ‘‘the combination of these two\\\\nsearch techniques that are independent of each other seems\\\\nto compensate the respective weaknesses.’’\\\\n\\\\nssbs search by waffenschmidt was constructed in\\\\npubmed from search terms selected for the indication and\\\\nintervention with pubmed’s narrow cq ﬁlter (category:\\\\ntherapy). their search using the similar articles feature (re-\\\\nlcits) did not use a set of seed articles; rather, the relcits\\\\nfunction was applied for each relevant citation previously\\\\nidentiﬁed in pubmed. their test articles were the included\\\\nstudies in 19 systematic reviews of drugs.\\\\n\\\\nagoritsas et al. also described search construction\\\\nmethods for searches based on cq and pubmed similar ar-\\\\nticles, tested for their ability to retrieve the included studies\\\\nof 30 cochrane reviews [17]. although their search con-\\\\nstruction methods differed from those used here, both\\\\n\\\\n\\\\x0c', 'journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\ncomplementary approaches to searching medline may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nmargaret sampsona,*, berry de bruijnb, christine urquhartc, kaveh shojaniad\\\\nalibrary and media services, children’s hospital of eastern ontario, 401 smyth road, ottawa, ontario k1h 8l1, canada\\\\n\\\\nbnational research councildinformation and communications technologies portfolio (nrc-ict), 1200 montreal rd, bldg m-50, ottawa, ontario k1a\\\\n\\\\ncdepartment of information studies, sunnybrook health sciences centre, aberystwyth university, llanbadarn campus, aberystwyth, wales sy23 3al,\\\\n\\\\n0r6, canada\\\\n\\\\nunited kingdom\\\\n\\\\ndcentre for quality improvement and patient safety, room h468, 2075 bayview avenue toronto, ontario m4n 3m5, canada\\\\n\\\\naccepted 7 march 2016; published online 11 march 2016\\\\n\\\\nabstract\\\\n\\\\nobjectives: to maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming boolean searches across multiple databases are common. although medline provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through boolean searches alone.\\\\n\\\\nstudy design and setting: recall of one boolean search method, the clinical query (cq), combined with a ranking method, support\\\\nvector machine (svm), or pubmed-related articles, was tested against a gold standard of studies added to 6 updated cochrane reviews and\\\\n10 agency for healthcare research and quality (ahrq) evidence reviews. for the ahrq sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nresults: recall of new studies was 0.69 for the cq, 0.66 for related articles, 0.50 for svm, 0.91 for the combination of cq and related\\\\narticles, and 0.89 for the combination of cq and svm. precision was 0.11 for cq and related articles combined, and 0.11 for cq and svm\\\\ncombined. related articles showed least stability over time.\\\\n\\\\nconclusions: the complementary combination of a boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in medline. (cid:1) 2016 elsevier inc. all rights reserved.\\\\n\\\\nkeywords: information retrieval; systematic reviews; support vector machine; clinical query; pubmed similar articles; searches; updating; medline\\\\n\\\\n1. introduction\\\\n\\\\nsystematic review searches need to have high recall.\\\\nmechanisms to achieve this usually include expansive\\\\nboolean searches of multiple databases. this approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nmedline gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. however,\\\\n\\\\nconﬂict of interest: none.\\\\nfunding: some of this work was funded by the us agency for health-\\\\ncare research and quality, department of health and human services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* corresponding author. tel.: 613-737-7600.\\\\ne-mail address: msampson@cheo.on.ca (m. sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 elsevier inc. all rights reserved.\\\\n\\\\nin 1994, a landmark article by dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through medline. recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in medline, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nsuccessful retrieval through a boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. the search of\\\\nmultiple databases can therefore be helpful. the target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. a text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. these tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nwhat is new?\\\\n\\\\nkey ﬁndings\\\\n(cid:1) searches using known relevant studies and the\\\\nsimilar articles feature of pubmed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) for a given question, if the boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. the two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) the precision of\\\\n\\\\nmethod appears better\\\\nexhaustive boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nwhat this adds to what was known?\\\\n(cid:1) the paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nit is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nwhat is the implication and what should change\\\\nnow?\\\\n(cid:1) using the simple and universally available pubmed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) if the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nmedline.\\\\n\\\\nto large retrievals with low precision [9]. these resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional boolean searches have\\\\nbeen sought [10].\\\\n\\\\nmost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nthese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. these methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nwe examined one method, support vector machine\\\\n(svm), and compared it with a simple and readily available\\\\nmethod based on the pubmed similar articles feature. we\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. we paired both\\\\nwith a focused boolean search within medline. we\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for svm and seed articles for the pubmed-\\\\n\\\\nrelated articles search. we therefore sought to determine\\\\nif a focused boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\ncomparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. in this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (cq), and pubmed-related articles are tested in a\\\\ncohort of six updated cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 agency for healthcare\\\\nresearch and quality (ahrq) evidence reports. the\\\\ncochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. the replication in the\\\\nahrq cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. all records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nother research [17,18] suggests that searches using the\\\\npubmed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a boolean-type search of\\\\nmedline. we tested an additional search method, svm,\\\\nin the cochrane and ahrq samples to permit comparison\\\\nwith our pubmed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the pubmed\\\\nsimilar articles method.\\\\n\\\\nthe aim of this article was to test whether the combined\\\\napproach of a focused boolean search paired with a second\\\\nsearch using the similar articles feature of pubmed or svm\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. methods\\\\n\\\\n2.1. formation of the study cohorts\\\\n\\\\nthis analysis uses a data set created for an updating\\\\nstudy sponsored by ahrq [16]. methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nbrieﬂy, cochrane reviews were identiﬁed through a\\\\nsearch of the acp journal club database (ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or medline(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nahrq reports were identiﬁed through the pubmed\\\\nquery ‘‘evid rep technol assess (summ)’’[journal:__\\\\njrid21544]. screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c110\\\\n\\\\nm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\nfor inclusion, all reviews must have included random-\\\\nized controlled trials (rcts) and provided meta-analysis\\\\nfor at least one outcome. the medline search strategies\\\\nhad to be reported in enough detail to permit replication.\\\\nfifteen ahrq evidence reports thought\\\\nlikely to have\\\\nimportant new evidence were selected and used to validate\\\\nthe updating methods used in the main cohort.\\\\n\\\\ncochrane reviews were selected to meet minimum qual-\\\\nity and relevance standards, as deﬁned by acp journal\\\\nclub [19]. we also required that the review had been up-\\\\ndated and that the text of both the original review and an\\\\nupdated version was available. the search for the original\\\\nreview must have included medline and at least one\\\\nother electronic bibliographic database and one or more\\\\nnondatabase method such as hand searching or checking\\\\nreference lists. such comprehensive searching was likely\\\\nto identify most relevant\\\\nliterature and form a useful\\\\ntraining set of included examples. at least 10 rcts or\\\\nquasi-rcts must have been included in the original review\\\\nto give an adequate training set for svm.\\\\n\\\\n2.2. test searches\\\\n\\\\n2.2.1. clinical query\\\\n\\\\nboolean searches were developed by a librarian experi-\\\\nenced in systematic review searches using a protocol\\\\n(appendix a at www.jclinepi.com). search strategies were\\\\ndeliberately simple, usually consisting of two or three\\\\nmedical subject heading (mesh) terms representing the\\\\npopulation and intervention of the review. this search\\\\nwas limited by a cq ﬁlter to ‘‘therapy (best balance of\\\\nsensitivity\\\\n‘‘randomized\\\\ncontrolled trial.pt. or\\\\nrandomized.mp. or placebo.mp.’’\\\\n[20]. boolean searches were run in ovid medline.\\\\n\\\\nspeciﬁcity)’’\\\\n\\\\nthat\\\\n\\\\nand\\\\n\\\\nis,\\\\n\\\\n2.2.2. related articles\\\\n\\\\nsearches using the pubmed similar articles feature were\\\\nperformed using the pubmed unique identiﬁers (pmid) of\\\\nthe three newest and three largest studies included in the\\\\noriginal review as seed articles. there was no replacement\\\\nin the event of overlap between the largest and newest sets,\\\\nor if one of those studies was not indexed in medline.\\\\n(all studies included in the original reviews were checked\\\\nfor indexing status in medline.) the resulting set was\\\\nlimited to the publication type rct and date limited to\\\\nthe period since the search date of the original review.\\\\n\\\\n2.2.3. support vector machine\\\\n\\\\nthe svm searches used for this project have not been\\\\npreviously described so the methods will be reported in\\\\nmore detail. svm is a well established and broadly applied\\\\nclassiﬁcation method from machine learning [21,22]. a\\\\nclassiﬁcation task is closely related to an information\\\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\\\nthe collection. as svm is a supervised algorithm,\\\\nit\\\\n\\\\nrequires training examples with known labels. it then places\\\\nthese training examples in a high-dimensional vector space\\\\nin such a way that examples of one class are separated from\\\\nexamples of the other class with the greatest distance to the\\\\nseparating boundary. predicting the class of a new instance\\\\nhappens by placing it in the same space and determining on\\\\nwhich side of the boundary it falls. the distance to the\\\\nboundary is an indication of the prediction conﬁdence and\\\\nis used to rank instances (documents) as is essential in in-\\\\nformation retrieval tasks or to provide a cutoff threshold.\\\\n\\\\nin our setup, the training set consisted of search results\\\\nfrom the original review formed with included instances\\\\nbeing the studies included in the review (the relevant re-\\\\ntrievals) and excluded instances being the studies found\\\\nby the search used in the original review but excluded from\\\\nthe review (irrelevant retrievals). the new examples to be\\\\nclassiﬁed were articles indexed since the date of the search\\\\nperformed in the original review.\\\\n\\\\nsvm searches were run on medline data stored\\\\nlocally at\\\\nthe national research council of canada.\\\\nmedline was refreshed before running the searches.\\\\nthe medline records were represented as bags of fea-\\\\ntures, where features consisted of lowercased words and\\\\nword combinations (up to four words) from the title and ab-\\\\nstract ﬁelds, full mesh terms, and contents of all other\\\\nﬁelds in the medline record. features were uniformly\\\\nweighted. the svm implementation used was svm light\\\\n[23] used with a linear kernel.\\\\n\\\\nthe approach was piloted with one ahrq evidence\\\\nreport, and various conﬁgurations were tried, observing\\\\nthe placement of several known new relevant studies within\\\\nthe retrieval. the medline set was ﬁltered with the\\\\nrevised highly sensitive search strategy (hsss) [24]. pre-\\\\nliminary testing indicated that this ﬁlter would retrieve\\\\n99.1% of medline-indexed relevant new evidence from\\\\nthe larger cohort of journal-published systematic reviews\\\\n(n 5 72) and cochrane reviews (n 5 27). this hsss ﬁlter\\\\nwas added primarily to improve processing speed, relative\\\\nto running the classiﬁer against all of medline. with\\\\nthe ﬁlter, processing for one systematic review took about\\\\n10 minutes. next, a coarse-grained ﬁlter was applied;\\\\nincluded and excluded instances were combined in a single\\\\nset labeled pos. adjacent pubmed ids (the next higher\\\\npmid to each member of pos, as long as that higher num-\\\\nber was not itself a member of pos) were combined in a set\\\\nto represent world. the model was trained on pos vs.\\\\nworld, using words and phrases from title, abstract,\\\\nauthor names, and journal name. this model was run\\\\nagainst the records passing the hsss ﬁlter.\\\\n\\\\nfour different subsets of\\\\n\\\\nthe svm retrieval were\\\\nexamineddone set included all records with a relevance\\\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\\\n(svm200point5). other sets consisted of all records with\\\\nrelevance scores of 0.95 or more (svm95), 0.90 or more\\\\n(svm90), and 0.80 or more (svm80). pubmed id and\\\\nrelevance scores were recorded, and pubmed ids were\\\\n\\\\n\\\\x0cm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\n111\\\\n\\\\ntable 1. characteristics of the included reviews\\\\n\\\\ncochrane\\\\nreviews; n\\\\n\\\\nahrq evidence\\\\n\\\\nreports; n\\\\n\\\\nincorporated into the database of records for the reviewers\\\\nto screen. only the results from svm200point5 are re-\\\\nported here and are described as svm.\\\\n\\\\n2.3. determining performance of the test searches\\\\n\\\\nfor the updated cochrane reviews, reference standard ar-\\\\nticles were those studies included in the updated review that\\\\nwere not in the original and which entered medline after\\\\nthe search date of the original review. for the ahrq evi-\\\\ndence reports, the full retrieval set was screened. records\\\\nfound relevant by the consensus of reviewers were consid-\\\\nered reference standard articles. recall was the proportion\\\\nof reference standard articles identiﬁed by the search:\\\\n\\\\nnumber of reference standard articles found\\\\ntotal number of reference standard articles\\\\n\\\\nbiovenn software was used to analyze the overlap be-\\\\ntween the retrieval of relevant articles from the three\\\\nsearches and to create venn diagrams [25].\\\\n\\\\nall included reviews were classiﬁed into clinical area\\\\nbased on factors such as isi journal classiﬁcation, the co-\\\\nchrane collaboration review group where the topic might\\\\nbe placed and the high level mesh term under which the pop-\\\\nulation (condition) would be indexed. performance of the\\\\nsearches in different clinical areas was displayed graphically\\\\nfor the searches both alone and in combinations, to allow ex-\\\\namination of differences in parallelism, level, and ﬂatness.\\\\n\\\\nprecision is the proportion of all retrieved records that\\\\n\\\\nare relevant:\\\\n\\\\nnumber of reference standard articles found\\\\n\\\\ntotal number of records retrieved\\\\n\\\\nthe inverse of precision is the number needed to read to\\\\nﬁnd one eligible study, thus precision inﬂuences the work\\\\nof the review. precision was calculated only for the ahrq\\\\nevidence reviews. because not all candidates retrieved by\\\\nthe cochrane searches were assessed, precision could not\\\\nbe established in that sample.\\\\n\\\\n2.4. determining stability of results over time\\\\n\\\\nthe cq and related article searches, originally run in\\\\nmarch 2008, were repeated in february 2015 in the ahrq\\\\ncohort. recall of relevant items was compared with the\\\\noriginal retrievals to determine if any changes introduced\\\\nby national library of medicine might invalidate ﬁndings.\\\\nas svm conﬁguration is under the control of the investiga-\\\\ntors, it was not retested.\\\\n\\\\n3. results\\\\n\\\\nsix cochrane reviews and 10 ahrq evidence reports\\\\nmet all inclusion criteria (appendix b at www.jclinepi.\\\\ncom). characteristics of the included reviews are shown\\\\nin table 1.\\\\n\\\\n9a\\\\n1\\\\n3\\\\n\\\\n3\\\\n\\\\nd\\\\n1\\\\n\\\\nd\\\\n1\\\\n2\\\\n1\\\\n1\\\\n\\\\n1\\\\nd\\\\nd\\\\n\\\\nd\\\\n3\\\\n3\\\\n4\\\\n\\\\ncharacteristic\\\\n\\\\ntherapy evaluated\\\\n\\\\nmedications\\\\nmedical devices\\\\nprocedures\\\\n\\\\nclinical topic area\\\\n\\\\ncardiac and cardiovascular\\\\n\\\\nsystems\\\\n\\\\ncritical care\\\\nendocrinology and\\\\n\\\\nmetabolism\\\\n\\\\ninfectious disease\\\\nclinical neurology\\\\nobstetrics and gynecology\\\\noncology\\\\nperipheral vascular\\\\n\\\\ndiseases\\\\npsychiatry\\\\nrespiratory systems\\\\nurology and nephrology\\\\n\\\\npublication period\\\\n\\\\nmarch 1997eapril 1999\\\\nmay 1999ejune 2001\\\\njuly 2001eaugust 2003\\\\nseptember 2003e\\\\ndecember 2005\\\\n\\\\nmedian included trials\\\\n\\\\nmedian included\\\\n\\\\nparticipants\\\\n\\\\nmedline coverage of\\\\n\\\\n5\\\\n1\\\\nd\\\\n\\\\nd\\\\n\\\\n1\\\\nd\\\\n\\\\n3\\\\nd\\\\nd\\\\nd\\\\nd\\\\n\\\\nd\\\\n1\\\\n1\\\\n\\\\n3\\\\n1\\\\n2\\\\nd\\\\n\\\\n17 (iqr, 14e20)\\\\n\\\\n96 (iqr,\\\\n\\\\n31.75e121.5)\\\\n8,679 (iqr,\\\\n22,830 (iqr, 14,\\\\n4,085e50,109)\\\\n172e49,687)\\\\n99/107 (92.5%) 969/980 (98.8%)\\\\n\\\\nincluded articles\\\\nabbreviations: ahrq, agency for healthcare research and qual-\\\\n\\\\nity; iqr, interquartile range.\\\\n\\\\na some ahrq reviews included more than one class of therapy.\\\\n\\\\nrecall of new relevant studies is shown in table 2. the\\\\nupdated cochrane reviews had 20 new studies identiﬁed by\\\\nthe review authors and added in the updates. recall of these\\\\n20 ranged from 1.00 for the related articles method to a low\\\\nof 0.80 cqs (table 2). our team identiﬁed 277 new studies\\\\nas relevant for inclusion in the 10 evidence reports that\\\\nwere updated. all test searches showed lower recall for this\\\\ncohort than for the cochrane reviews and here the cq out-\\\\nperformed both ranking searches (table 2).\\\\n\\\\n3.1. recall of cq combined with a ranking method\\\\n\\\\nof the 297 new relevant studies identiﬁed, the combina-\\\\ntion of cq and related articles searches identiﬁed 270\\\\n(overall\\\\nin\\\\nthe cochrane cohort and 0.90 (250/277) in the ahrq\\\\ncohort.\\\\n\\\\nrecall of 0.91). recall was 1.00 (20/20)\\\\n\\\\nthe combination of cq and svm identiﬁed 263 studies\\\\n(overall recall of 0.89). recall was 1.00 (20/20) in the\\\\ncochrane cohort and 0.88 (243/277) in the ahrq cohort.\\\\nwhen all three methods were used together, recall was\\\\n0.997 with 296 of the 297 studies identiﬁed.\\\\n\\\\n\\\\x0c112\\\\n\\\\nm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\ntable 2. recall of eligible studies by the search methods\\\\n\\\\ncochrane reviews\\\\n\\\\nahrq evidence\\\\n\\\\nreports\\\\n\\\\nretrieval method\\\\n\\\\nclinical query\\\\nrelated articles\\\\nsvm\\\\ntotal\\\\n\\\\nn\\\\n\\\\n16\\\\n20\\\\n19\\\\n20\\\\n\\\\nrecall\\\\n\\\\n0.80\\\\n1.00\\\\n0.95\\\\n\\\\nn\\\\n\\\\n188\\\\n176\\\\n129\\\\n277\\\\n\\\\nrecall\\\\n\\\\n0.68\\\\n0.64\\\\n0.47\\\\n\\\\nabbreviations: ahrq, agency for healthcare research and qual-\\\\n\\\\nity; svm, support vector machine.\\\\n\\\\nacross the two cohorts, the overlap and unique component from\\\\nthe retrieval of relevant records by the test searches was examined\\\\n(fig. 1).\\\\n\\\\n3.2. consistency across clinical areas\\\\n\\\\nthirteen clinical areas were represented in the larger\\\\nstudy that included 72 journal-published reviews, but only\\\\neight of those clinical areas had two or more new relevant\\\\nstudies in the smaller cochrane and ahrq cohorts. fig. 2\\\\nshows recall of new studies by the three types of searches,\\\\nfor these eight clinical areas.\\\\n\\\\ncombined recall of the cq boolean search paired with a\\\\nranking search is shown in fig. 3. the combination showed\\\\ncomplete recall of relevant new studies in three of eight\\\\nareas when cq was paired with related articles and recall\\\\nof 0.67 or higher in all areas. four of eight clinical areas\\\\nhad complete recall for the combination of cq and svm,\\\\nand recall was 0.80 or higher in all areas. even for periph-\\\\neral vascular disease, where all three searches performed\\\\n\\\\nfig. 2. recall of new studies by clinical area for each search method.\\\\nabbreviations: svm, support vector machine.\\\\n\\\\nfairly poorly individually, recall was 0.76 for the cq/\\\\nrelated articles and 0.90 in the cq/svm pairing.\\\\n\\\\n3.3. search precision\\\\n\\\\nprecision, across the 10 ahrq reviews, was 0.11 for the\\\\ncq, 0.22 for related articles, and 0.19 for svm (table 3).\\\\n\\\\nfig. 1. overlapping and unique retrieval of relevant studies by each\\\\nsearch method. the size of the circle is proportional to the size of\\\\nthe retrieval, but the overlap and unique portions are approximations.\\\\nexact ﬁgures for the components are labeled. abbreviations: svm,\\\\nsupport vector machine.\\\\n\\\\nfig. 3. recall of new studies by clinical area for search methods in\\\\ncombination. abbreviations: cq, clinical query; svm, support vector\\\\nmachine.\\\\n\\\\n\\\\x0ctable 3. overall precision in the ahrq sample\\\\n\\\\n4. discussion\\\\n\\\\nm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\n113\\\\n\\\\nretrieval method\\\\n\\\\nclinical query\\\\nrelated articles\\\\nsvm\\\\nclinical query þ related\\\\nclinical query þ svm\\\\nclinical query þ related\\\\n\\\\narticles\\\\n\\\\neligible\\\\nstudies\\\\nretrieved\\\\n\\\\n187\\\\n176\\\\n128\\\\n250\\\\n\\\\n243\\\\n276\\\\n\\\\nno. of candidates\\\\n\\\\nretrieved\\\\n\\\\nprecision\\\\n\\\\n1,637\\\\n814\\\\n659\\\\n2,318a\\\\n\\\\n2,247a\\\\n3,264a\\\\n\\\\n0.11\\\\n0.22\\\\n0.19\\\\n0.11\\\\n\\\\n0.11\\\\n0.08\\\\n\\\\narticles þ svm\\\\nabbreviations: ahrq, agency for healthcare research and\\\\n\\\\nquality; svm, support vector machine.\\\\n\\\\na number of candidates after removal of duplicate records.\\\\n\\\\nprecision for the individual reviews ranged from 0.0 to 0.38\\\\nfor cqs, 0.004 to 0.80 for related articles, and 0.01 to 0.43\\\\nfor svm. overall precision was 0.11 for cq and either\\\\nrelated articles or svm. overall precision was 0.08 when\\\\ncq and both related articles and svm were used. for\\\\nranked searches where a ﬁxed number of records will be\\\\nscreened, that number is always the denominator, so preci-\\\\nsion will tend to increase as the number of relevant records\\\\nincreasesdthe maximum retrieval size for svm was cap-\\\\nped at 200 records.\\\\n\\\\n3.4. retrieval consistency over time\\\\n\\\\nsearches were retested for the ahrq cohort using\\\\npubmed results obtained february 24, 2015. recall of the\\\\nrelated articles searches across the 10 ahrq reports was\\\\n0.64 originally and 0.50 when repeated (fig. 4). of the\\\\nthree extreme cases, one rose from 0.00 to 0.80, one re-\\\\nmained at 0.00, and one dropped from 1.00 to 0.60. cq\\\\nhad overall\\\\nrecall of 0.68 originally and 0.70 when\\\\nrepeated. changes in extreme cases were minimal. overall\\\\ncombined recall of cq and related article was 0.90 origi-\\\\nnally,\\\\nfalling to 0.85 when repeated. considering the\\\\nextreme values, all review with combined recall of 1.00\\\\noriginally remained at 1.00, whereas the one review with\\\\n0.00 recall in combination showed recall of 0.80 when\\\\nrepeated.\\\\n\\\\n1.0\\\\n0.9\\\\n0.8\\\\n0.7\\\\n0.6\\\\n0.5\\\\n0.4\\\\n0.3\\\\n0.2\\\\n0.1\\\\n0.0\\\\n\\\\noriginal\\\\n\\\\n2015\\\\n\\\\nclinical query\\\\n\\\\nrelated ar(cid:415)cles\\\\n\\\\ncombined\\\\n\\\\nfig. 4. recall of new studies for search methods, alone and in combi-\\\\nnation originally (left) and when retested in 2015 (right).\\\\n\\\\nwe expected that\\\\n\\\\nthe sophisticated svm approach\\\\nwould perform well when paired with more stripped-\\\\ndown, focused boolean searches. indeed, the two together\\\\nwere able to replace the multidatabase, multimodel\\\\nsearches used by the original review teams in updating\\\\nthe cochrane searches. the new relevant studies for the co-\\\\nchrane reviews may have been relatively easy to ﬁnd, but\\\\nthe pairing was also effective in the ahrq set, and there\\\\nshowed better precision than is usually seen with traditional\\\\nsearches for systematic review of rcts [9]. the ahrq set\\\\nwas formed from the new studies identiﬁed for more com-\\\\nplex interventions in a rigorous, well-funded study [16].\\\\n\\\\nwe were surprised that the related articles approach per-\\\\nformed almost as well as svm when used in combination\\\\nwith the cq. the related article search has advantages over\\\\nsvmdit requires far less data preparation, and no special\\\\nsoftware is needed for its use. this makes it useful not only\\\\nin updating, but also, if appropriate seed articles can be\\\\nfound, in original reviews.\\\\n\\\\nthat both svm and related articles sometimes showed\\\\npoor recall when used alone, but consistently good recall\\\\nwhen used with a boolean method suggests that there is\\\\nreal beneﬁt in using complementary search methods to\\\\nquerying medline. the related article method is not very\\\\ntime consuming, and easily added to other planned search\\\\nefforts, while the third method used here, svm, is more\\\\ntechnical. the combined performance of all three methods\\\\nwas surprising, but such a setup might become unpractical\\\\nin day-to-day use.\\\\n\\\\nother investigators have similar ﬁndings. examination\\\\nof the supplemental material presented in the appendix c\\\\nto the article by waffenschmidt et al. [18] reveals that\\\\nacross the 19 reviews tested, the combination of a simple\\\\nstructured boolean search (ssbs) and the ﬁrst 50 retrieval\\\\nof pubmed similar articles showed complete retrieval of all\\\\nreference standard articles in 14 of 19 reviews and never\\\\nless than 0.90 recall (appendix c at www.jclinepi.com pre-\\\\nsents data from the table by waffenschmidt graphically).\\\\nwaffenschmidt concluded ‘‘the combination of these two\\\\nsearch techniques that are independent of each other seems\\\\nto compensate the respective weaknesses.’’\\\\n\\\\nssbs search by waffenschmidt was constructed in\\\\npubmed from search terms selected for the indication and\\\\nintervention with pubmed’s narrow cq ﬁlter (category:\\\\ntherapy). their search using the similar articles feature (re-\\\\nlcits) did not use a set of seed articles; rather, the relcits\\\\nfunction was applied for each relevant citation previously\\\\nidentiﬁed in pubmed. their test articles were the included\\\\nstudies in 19 systematic reviews of drugs.\\\\n\\\\nagoritsas et al. also described search construction\\\\nmethods for searches based on cq and pubmed similar ar-\\\\nticles, tested for their ability to retrieve the included studies\\\\nof 30 cochrane reviews [17]. although their search con-\\\\nstruction methods differed from those used here, both\\\\n\\\\n\\\\x0c114\\\\n\\\\nm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\nsearches were derived using standard methods. their\\\\napproach to the structured boolean search used terms from\\\\nthe population, intervention, and comparison with the cq,\\\\nlimited to humans and english. two clinicians selected the\\\\npubmed similar articles seeds from the initial pubmed\\\\nretrieval. they noted that no one method provided consis-\\\\ntently high retrieval [17].\\\\n\\\\nthus, the robust nature of this pairing of boolean and\\\\nnon-boolean searches has been shown in several contexts,\\\\ngiving support\\\\nto the hypothesis that methods such as\\\\nrelated articles and svm can compensate for the variation\\\\ninherent\\\\nin selecting search terms or assigning subject\\\\nheadings during indexing.\\\\n\\\\nconsidering stability of the searches over time, the\\\\nminimal change seen in the performance of the cq is\\\\nlikely explained by indexing changes of a few records.\\\\nthe computation appears not to have changed. this sug-\\\\ngests that indexing changes may also impact svm results\\\\nover time; however, this is likely to result in improved\\\\nperformance, as was seen with the cq. the computation\\\\nof similar articles is still described by national library\\\\nof medicine as being based on the algorithm described\\\\nby kim et al. [26]. that algorithm would suggest changes\\\\nover time in the nearest neighbor score as the frequency of\\\\ncertain terms in the medline corpus changes. a full\\\\nexploration of the changes in nearest neighbor scores of\\\\nsimilar articles over time is beyond the scope of this\\\\nstudy. it should be noted that when the similar articles\\\\nfeature was initially studied, it was simple to submit seed\\\\narticles, and then add limits such as date or rct publica-\\\\ntion type. on retesting, such additional limits were more\\\\ncomplex to apply, and the elink utility seemed to be the\\\\nmost practical approach to identifying the related articles\\\\n[27].\\\\n\\\\nthe beneﬁt of the complementary searches, relative to\\\\nboolean searching alone, was greatest for ahrq evidence\\\\nreports and less for the simpler cochrane reviews, suggest-\\\\ning that this approach may be particularly useful for more\\\\ncomplex evidence. there are examples of its utility in the\\\\nliterature. for example, in a realist review of a multidisci-\\\\nplinary body of\\\\nliterature identifying six domains of\\\\nclinical practice guideline implementability, the use of\\\\npubmed’s similar articles feature as a third stage of search-\\\\ning identiﬁed 131 records of which 104 were relevant [28].\\\\nin a systematic review of evidence on the links between\\\\npatient experience and clinical safety and effectiveness,\\\\nthe authors used pubmed similar articles to snowball on\\\\narticles\\\\nidentiﬁed through an embase search to\\\\novercome the limitations of predeﬁned searches for com-\\\\nplex evidence [29].\\\\n\\\\nthere is always the possibility that a search, or even a\\\\npair of searches, will fail. one method to detect such\\\\nfailures is to test whether the search strategies ﬁnd known\\\\nrelevant items [30]. in the updating case, this is easily done\\\\nusing the included studies of the original review as a test\\\\n\\\\nset, allowing the review to determine the medline\\\\ncoverage of their particular topic at the same time.\\\\n\\\\n4.1. limitations\\\\n\\\\nthere are two limitations to our proposed strategy. other\\\\ndatabases should be searched in the unusual event that\\\\nnumerous studies, representing more than a small propor-\\\\ntion of the total n, are not included in medline. second,\\\\nwhen it is important to ﬁnd articles too new to be indexed\\\\nby medline, systematic reviewers may wish to conduct a\\\\nsimple pubmed search limited to the nonindexed subsets\\\\n[31,32].\\\\n\\\\n5. conclusion\\\\n\\\\nthe general approach of a boolean plus a ranking\\\\nsearch is effective in medline retrieval for systematic\\\\nreviews. very high levels of identiﬁcation of relevant\\\\nmedline records, with adequate precision, are possible\\\\nusing a focused boolean search complemented by a docu-\\\\nment similarity or ranking method. the efﬁcacy of a\\\\nfocused boolean search paired with a search using the\\\\npubmed similar articles feature is in agreement with pre-\\\\nvious studies [17,18], and this study shows that this com-\\\\nplementary effect also occurs when svm is used as the\\\\nadditional method. the beneﬁt of using two complemen-\\\\ntary approaches to achieving high recall in medline is\\\\na robust effect.\\\\n\\\\npubmed-related articles is a parsimonious method, as it\\\\nis readily available to all review teams without cost. the\\\\napproach is robust across clinical domains, and the effect\\\\nhas now been demonstrated in several samples. the method\\\\nmay be sufﬁcient for updating systematic reviews of inter-\\\\nventions and may be used for new reviews of interventions\\\\nwhen paired with a trials registry search. it is likely to work\\\\nin any type of search where medline provides good sub-\\\\nject coverage even if retrieval through traditional search\\\\nmethods has been challenging.\\\\n\\\\nacknowledgments\\\\n\\\\nmohamed ansari and jun ji undertook screening and\\\\nrelevance assessment of candidate studies retrieved by the\\\\ntest\\\\nsearches, tamara rader developed the boolean\\\\nsearches, and raymond daniel processed related articles\\\\nsearches and obtained full-text articles. david moher cosu-\\\\npervised margaret sampson doctoral dissertation and pro-\\\\nvided invaluable advice and guidance.\\\\n\\\\nsupplementary data\\\\n\\\\nsupplementary data related to this article can be found at\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004.\\\\n\\\\n\\\\x0c', 'journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\ncomplementary approaches to searching medline may be\\\\n\\\\nsufﬁcient for updating systematic reviews\\\\n\\\\nmargaret sampsona,*, berry de bruijnb, christine urquhartc, kaveh shojaniad\\\\nalibrary and media services, children’s hospital of eastern ontario, 401 smyth road, ottawa, ontario k1h 8l1, canada\\\\n\\\\nbnational research councildinformation and communications technologies portfolio (nrc-ict), 1200 montreal rd, bldg m-50, ottawa, ontario k1a\\\\n\\\\ncdepartment of information studies, sunnybrook health sciences centre, aberystwyth university, llanbadarn campus, aberystwyth, wales sy23 3al,\\\\n\\\\n0r6, canada\\\\n\\\\nunited kingdom\\\\n\\\\ndcentre for quality improvement and patient safety, room h468, 2075 bayview avenue toronto, ontario m4n 3m5, canada\\\\n\\\\naccepted 7 march 2016; published online 11 march 2016\\\\n\\\\nabstract\\\\n\\\\nobjectives: to maximize the proportion of relevant studies identiﬁed for inclusion in systematic reviews (recall), complex time-\\\\nconsuming boolean searches across multiple databases are common. although medline provides excellent coverage of health science\\\\nevidence, it has proved challenging to achieve high levels of recall through boolean searches alone.\\\\n\\\\nstudy design and setting: recall of one boolean search method, the clinical query (cq), combined with a ranking method, support\\\\nvector machine (svm), or pubmed-related articles, was tested against a gold standard of studies added to 6 updated cochrane reviews and\\\\n10 agency for healthcare research and quality (ahrq) evidence reviews. for the ahrq sample, precision and temporal stability were\\\\nexamined for each method.\\\\n\\\\nresults: recall of new studies was 0.69 for the cq, 0.66 for related articles, 0.50 for svm, 0.91 for the combination of cq and related\\\\narticles, and 0.89 for the combination of cq and svm. precision was 0.11 for cq and related articles combined, and 0.11 for cq and svm\\\\ncombined. related articles showed least stability over time.\\\\n\\\\nconclusions: the complementary combination of a boolean search strategy and a ranking strategy appears to provide a robust method\\\\n\\\\nfor identifying relevant studies in medline. (cid:1) 2016 elsevier inc. all rights reserved.\\\\n\\\\nkeywords: information retrieval; systematic reviews; support vector machine; clinical query; pubmed similar articles; searches; updating; medline\\\\n\\\\n1. introduction\\\\n\\\\nsystematic review searches need to have high recall.\\\\nmechanisms to achieve this usually include expansive\\\\nboolean searches of multiple databases. this approach\\\\nleads to long development times for the searches [1], neces-\\\\nsitates accessing multiple sources that may not be acces-\\\\nsible in some institutions [2], and entails time-consuming\\\\nremoval of duplicate records for articles indexed in more\\\\nthan one of the databases searched [3].\\\\n\\\\nmedline gives excellent coverage of most biomed-\\\\nical topics, in particular, intervention studies. however,\\\\n\\\\nconﬂict of interest: none.\\\\nfunding: some of this work was funded by the us agency for health-\\\\ncare research and quality, department of health and human services\\\\n(contract no. 290-02-0021).\\\\n\\\\n* corresponding author. tel.: 613-737-7600.\\\\ne-mail address: msampson@cheo.on.ca (m. sampson).\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004\\\\n0895-4356/(cid:1) 2016 elsevier inc. all rights reserved.\\\\n\\\\nin 1994, a landmark article by dickersin et al. [4] estab-\\\\nlished that only about half the studies included in system-\\\\natic reviews were identiﬁed through medline. recent\\\\nresearch has demonstrated that a much higher percentage\\\\nis present in medline, but sometimes their retrieval is\\\\nproblematic [5e8].\\\\n\\\\nsuccessful retrieval through a boolean search is oper-\\\\nator dependent, with search performance being inﬂuenced\\\\nby skill of the indexer and the searcher. the search of\\\\nmultiple databases can therefore be helpful. the target re-\\\\ncords may be indexed differently in the second, third, or\\\\nsubsequent source searched, increasing the probability\\\\nof a match between the search terms entered and the in-\\\\ndexing of the additional records. a text search, querying\\\\nterms appearing in the title or abstract, may help improve\\\\nretrieval, as may the use of indexing terms that are broad-\\\\ner (less speciﬁc) than, or related to, the single best index-\\\\ning term. these tactics on the part of the searcher lead\\\\n\\\\n\\\\x0cm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\n109\\\\n\\\\nwhat is new?\\\\n\\\\nkey ﬁndings\\\\n(cid:1) searches using known relevant studies and the\\\\nsimilar articles feature of pubmed will identify\\\\nand rank additional articles of potential relevance.\\\\n(cid:1) for a given question, if the boolean search has low\\\\nrecall, the ranking search tends to have higher\\\\nrecall, and vice versa. the two approaches comple-\\\\nment each other.\\\\n\\\\n(cid:1) the precision of\\\\n\\\\nmethod appears better\\\\nexhaustive boolean searches.\\\\n\\\\nthis complementary paired\\\\nthan the precision of\\\\n\\\\nwhat this adds to what was known?\\\\n(cid:1) the paired approach performed well regardless of\\\\nwhich of two tested similarity searches were used.\\\\nit is the use of independent retrieval methods that is\\\\nimportant.\\\\n\\\\nwhat is the implication and what should change\\\\nnow?\\\\n(cid:1) using the simple and universally available pubmed\\\\nsimilar feature makes this paired approach prac-\\\\ntical for most systematic review teams.\\\\n\\\\n(cid:1) if the paired complementary approach is used, the\\\\nrecall may be sufﬁcient to consider using only\\\\nmedline.\\\\n\\\\nto large retrievals with low precision [9]. these resulting\\\\nproblems may be particularly challenging for complex or\\\\nnewly emerging interventions with highly variant terminol-\\\\nogy, where alternatives to traditional boolean searches have\\\\nbeen sought [10].\\\\n\\\\nmost recent systematic review information retrieval\\\\nresearch has focused on text mining approaches [11e14].\\\\nthese approaches often harvest an intentionally overinclu-\\\\nsive set of records and then use machine learning, similarity\\\\nranking and other techniques to reﬁne the set to identify the\\\\nmaterial most likely to be relevant, thereby reducing human\\\\nscreening effort. these methods show promise, but are not\\\\nyet widely available to reviewers.\\\\n\\\\nwe examined one method, support vector machine\\\\n(svm), and compared it with a simple and readily available\\\\nmethod based on the pubmed similar articles feature. we\\\\ncall this method related articles to distinguish the method\\\\nfrom the similar articles feature itself. we paired both\\\\nwith a focused boolean search within medline. we\\\\ntested this approach in an updating context where studies\\\\nincluded in the original review comprise the reference\\\\nstandard for svm and seed articles for the pubmed-\\\\n\\\\nrelated articles search. we therefore sought to determine\\\\nif a focused boolean search paired with one of the search\\\\nmethods that does not depend on operator skill could pro-\\\\nvide consistently complete retrieval of relevant new studies.\\\\ncomparison of a number of searches, including two\\\\ntested here, has been previously reported [15]. in this cur-\\\\nrent article, two of the most successful methods in a larger\\\\nsample of 72 journal-published systematic reviews, clinical\\\\nquery (cq), and pubmed-related articles are tested in a\\\\ncohort of six updated cochrane reviews, as well as in a pre-\\\\nviously unreported sample of 10 agency for healthcare\\\\nresearch and quality (ahrq) evidence reports. the\\\\ncochrane reviews provide a true gold standard as updates\\\\nwere made by the review team based on evidence identiﬁed\\\\nthrough comprehensive searches; however, the new relevant\\\\nstudies proved fairly easy to ﬁnd. the replication in the\\\\nahrq cohort, of more complex interventions, provided a\\\\nmeans to validate the generalizability of the approach\\\\n[16]. all records were assessed for eligibility by two re-\\\\nviewers, and this complete screening allowed the precision\\\\nof the methods to be calculated for the ﬁrst time.\\\\n\\\\nother research [17,18] suggests that searches using the\\\\npubmed similar articles feature are effective in increasing\\\\nrecall of relevant items for reviews or more general clinical\\\\nsearching when combined with a boolean-type search of\\\\nmedline. we tested an additional search method, svm,\\\\nin the cochrane and ahrq samples to permit comparison\\\\nwith our pubmed-related articles search and assess whether\\\\nthe complementary effect generalized beyond the pubmed\\\\nsimilar articles method.\\\\n\\\\nthe aim of this article was to test whether the combined\\\\napproach of a focused boolean search paired with a second\\\\nsearch using the similar articles feature of pubmed or svm\\\\ncan yield high recall with reasonable precision.\\\\n\\\\n2. methods\\\\n\\\\n2.1. formation of the study cohorts\\\\n\\\\nthis analysis uses a data set created for an updating\\\\nstudy sponsored by ahrq [16]. methods for the selection\\\\nof the cohorts, search approaches tested, and rigorous\\\\nmechanisms to screen search results for relevance have\\\\nbeen previously reported, along with the criteria to deter-\\\\nmine if a review was in need of update [16].\\\\n\\\\nbrieﬂy, cochrane reviews were identiﬁed through a\\\\nsearch of the acp journal club database (ovid) using the\\\\nstrategy:\\\\n\\\\n1. review$.ti. 2. meta-analy$.mp. 3. data sources.ab. 4.\\\\n(search$ or medline(cid:3)).ab. 5. or/1-4 6. limit 5 to articles\\\\nwith commentary.\\\\n\\\\nahrq reports were identiﬁed through the pubmed\\\\nquery ‘‘evid rep technol assess (summ)’’[journal:__\\\\njrid21544]. screening was undertaken in two phases with\\\\ntwo reviewers reaching consensus on eligibility. screening\\\\ncontinued until the predetermined sample size was reached.\\\\n\\\\n\\\\x0c110\\\\n\\\\nm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\nfor inclusion, all reviews must have included random-\\\\nized controlled trials (rcts) and provided meta-analysis\\\\nfor at least one outcome. the medline search strategies\\\\nhad to be reported in enough detail to permit replication.\\\\nfifteen ahrq evidence reports thought\\\\nlikely to have\\\\nimportant new evidence were selected and used to validate\\\\nthe updating methods used in the main cohort.\\\\n\\\\ncochrane reviews were selected to meet minimum qual-\\\\nity and relevance standards, as deﬁned by acp journal\\\\nclub [19]. we also required that the review had been up-\\\\ndated and that the text of both the original review and an\\\\nupdated version was available. the search for the original\\\\nreview must have included medline and at least one\\\\nother electronic bibliographic database and one or more\\\\nnondatabase method such as hand searching or checking\\\\nreference lists. such comprehensive searching was likely\\\\nto identify most relevant\\\\nliterature and form a useful\\\\ntraining set of included examples. at least 10 rcts or\\\\nquasi-rcts must have been included in the original review\\\\nto give an adequate training set for svm.\\\\n\\\\n2.2. test searches\\\\n\\\\n2.2.1. clinical query\\\\n\\\\nboolean searches were developed by a librarian experi-\\\\nenced in systematic review searches using a protocol\\\\n(appendix a at www.jclinepi.com). search strategies were\\\\ndeliberately simple, usually consisting of two or three\\\\nmedical subject heading (mesh) terms representing the\\\\npopulation and intervention of the review. this search\\\\nwas limited by a cq ﬁlter to ‘‘therapy (best balance of\\\\nsensitivity\\\\n‘‘randomized\\\\ncontrolled trial.pt. or\\\\nrandomized.mp. or placebo.mp.’’\\\\n[20]. boolean searches were run in ovid medline.\\\\n\\\\nspeciﬁcity)’’\\\\n\\\\nthat\\\\n\\\\nand\\\\n\\\\nis,\\\\n\\\\n2.2.2. related articles\\\\n\\\\nsearches using the pubmed similar articles feature were\\\\nperformed using the pubmed unique identiﬁers (pmid) of\\\\nthe three newest and three largest studies included in the\\\\noriginal review as seed articles. there was no replacement\\\\nin the event of overlap between the largest and newest sets,\\\\nor if one of those studies was not indexed in medline.\\\\n(all studies included in the original reviews were checked\\\\nfor indexing status in medline.) the resulting set was\\\\nlimited to the publication type rct and date limited to\\\\nthe period since the search date of the original review.\\\\n\\\\n2.2.3. support vector machine\\\\n\\\\nthe svm searches used for this project have not been\\\\npreviously described so the methods will be reported in\\\\nmore detail. svm is a well established and broadly applied\\\\nclassiﬁcation method from machine learning [21,22]. a\\\\nclassiﬁcation task is closely related to an information\\\\nretrieval task, once the task is framed as predicting a ‘‘rele-\\\\nvant’’ vs. ‘‘nonrelevant’’ label to instances (documents) in\\\\nthe collection. as svm is a supervised algorithm,\\\\nit\\\\n\\\\nrequires training examples with known labels. it then places\\\\nthese training examples in a high-dimensional vector space\\\\nin such a way that examples of one class are separated from\\\\nexamples of the other class with the greatest distance to the\\\\nseparating boundary. predicting the class of a new instance\\\\nhappens by placing it in the same space and determining on\\\\nwhich side of the boundary it falls. the distance to the\\\\nboundary is an indication of the prediction conﬁdence and\\\\nis used to rank instances (documents) as is essential in in-\\\\nformation retrieval tasks or to provide a cutoff threshold.\\\\n\\\\nin our setup, the training set consisted of search results\\\\nfrom the original review formed with included instances\\\\nbeing the studies included in the review (the relevant re-\\\\ntrievals) and excluded instances being the studies found\\\\nby the search used in the original review but excluded from\\\\nthe review (irrelevant retrievals). the new examples to be\\\\nclassiﬁed were articles indexed since the date of the search\\\\nperformed in the original review.\\\\n\\\\nsvm searches were run on medline data stored\\\\nlocally at\\\\nthe national research council of canada.\\\\nmedline was refreshed before running the searches.\\\\nthe medline records were represented as bags of fea-\\\\ntures, where features consisted of lowercased words and\\\\nword combinations (up to four words) from the title and ab-\\\\nstract ﬁelds, full mesh terms, and contents of all other\\\\nﬁelds in the medline record. features were uniformly\\\\nweighted. the svm implementation used was svm light\\\\n[23] used with a linear kernel.\\\\n\\\\nthe approach was piloted with one ahrq evidence\\\\nreport, and various conﬁgurations were tried, observing\\\\nthe placement of several known new relevant studies within\\\\nthe retrieval. the medline set was ﬁltered with the\\\\nrevised highly sensitive search strategy (hsss) [24]. pre-\\\\nliminary testing indicated that this ﬁlter would retrieve\\\\n99.1% of medline-indexed relevant new evidence from\\\\nthe larger cohort of journal-published systematic reviews\\\\n(n 5 72) and cochrane reviews (n 5 27). this hsss ﬁlter\\\\nwas added primarily to improve processing speed, relative\\\\nto running the classiﬁer against all of medline. with\\\\nthe ﬁlter, processing for one systematic review took about\\\\n10 minutes. next, a coarse-grained ﬁlter was applied;\\\\nincluded and excluded instances were combined in a single\\\\nset labeled pos. adjacent pubmed ids (the next higher\\\\npmid to each member of pos, as long as that higher num-\\\\nber was not itself a member of pos) were combined in a set\\\\nto represent world. the model was trained on pos vs.\\\\nworld, using words and phrases from title, abstract,\\\\nauthor names, and journal name. this model was run\\\\nagainst the records passing the hsss ﬁlter.\\\\n\\\\nfour different subsets of\\\\n\\\\nthe svm retrieval were\\\\nexamineddone set included all records with a relevance\\\\nscore of 0.5 or more, up to a maximum of 200 retrievals\\\\n(svm200point5). other sets consisted of all records with\\\\nrelevance scores of 0.95 or more (svm95), 0.90 or more\\\\n(svm90), and 0.80 or more (svm80). pubmed id and\\\\nrelevance scores were recorded, and pubmed ids were\\\\n\\\\n\\\\x0cm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\n111\\\\n\\\\ntable 1. characteristics of the included reviews\\\\n\\\\ncochrane\\\\nreviews; n\\\\n\\\\nahrq evidence\\\\n\\\\nreports; n\\\\n\\\\nincorporated into the database of records for the reviewers\\\\nto screen. only the results from svm200point5 are re-\\\\nported here and are described as svm.\\\\n\\\\n2.3. determining performance of the test searches\\\\n\\\\nfor the updated cochrane reviews, reference standard ar-\\\\nticles were those studies included in the updated review that\\\\nwere not in the original and which entered medline after\\\\nthe search date of the original review. for the ahrq evi-\\\\ndence reports, the full retrieval set was screened. records\\\\nfound relevant by the consensus of reviewers were consid-\\\\nered reference standard articles. recall was the proportion\\\\nof reference standard articles identiﬁed by the search:\\\\n\\\\nnumber of reference standard articles found\\\\ntotal number of reference standard articles\\\\n\\\\nbiovenn software was used to analyze the overlap be-\\\\ntween the retrieval of relevant articles from the three\\\\nsearches and to create venn diagrams [25].\\\\n\\\\nall included reviews were classiﬁed into clinical area\\\\nbased on factors such as isi journal classiﬁcation, the co-\\\\nchrane collaboration review group where the topic might\\\\nbe placed and the high level mesh term under which the pop-\\\\nulation (condition) would be indexed. performance of the\\\\nsearches in different clinical areas was displayed graphically\\\\nfor the searches both alone and in combinations, to allow ex-\\\\namination of differences in parallelism, level, and ﬂatness.\\\\n\\\\nprecision is the proportion of all retrieved records that\\\\n\\\\nare relevant:\\\\n\\\\nnumber of reference standard articles found\\\\n\\\\ntotal number of records retrieved\\\\n\\\\nthe inverse of precision is the number needed to read to\\\\nﬁnd one eligible study, thus precision inﬂuences the work\\\\nof the review. precision was calculated only for the ahrq\\\\nevidence reviews. because not all candidates retrieved by\\\\nthe cochrane searches were assessed, precision could not\\\\nbe established in that sample.\\\\n\\\\n2.4. determining stability of results over time\\\\n\\\\nthe cq and related article searches, originally run in\\\\nmarch 2008, were repeated in february 2015 in the ahrq\\\\ncohort. recall of relevant items was compared with the\\\\noriginal retrievals to determine if any changes introduced\\\\nby national library of medicine might invalidate ﬁndings.\\\\nas svm conﬁguration is under the control of the investiga-\\\\ntors, it was not retested.\\\\n\\\\n3. results\\\\n\\\\nsix cochrane reviews and 10 ahrq evidence reports\\\\nmet all inclusion criteria (appendix b at www.jclinepi.\\\\ncom). characteristics of the included reviews are shown\\\\nin table 1.\\\\n\\\\n9a\\\\n1\\\\n3\\\\n\\\\n3\\\\n\\\\nd\\\\n1\\\\n\\\\nd\\\\n1\\\\n2\\\\n1\\\\n1\\\\n\\\\n1\\\\nd\\\\nd\\\\n\\\\nd\\\\n3\\\\n3\\\\n4\\\\n\\\\ncharacteristic\\\\n\\\\ntherapy evaluated\\\\n\\\\nmedications\\\\nmedical devices\\\\nprocedures\\\\n\\\\nclinical topic area\\\\n\\\\ncardiac and cardiovascular\\\\n\\\\nsystems\\\\n\\\\ncritical care\\\\nendocrinology and\\\\n\\\\nmetabolism\\\\n\\\\ninfectious disease\\\\nclinical neurology\\\\nobstetrics and gynecology\\\\noncology\\\\nperipheral vascular\\\\n\\\\ndiseases\\\\npsychiatry\\\\nrespiratory systems\\\\nurology and nephrology\\\\n\\\\npublication period\\\\n\\\\nmarch 1997eapril 1999\\\\nmay 1999ejune 2001\\\\njuly 2001eaugust 2003\\\\nseptember 2003e\\\\ndecember 2005\\\\n\\\\nmedian included trials\\\\n\\\\nmedian included\\\\n\\\\nparticipants\\\\n\\\\nmedline coverage of\\\\n\\\\n5\\\\n1\\\\nd\\\\n\\\\nd\\\\n\\\\n1\\\\nd\\\\n\\\\n3\\\\nd\\\\nd\\\\nd\\\\nd\\\\n\\\\nd\\\\n1\\\\n1\\\\n\\\\n3\\\\n1\\\\n2\\\\nd\\\\n\\\\n17 (iqr, 14e20)\\\\n\\\\n96 (iqr,\\\\n\\\\n31.75e121.5)\\\\n8,679 (iqr,\\\\n22,830 (iqr, 14,\\\\n4,085e50,109)\\\\n172e49,687)\\\\n99/107 (92.5%) 969/980 (98.8%)\\\\n\\\\nincluded articles\\\\nabbreviations: ahrq, agency for healthcare research and qual-\\\\n\\\\nity; iqr, interquartile range.\\\\n\\\\na some ahrq reviews included more than one class of therapy.\\\\n\\\\nrecall of new relevant studies is shown in table 2. the\\\\nupdated cochrane reviews had 20 new studies identiﬁed by\\\\nthe review authors and added in the updates. recall of these\\\\n20 ranged from 1.00 for the related articles method to a low\\\\nof 0.80 cqs (table 2). our team identiﬁed 277 new studies\\\\nas relevant for inclusion in the 10 evidence reports that\\\\nwere updated. all test searches showed lower recall for this\\\\ncohort than for the cochrane reviews and here the cq out-\\\\nperformed both ranking searches (table 2).\\\\n\\\\n3.1. recall of cq combined with a ranking method\\\\n\\\\nof the 297 new relevant studies identiﬁed, the combina-\\\\ntion of cq and related articles searches identiﬁed 270\\\\n(overall\\\\nin\\\\nthe cochrane cohort and 0.90 (250/277) in the ahrq\\\\ncohort.\\\\n\\\\nrecall of 0.91). recall was 1.00 (20/20)\\\\n\\\\nthe combination of cq and svm identiﬁed 263 studies\\\\n(overall recall of 0.89). recall was 1.00 (20/20) in the\\\\ncochrane cohort and 0.88 (243/277) in the ahrq cohort.\\\\nwhen all three methods were used together, recall was\\\\n0.997 with 296 of the 297 studies identiﬁed.\\\\n\\\\n\\\\x0c112\\\\n\\\\nm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\ntable 2. recall of eligible studies by the search methods\\\\n\\\\ncochrane reviews\\\\n\\\\nahrq evidence\\\\n\\\\nreports\\\\n\\\\nretrieval method\\\\n\\\\nclinical query\\\\nrelated articles\\\\nsvm\\\\ntotal\\\\n\\\\nn\\\\n\\\\n16\\\\n20\\\\n19\\\\n20\\\\n\\\\nrecall\\\\n\\\\n0.80\\\\n1.00\\\\n0.95\\\\n\\\\nn\\\\n\\\\n188\\\\n176\\\\n129\\\\n277\\\\n\\\\nrecall\\\\n\\\\n0.68\\\\n0.64\\\\n0.47\\\\n\\\\nabbreviations: ahrq, agency for healthcare research and qual-\\\\n\\\\nity; svm, support vector machine.\\\\n\\\\nacross the two cohorts, the overlap and unique component from\\\\nthe retrieval of relevant records by the test searches was examined\\\\n(fig. 1).\\\\n\\\\n3.2. consistency across clinical areas\\\\n\\\\nthirteen clinical areas were represented in the larger\\\\nstudy that included 72 journal-published reviews, but only\\\\neight of those clinical areas had two or more new relevant\\\\nstudies in the smaller cochrane and ahrq cohorts. fig. 2\\\\nshows recall of new studies by the three types of searches,\\\\nfor these eight clinical areas.\\\\n\\\\ncombined recall of the cq boolean search paired with a\\\\nranking search is shown in fig. 3. the combination showed\\\\ncomplete recall of relevant new studies in three of eight\\\\nareas when cq was paired with related articles and recall\\\\nof 0.67 or higher in all areas. four of eight clinical areas\\\\nhad complete recall for the combination of cq and svm,\\\\nand recall was 0.80 or higher in all areas. even for periph-\\\\neral vascular disease, where all three searches performed\\\\n\\\\nfig. 2. recall of new studies by clinical area for each search method.\\\\nabbreviations: svm, support vector machine.\\\\n\\\\nfairly poorly individually, recall was 0.76 for the cq/\\\\nrelated articles and 0.90 in the cq/svm pairing.\\\\n\\\\n3.3. search precision\\\\n\\\\nprecision, across the 10 ahrq reviews, was 0.11 for the\\\\ncq, 0.22 for related articles, and 0.19 for svm (table 3).\\\\n\\\\nfig. 1. overlapping and unique retrieval of relevant studies by each\\\\nsearch method. the size of the circle is proportional to the size of\\\\nthe retrieval, but the overlap and unique portions are approximations.\\\\nexact ﬁgures for the components are labeled. abbreviations: svm,\\\\nsupport vector machine.\\\\n\\\\nfig. 3. recall of new studies by clinical area for search methods in\\\\ncombination. abbreviations: cq, clinical query; svm, support vector\\\\nmachine.\\\\n\\\\n\\\\x0ctable 3. overall precision in the ahrq sample\\\\n\\\\n4. discussion\\\\n\\\\nm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\n113\\\\n\\\\nretrieval method\\\\n\\\\nclinical query\\\\nrelated articles\\\\nsvm\\\\nclinical query þ related\\\\nclinical query þ svm\\\\nclinical query þ related\\\\n\\\\narticles\\\\n\\\\neligible\\\\nstudies\\\\nretrieved\\\\n\\\\n187\\\\n176\\\\n128\\\\n250\\\\n\\\\n243\\\\n276\\\\n\\\\nno. of candidates\\\\n\\\\nretrieved\\\\n\\\\nprecision\\\\n\\\\n1,637\\\\n814\\\\n659\\\\n2,318a\\\\n\\\\n2,247a\\\\n3,264a\\\\n\\\\n0.11\\\\n0.22\\\\n0.19\\\\n0.11\\\\n\\\\n0.11\\\\n0.08\\\\n\\\\narticles þ svm\\\\nabbreviations: ahrq, agency for healthcare research and\\\\n\\\\nquality; svm, support vector machine.\\\\n\\\\na number of candidates after removal of duplicate records.\\\\n\\\\nprecision for the individual reviews ranged from 0.0 to 0.38\\\\nfor cqs, 0.004 to 0.80 for related articles, and 0.01 to 0.43\\\\nfor svm. overall precision was 0.11 for cq and either\\\\nrelated articles or svm. overall precision was 0.08 when\\\\ncq and both related articles and svm were used. for\\\\nranked searches where a ﬁxed number of records will be\\\\nscreened, that number is always the denominator, so preci-\\\\nsion will tend to increase as the number of relevant records\\\\nincreasesdthe maximum retrieval size for svm was cap-\\\\nped at 200 records.\\\\n\\\\n3.4. retrieval consistency over time\\\\n\\\\nsearches were retested for the ahrq cohort using\\\\npubmed results obtained february 24, 2015. recall of the\\\\nrelated articles searches across the 10 ahrq reports was\\\\n0.64 originally and 0.50 when repeated (fig. 4). of the\\\\nthree extreme cases, one rose from 0.00 to 0.80, one re-\\\\nmained at 0.00, and one dropped from 1.00 to 0.60. cq\\\\nhad overall\\\\nrecall of 0.68 originally and 0.70 when\\\\nrepeated. changes in extreme cases were minimal. overall\\\\ncombined recall of cq and related article was 0.90 origi-\\\\nnally,\\\\nfalling to 0.85 when repeated. considering the\\\\nextreme values, all review with combined recall of 1.00\\\\noriginally remained at 1.00, whereas the one review with\\\\n0.00 recall in combination showed recall of 0.80 when\\\\nrepeated.\\\\n\\\\n1.0\\\\n0.9\\\\n0.8\\\\n0.7\\\\n0.6\\\\n0.5\\\\n0.4\\\\n0.3\\\\n0.2\\\\n0.1\\\\n0.0\\\\n\\\\noriginal\\\\n\\\\n2015\\\\n\\\\nclinical query\\\\n\\\\nrelated ar(cid:415)cles\\\\n\\\\ncombined\\\\n\\\\nfig. 4. recall of new studies for search methods, alone and in combi-\\\\nnation originally (left) and when retested in 2015 (right).\\\\n\\\\nwe expected that\\\\n\\\\nthe sophisticated svm approach\\\\nwould perform well when paired with more stripped-\\\\ndown, focused boolean searches. indeed, the two together\\\\nwere able to replace the multidatabase, multimodel\\\\nsearches used by the original review teams in updating\\\\nthe cochrane searches. the new relevant studies for the co-\\\\nchrane reviews may have been relatively easy to ﬁnd, but\\\\nthe pairing was also effective in the ahrq set, and there\\\\nshowed better precision than is usually seen with traditional\\\\nsearches for systematic review of rcts [9]. the ahrq set\\\\nwas formed from the new studies identiﬁed for more com-\\\\nplex interventions in a rigorous, well-funded study [16].\\\\n\\\\nwe were surprised that the related articles approach per-\\\\nformed almost as well as svm when used in combination\\\\nwith the cq. the related article search has advantages over\\\\nsvmdit requires far less data preparation, and no special\\\\nsoftware is needed for its use. this makes it useful not only\\\\nin updating, but also, if appropriate seed articles can be\\\\nfound, in original reviews.\\\\n\\\\nthat both svm and related articles sometimes showed\\\\npoor recall when used alone, but consistently good recall\\\\nwhen used with a boolean method suggests that there is\\\\nreal beneﬁt in using complementary search methods to\\\\nquerying medline. the related article method is not very\\\\ntime consuming, and easily added to other planned search\\\\nefforts, while the third method used here, svm, is more\\\\ntechnical. the combined performance of all three methods\\\\nwas surprising, but such a setup might become unpractical\\\\nin day-to-day use.\\\\n\\\\nother investigators have similar ﬁndings. examination\\\\nof the supplemental material presented in the appendix c\\\\nto the article by waffenschmidt et al. [18] reveals that\\\\nacross the 19 reviews tested, the combination of a simple\\\\nstructured boolean search (ssbs) and the ﬁrst 50 retrieval\\\\nof pubmed similar articles showed complete retrieval of all\\\\nreference standard articles in 14 of 19 reviews and never\\\\nless than 0.90 recall (appendix c at www.jclinepi.com pre-\\\\nsents data from the table by waffenschmidt graphically).\\\\nwaffenschmidt concluded ‘‘the combination of these two\\\\nsearch techniques that are independent of each other seems\\\\nto compensate the respective weaknesses.’’\\\\n\\\\nssbs search by waffenschmidt was constructed in\\\\npubmed from search terms selected for the indication and\\\\nintervention with pubmed’s narrow cq ﬁlter (category:\\\\ntherapy). their search using the similar articles feature (re-\\\\nlcits) did not use a set of seed articles; rather, the relcits\\\\nfunction was applied for each relevant citation previously\\\\nidentiﬁed in pubmed. their test articles were the included\\\\nstudies in 19 systematic reviews of drugs.\\\\n\\\\nagoritsas et al. also described search construction\\\\nmethods for searches based on cq and pubmed similar ar-\\\\nticles, tested for their ability to retrieve the included studies\\\\nof 30 cochrane reviews [17]. although their search con-\\\\nstruction methods differed from those used here, both\\\\n\\\\n\\\\x0c114\\\\n\\\\nm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\nsearches were derived using standard methods. their\\\\napproach to the structured boolean search used terms from\\\\nthe population, intervention, and comparison with the cq,\\\\nlimited to humans and english. two clinicians selected the\\\\npubmed similar articles seeds from the initial pubmed\\\\nretrieval. they noted that no one method provided consis-\\\\ntently high retrieval [17].\\\\n\\\\nthus, the robust nature of this pairing of boolean and\\\\nnon-boolean searches has been shown in several contexts,\\\\ngiving support\\\\nto the hypothesis that methods such as\\\\nrelated articles and svm can compensate for the variation\\\\ninherent\\\\nin selecting search terms or assigning subject\\\\nheadings during indexing.\\\\n\\\\nconsidering stability of the searches over time, the\\\\nminimal change seen in the performance of the cq is\\\\nlikely explained by indexing changes of a few records.\\\\nthe computation appears not to have changed. this sug-\\\\ngests that indexing changes may also impact svm results\\\\nover time; however, this is likely to result in improved\\\\nperformance, as was seen with the cq. the computation\\\\nof similar articles is still described by national library\\\\nof medicine as being based on the algorithm described\\\\nby kim et al. [26]. that algorithm would suggest changes\\\\nover time in the nearest neighbor score as the frequency of\\\\ncertain terms in the medline corpus changes. a full\\\\nexploration of the changes in nearest neighbor scores of\\\\nsimilar articles over time is beyond the scope of this\\\\nstudy. it should be noted that when the similar articles\\\\nfeature was initially studied, it was simple to submit seed\\\\narticles, and then add limits such as date or rct publica-\\\\ntion type. on retesting, such additional limits were more\\\\ncomplex to apply, and the elink utility seemed to be the\\\\nmost practical approach to identifying the related articles\\\\n[27].\\\\n\\\\nthe beneﬁt of the complementary searches, relative to\\\\nboolean searching alone, was greatest for ahrq evidence\\\\nreports and less for the simpler cochrane reviews, suggest-\\\\ning that this approach may be particularly useful for more\\\\ncomplex evidence. there are examples of its utility in the\\\\nliterature. for example, in a realist review of a multidisci-\\\\nplinary body of\\\\nliterature identifying six domains of\\\\nclinical practice guideline implementability, the use of\\\\npubmed’s similar articles feature as a third stage of search-\\\\ning identiﬁed 131 records of which 104 were relevant [28].\\\\nin a systematic review of evidence on the links between\\\\npatient experience and clinical safety and effectiveness,\\\\nthe authors used pubmed similar articles to snowball on\\\\narticles\\\\nidentiﬁed through an embase search to\\\\novercome the limitations of predeﬁned searches for com-\\\\nplex evidence [29].\\\\n\\\\nthere is always the possibility that a search, or even a\\\\npair of searches, will fail. one method to detect such\\\\nfailures is to test whether the search strategies ﬁnd known\\\\nrelevant items [30]. in the updating case, this is easily done\\\\nusing the included studies of the original review as a test\\\\n\\\\nset, allowing the review to determine the medline\\\\ncoverage of their particular topic at the same time.\\\\n\\\\n4.1. limitations\\\\n\\\\nthere are two limitations to our proposed strategy. other\\\\ndatabases should be searched in the unusual event that\\\\nnumerous studies, representing more than a small propor-\\\\ntion of the total n, are not included in medline. second,\\\\nwhen it is important to ﬁnd articles too new to be indexed\\\\nby medline, systematic reviewers may wish to conduct a\\\\nsimple pubmed search limited to the nonindexed subsets\\\\n[31,32].\\\\n\\\\n5. conclusion\\\\n\\\\nthe general approach of a boolean plus a ranking\\\\nsearch is effective in medline retrieval for systematic\\\\nreviews. very high levels of identiﬁcation of relevant\\\\nmedline records, with adequate precision, are possible\\\\nusing a focused boolean search complemented by a docu-\\\\nment similarity or ranking method. the efﬁcacy of a\\\\nfocused boolean search paired with a search using the\\\\npubmed similar articles feature is in agreement with pre-\\\\nvious studies [17,18], and this study shows that this com-\\\\nplementary effect also occurs when svm is used as the\\\\nadditional method. the beneﬁt of using two complemen-\\\\ntary approaches to achieving high recall in medline is\\\\na robust effect.\\\\n\\\\npubmed-related articles is a parsimonious method, as it\\\\nis readily available to all review teams without cost. the\\\\napproach is robust across clinical domains, and the effect\\\\nhas now been demonstrated in several samples. the method\\\\nmay be sufﬁcient for updating systematic reviews of inter-\\\\nventions and may be used for new reviews of interventions\\\\nwhen paired with a trials registry search. it is likely to work\\\\nin any type of search where medline provides good sub-\\\\nject coverage even if retrieval through traditional search\\\\nmethods has been challenging.\\\\n\\\\nacknowledgments\\\\n\\\\nmohamed ansari and jun ji undertook screening and\\\\nrelevance assessment of candidate studies retrieved by the\\\\ntest\\\\nsearches, tamara rader developed the boolean\\\\nsearches, and raymond daniel processed related articles\\\\nsearches and obtained full-text articles. david moher cosu-\\\\npervised margaret sampson doctoral dissertation and pro-\\\\nvided invaluable advice and guidance.\\\\n\\\\nsupplementary data\\\\n\\\\nsupplementary data related to this article can be found at\\\\n\\\\nhttp://dx.doi.org/10.1016/j.jclinepi.2016.03.004.\\\\n\\\\n\\\\x0cm. sampson et al. / journal of clinical epidemiology 78 (2016) 108e115\\\\n\\\\n115\\\\n\\\\n\""
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Text pre-processing\n",
    "data = data.lower()\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = re.sub(r'\\d+', '', data)\n",
    "#data = re.sub(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+','', data)\n",
    "data = data.replace(\"\\n\", \"\")\n",
    "#data = data.strip()\n",
    "data = sent_tokenize(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rank-bm25 in f:\\python\\lib\\site-packages (0.2)\n",
      "Requirement already satisfied: numpy in f:\\python\\lib\\site-packages (from rank-bm25) (1.18.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "global data_cleaned\n",
    "def clean(data):\n",
    "    data_cleaned=[]\n",
    "    for line in data:\n",
    "        #line = line.lower()\n",
    "        line = re.sub(r'\\d+', '', line)\n",
    "        #line = re.sub(r'[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+','', line)\n",
    "        #line = line.translate(str.maketrans('','', string.punctuation))\n",
    "        line = line.strip()\n",
    "        line = line.replace(\"\\n\", \"\")\n",
    "        line = line.replace(\"\\\\n\", \"\")\n",
    "        #line = sent_tokenize(line)\n",
    "        \n",
    "        data_cleaned.append(line)\n",
    "    return data_cleaned\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokens_questions(question):\n",
    "    tokenized_question = question.split()\n",
    "    create_chunks = nltk.ne_chunk(nltk.pos_tag(tokenized_question))\n",
    "    assign_pos_tags = tree2conlltags(create_chunks)\n",
    "    return assign_pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pos(question):\n",
    "    noun_add = []\n",
    "    assign_pos_tags = create_tokens_questions(question)\n",
    "    print(assign_pos_tags)\n",
    "    for i in range(0, len(assign_pos_tags) - 1):\n",
    "        if (assign_pos_tags[i][1] == 'NN' and assign_pos_tags[i + 1][1] == 'NN'):\n",
    "            extract_nouns = assign_pos_tags[i] + assign_pos_tags[i + 1]\n",
    "            noun_add.append(extract_nouns)\n",
    "        elif assign_pos_tags[i][1] == 'NNP':\n",
    "            extract_nouns = assign_pos_tags[i]\n",
    "            print(\"The word\", extract_nouns[0], \"is extracted and added to the query list\")\n",
    "            noun_add.append(extract_nouns[0])\n",
    "        elif assign_pos_tags[i][1] == 'NN':\n",
    "            extract_nouns = assign_pos_tags[i]\n",
    "            print(\"The word\", extract_nouns[0], \"is extracted and added to the query list\")\n",
    "            noun_add.append(extract_nouns[0])\n",
    "    return noun_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDocs = []\n",
    "global get_final_results\n",
    "extractedData = []\n",
    "def conductSentenceExtraction_Objective(keyword):\n",
    "    if keyword == \"objective\":\n",
    "        question = \"What is the aim and objective of this paper\"\n",
    "        extractedData = data\n",
    "        extractedData = clean(extractedData)\n",
    "                \n",
    "    #to do for the next QA questions\n",
    "    '''elif keyword == \"introduction\":\n",
    "        question = \"Is the introduction and the context of the paper described\"'''\n",
    "    if keyword == \"conclusion\":\n",
    "        question = \"what is the conclusion\"\n",
    "        extractedData = data\n",
    "        extractedData = clean(extractedData)\n",
    "        noun_add = \"conclusion\"\n",
    "        \n",
    "    tokenized_corpus = [doc.split(\" \") for doc in extractedData]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "    #noun_add = extract_pos(question)\n",
    "    #print(noun_add)\n",
    "    #noun_add = \"intention\"\n",
    "    get_final_results = bm25.get_top_n(noun_add, extractedData, n=2)\n",
    "    #rawDocs.append(get_final_results)\n",
    "    #print(get_final_results)\n",
    "    return get_final_results\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "QA2_Ans = conductSentenceExtraction_Objective(\"conclusion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['recall (appendix c at www.jclinepi.com pre-sents data from the table by waffenschmidt graphically).waffenschmidt concluded ‘‘the combination of these twosearch techniques that are independent of each other seemsto compensate the respective weaknesses.’’ssbs search by waffenschmidt was constructed inpubmed from search terms selected for the indication andintervention with pubmed’s narrow cq ﬁlter (category:therapy).',\n",
       " 'recall (appendix c at www.jclinepi.com pre-sents data from the table by waffenschmidt graphically).waffenschmidt concluded ‘‘the combination of these twosearch techniques that are independent of each other seemsto compensate the respective weaknesses.’’ssbs search by waffenschmidt was constructed inpubmed from search terms selected for the indication andintervention with pubmed’s narrow cq ﬁlter (category:therapy).']"
      ]
     },
     "execution_count": 571,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA2_Ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('What', 'WP', 'O'), ('is', 'VBZ', 'O'), ('the', 'DT', 'O'), ('aim', 'NN', 'O'), ('and', 'CC', 'O'), ('objective', 'NN', 'O'), ('of', 'IN', 'O'), ('this', 'DT', 'O'), ('paper', 'NN', 'O')]\n",
      "The word aim is extracted and added to the query list\n",
      "The word objective is extracted and added to the query list\n"
     ]
    }
   ],
   "source": [
    "QA1_Ans = conductSentenceExtraction_Objective(\"objective\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['its goalis to extract knowledge about events/data from the work carried out in the differentphases of a business process, seeking to improve it, discovering associations betweenvariables, behavior or misbehavior patterns (van der aalst, ).in this context, the main objective of this paper is to present the results of a studythat identified and analyzed the primary studies related to process mining thatexclusively use ann or svm as the data mining technique.',\n",
       " 'we tested an additional search method, svm,in the cochrane and ahrq samples to permit comparisonwith our pubmed-related articles search and assess whetherthe complementary effect generalized beyond the pubmedsimilar articles method.the aim of this article was to test whether the combinedapproach of a focused boolean search paired with a secondsearch using the similar articles feature of pubmed or svmcan yield high recall with reasonable precision..']"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA1_Ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('What', 'WP', 'O'), ('is', 'VBZ', 'O'), ('the', 'DT', 'O'), ('objective', 'NN', 'O'), ('of', 'IN', 'O'), ('this', 'DT', 'O'), ('paper', 'NN', 'O')]\n",
      "The word objective is extracted and added to the query list\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['its goal\\\\nis to extract knowledge about events/data from the work carried out in the different\\\\nphases of a business process, seeking to improve it, discovering associations between\\\\nvariables, behavior or misbehavior patterns (van der aalst, ).\\\\n\\\\nin this context, the main objective of this paper is to present the results of a study\\\\nthat identified and analyzed the primary studies related to process mining that\\\\nexclusively use ann or svm as the data mining technique.',\n",
       " 'our intention was to reach\\\\nan overview of this type of process mining approach and also to more carefully\\\\nexplore the use of two of the most important computational intelligence techniques.\\\\nwith this purpose, this paper presents the following sections: a theoretical background;\\\\nthe detailed slr methodology; the results overview; the analysis of the selected\\\\nprimary studies; a discussion on the results; the validity threats analysis; and, finally,\\\\nthe conclusions.\\\\n\\\\n.',\n",
       " ', \\\\npp.']"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conductSentenceExtraction_Objective()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['its goalis to extract knowledge about events/data from the work carried out in the differentphases of a business process, seeking to improve it, discovering associations betweenvariables, behavior or misbehavior patterns (van der aalst, ).in this context, the main objective of this paper is to present the results of a studythat identified and analyzed the primary studies related to process mining thatexclusively use ann or svm as the data mining technique.',\n",
       " 'we tested an additional search method, svm,in the cochrane and ahrq samples to permit comparisonwith our pubmed-related articles search and assess whetherthe complementary effect generalized beyond the pubmedsimilar articles method.the aim of this article was to test whether the combinedapproach of a focused boolean search paired with a secondsearch using the similar articles feature of pubmed or svmcan yield high recall with reasonable precision..']"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA1_Ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we tested an additional search method, svm,in the cochrane and ahrq samples to permit comparisonwith our pubmed-related articles search and assess whetherthe complementary effect generalized beyond the pubmedsimilar articles method.the aim of this article was to test whether the combinedapproach of a focused boolean search paired with a secondsearch using the similar articles feature of pubmed or svmcan yield high recall with reasonable precision..'"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QA1_Ans[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#create a dictionary to save the pdf name and the objectives for QA questions\n",
    "q1 = {'pdf_slNo': [], 'aim_obj': [] }\n",
    "count = 0\n",
    "for item in QA1_Ans:\n",
    "    q1['aim_obj'].append(item)\n",
    "    count = count + 1\n",
    "    print(count)\n",
    "    q1['pdf_slNo'].append(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in f:\\python\\lib\\site-packages (3.0.3)\n",
      "Requirement already satisfied: et-xmlfile in f:\\python\\lib\\site-packages (from openpyxl) (1.0.1)\n",
      "Requirement already satisfied: jdcal in f:\\python\\lib\\site-packages (from openpyxl) (1.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.3.1; however, version 20.0.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export the results to .xslx\n",
    "from pandas import DataFrame\n",
    "\n",
    "df = DataFrame(q1, columns= ['pdf_slNo', 'aim_obj'])\n",
    "export_excel = df.to_excel (r'E:\\MasterThesis\\FinalPapers\\excel_QA\\export_dataframe.xlsx', index = None, header=True) #Don't forget to add '.xlsx' at the end of the path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "Objective = re.findall(r\"([^.]*?objective[^.]*\\.)\",str(sentences)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\\\\\\\n\\\\\\\\nin this context, the main objective of this paper is to present the results of a study\\\\\\\\nthat identified and analyzed the primary studies related to process mining that\\\\\\\\nexclusively use ann or svm as the data mining technique.']"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
